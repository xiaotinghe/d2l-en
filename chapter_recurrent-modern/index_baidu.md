# 现代递归神经网络
:label:`chap_modern_rnn`

我们已经介绍了RNNs的基础知识，它可以更好地处理序列数据。为了演示，我们在文本数据上实现了基于RNN的语言模型。然而，当实践者面对广泛的序列学习问题时，这样的技术可能是不够的。

例如，实际中一个值得注意的问题是RNN的数值不稳定性。虽然我们已经应用了梯度剪裁等实现技巧，但是通过更复杂的序列模型设计，这个问题可以得到进一步的缓解。具体来说，门控RNN在实践中更为常见。我们将首先介绍两种广泛使用的网络，即“选通循环单元”（GRU）和“长-短期记忆”（LSTM）。此外，我们将扩展RNN体系结构，使用迄今为止讨论过的单个无向隐藏层。我们将描述具有多个隐藏层的深层架构，并讨论具有正向和反向递归计算的双向设计。这种扩展在现代递归网络中经常被采用。在解释这些RNN变体时，我们继续考虑:numref:`chap_rnn`中引入的相同语言建模问题。

事实上，语言建模只揭示了序列学习能力的一小部分。在各种序列学习问题中，例如自动语音识别、文本语音转换和机器翻译，输入和输出都是任意长度的序列。为了说明如何适应这种类型的数据，我们将以机器翻译为例，介绍一种基于RNNs和波束搜索的编解码器结构来生成序列。

```toc
:maxdepth: 2

gru
lstm
deep-rnn
bi-rnn
machine-translation-and-dataset
encoder-decoder
seq2seq
beam-search
```
