# 现代递归神经网络
:label:`chap_modern_rnn`

我们已经介绍了RNNs的基础知识，它可以更好地处理序列数据。作为演示，我们在文本数据上实现了基于RNN的语言模型。然而，当实践者面临着广泛的序列学习问题时，这样的技术对于他们来说可能是不够的。

例如，实践中一个值得注意的问题是RNN的数值不稳定性。虽然我们已经应用了诸如渐变裁剪之类的实现技巧，但是可以通过更复杂的序列模型设计来进一步缓解这个问题。具体地说，门控RNN在实践中更为常见。我们将首先介绍两个这样的广泛使用的网络，即*门控循环单元*(GRU)和*长短期记忆*(LSTM)。此外，我们还将使用到目前为止已经讨论过的单个单向隐藏层来扩展RNN体系结构。我们将描述具有多隐层的深层结构，并讨论具有正向和反向递归计算的双向设计。这种扩展在现代循环网络中经常采用。在解释这些rnn变体时，我们继续考虑:numref:`chap_rnn`中引入的相同语言建模问题。

事实上，语言建模只揭示了序列学习能力的一小部分。在诸如自动语音识别、文本到语音和机器翻译等各种序列学习问题中，输入和输出都是任意长度的序列。为了说明如何适应这种类型的数据，我们将以机器翻译为例，介绍基于RNNs和波束搜索的编解码器结构来生成序列。

```toc
:maxdepth: 2

gru
lstm
deep-rnn
bi-rnn
machine-translation-and-dataset
encoder-decoder
seq2seq
beam-search
```
