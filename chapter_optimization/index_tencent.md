# 优化算法
:label:`chap_optimization`

如果您按顺序阅读这本书，那么到目前为止，您已经使用了许多优化算法来训练深度学习模型。它们是允许我们继续更新模型参数并使损失函数的值最小化的工具，正如在训练集上评估的那样。事实上，任何满足于将优化视为在简单设置中最小化目标函数的黑匣子设备的人，都很可能满足于这样的过程存在一系列咒语(名称如“sgd”和“Adam”)。

然而，要做好这项工作，还需要一些更深层次的知识。优化算法对于深度学习非常重要。一方面，训练一个复杂的深度学习模型可能需要数小时、数天甚至数周的时间。优化算法的性能直接影响到模型的训练效率。另一方面，了解不同优化算法的原理及其超参数的作用，有助于我们有针对性地调整超参数，提高深度学习模型的性能。

在本章中，我们深入探讨了常用的深度学习优化算法。深度学习中几乎所有的优化问题都是“非凸”的。尽管如此，在“凸”问题的背景下设计和分析算法已被证明是非常有启发性的。正因为如此，本章包括关于凸优化的入门知识和关于凸目标函数的非常简单的随机梯度下降算法的证明。

```toc
:maxdepth: 2

optimization-intro
convexity
gd
sgd
minibatch-sgd
momentum
adagrad
rmsprop
adadelta
adam
lr-scheduler
```
