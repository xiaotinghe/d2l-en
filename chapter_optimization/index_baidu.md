# 优化算法
:label:`chap_optimization`

如果你按顺序阅读这本书到目前为止，你已经使用了许多优化算法来训练深度学习模型。它们是允许我们继续更新模型参数和最小化损失函数值的工具，正如在训练集上评估的那样。事实上，任何满足于将优化视为在简单设置中最小化目标函数的黑盒设备的人，都很可能满足于这样一种知识，即存在这样一个过程的一系列咒语（名称如“SGD”和“Adam”）。

然而，要做好这件事，还需要一些更深层次的知识。优化算法对于深度学习非常重要。一方面，训练一个复杂的深度学习模型可能需要数小时、数天甚至数周的时间。优化算法的性能直接影响模型的训练效率。另一方面，了解不同优化算法的原理及其超参数的作用，可以有针对性地调整超参数，提高深度学习模型的性能。

在本章中，我们将深入探讨常见的深度学习优化算法。在深度学习中，几乎所有的优化问题都是非凸的。尽管如此，在*凸*问题的背景下设计和分析算法已经被证明是非常有益的。正是由于这个原因，本章包括了一个关于凸优化的入门和一个非常简单的随机梯度下降算法在凸目标函数上的证明。

```toc
:maxdepth: 2

optimization-intro
convexity
gd
sgd
minibatch-sgd
momentum
adagrad
rmsprop
adadelta
adam
lr-scheduler
```
