# 介绍
:label:`chap_introduction`

直到最近，几乎每一个我们日常交流的计算机程序都是由软件开发人员根据第一原理编写的。假设我们想编写一个应用程序来管理一个电子商务平台。在围着白板思考了几个小时之后，我们会想出一个大致可行的解决方案，可能看起来像这样：（i）用户通过运行在web浏览器或移动应用程序中的界面与应用程序交互；（ii）我们的应用程序与商业级数据库引擎交互，以跟踪每个用户的状态并维护历史交易记录；（iii）在我们应用程序的核心，*业务逻辑*（您可能会说，我们的应用程序的“大脑”详细地说明了程序在每一种可能的情况下应该采取的适当行动。

为了构建应用程序的大脑，我们必须逐步处理我们预期会遇到的每一个可能的角落案例，设计适当的规则。当用户第一次点击购物车时，我们可能会给它添加一个测试项，让它与购物车完全关联，我们可以从第一原理出发编写这样一个程序，并自信地启动它
*在见到真正的顾客之前。
我们能够根据驱动产品和系统的基本原理来设计自动化系统，通常在新的情况下，这是一项了不起的认知壮举。当你在学习的时候，你不应该用100%的时间去设计解决方案。

幸运的是，对于不断增长的机器学习科学家群体来说，我们希望自动化的许多任务并不是那么容易屈从于人类的创造力。想象一下，用你所知道的最聪明的头脑围着白板转，但这次你要解决以下问题之一：

* 编写一个程序，根据地理信息、卫星图像和过去天气的跟踪窗口来预测明天的天气。
* 编写一个程序，接受一个问题，用自由格式的文本表达，并正确地回答它。
* 编写一个程序，让给定的图像能够识别出其中包含的所有人，并在每个人周围画出轮廓。
* 编写一个程序，向用户展示他们可能喜欢但在自然浏览过程中不太可能遇到的产品。

在每一种情况下，即使是优秀的程序员也无法从头开始编写解决方案。原因可能各不相同。有时，我们正在寻找的程序遵循一种随时间变化的模式，我们需要我们的程序来适应。在其他情况下，这种关系（比如像素和抽象类别之间）可能太复杂了，即使我们的眼睛毫不费力地处理任务，也需要数千或数百万次超出我们有意识理解能力的计算。
*机器学习是一门强大的研究
从经验中学习的技巧。随着机器学习算法积累更多的经验，通常是以观测数据或与环境交互的形式，其性能会提高。与我们的确定性电子商务平台相比，无论积累了多少经验，它都按照相同的业务逻辑执行，直到开发人员自己了解并决定是时候更新软件了。在这本书中，我们将教你机器学习的基础知识，并特别关注“深度学习”，这是一套强有力的技术，推动了计算机视觉、自然语言处理、医疗保健和基因组学等领域的创新。

## 一个激励人心的例子

在开始写作之前，这本书的作者，像许多工作人员一样，不得不变得咖啡因。我们跳上车开始开车。亚历克斯用iPhone叫了一声“嘿Siri”，唤醒了手机的语音识别系统。然后穆命令“去蓝瓶咖啡店的方向”。手机很快显示了他的命令。它还认识到我们是在问路，并启动了地图应用程序（app）来满足我们的要求。地图应用程序一经推出，就确定了许多路线。在每条路线旁边，手机显示了一个预计的过境时间。虽然我们编造这个故事是为了方便教学，但它证明了在短短几秒钟内，我们与智能手机的日常互动可以使用多种机器学习模式。

想象一下，只要编写一个程序来响应一个叫醒词，比如“Alexa”、“OK Google”和“Hey Siri”。试着在一个房间里自己编写代码，除了一台电脑和一个代码编辑器，如:numref:`fig_wake_word`所示。从第一原则出发，你将如何编写这样一个程序？想想看。。。问题很难解决。每秒钟，麦克风将收集大约44000个样本。每个样品都是声波振幅的测量值。什么规则可以可靠地从原始音频片段映射到关于片段是否包含唤醒词的自信预测$\{\text{yes}, \text{no}\}$？如果你卡住了，别担心。我们也不知道如何从头开始编写这样一个程序。这就是我们使用机器学习的原因。

![Identify a wake word.](../img/wake-word.svg)
:label:`fig_wake_word`

这是诀窍。通常，即使我们不知道如何明确地告诉计算机如何从输入到输出进行映射，我们仍然能够自己执行认知壮举。换句话说，即使你不知道如何编程一台计算机来识别单词“Alexa”，你自己也能识别它。我们可以用一个包含大量音频的例子来收集这个词，而不是一个包含大量音频的数据集。在机器学习方法中，我们并不试图设计一个系统
*明确地识别唤醒词。
相反，我们定义了一个灵活的程序，其行为由许多*参数*决定。然后我们使用数据集来确定可能的最佳参数集，这些参数相对于感兴趣的任务的某种性能度量来提高程序的性能。

你可以把参数看作是旋钮，我们可以转动它，操纵程序的行为。修正参数后，我们称程序为*模型*。我们只需操纵参数就可以生成的所有不同程序集（输入-输出映射）称为“模型族”。使用我们的数据集来选择参数的元程序被称为学习算法。

在我们开始学习算法之前，我们必须精确地定义问题，确定输入和输出的确切性质，并选择适当的模型族。在本例中，我们的模型接收一段音频作为*input*，然后模型在$\{\text{yes}, \text{no}\}$中生成一个选择作为*output*。如果一切按计划进行，那么模型对于片段是否包含唤醒词的猜测通常是正确的。

如果我们选择了正确的模型族，应该有一个旋钮设置，这样模型每次听到“Alexa”这个词时都会发出“yes”的声音。因为唤醒词的确切选择是任意的，我们可能需要一个足够丰富的模型家族，通过另一个旋钮的设置，它只能在听到“杏子”这个词时发出“是”。我们期望同一个模型族应该适合于“Alexa”识别和“杏子”识别，因为它们在直觉上似乎是相似的任务。然而，如果我们想处理根本不同的输入或输出，比如说，如果我们想从图像映射到字幕，或者从英语句子映射到汉语句子，我们可能需要一个完全不同的模型族。

正如你可能猜到的，如果我们只是随机设置所有的旋钮，我们的模型不太可能识别出“Alexa”、“杏子”或任何其他英语单词。在机器学习中，*学习*是一个过程，通过这个过程，我们可以发现正确的旋钮设置，从而从我们的模型中强制执行所需的行为。换句话说，我们用数据训练我们的模型。如:numref:`fig_ml_loop`所示，培训过程通常如下所示：

1. 从一个随机初始化的模型开始，这个模型不能做任何有用的事情。
1. 获取一些数据（例如，音频片段和相应的$\{\text{yes}, \text{no}\}$标签）。
1. 调整旋钮，使模型在这些例子中表现得更少。
1. 重复第2步和第3步，直到模型很棒。

![A typical training process.](../img/ml-loop.svg)
:label:`fig_ml_loop`

总而言之，我们没有编写唤醒词识别器，而是编写了一个程序，如果我们用一个大的带标签的数据集，它可以“学习”识别唤醒词。您可以将此行为视为通过将程序与数据集呈现为*使用数据编程*来确定程序的行为。也就是说，我们可以通过向机器学习系统提供许多猫和狗的例子来“编程”猫检测器。通过这种方式，探测器最终将学会发出一个非常大的正数，如果它是一只狗，一个非常大的负数，如果它不确定，它将接近于零，这几乎触及了机器学习所能做的事情的表面。深度学习，我们稍后将更详细地解释，它只是解决机器学习问题的许多流行方法之一。

## 关键部件

在我们的wake word示例中，我们描述了一个由音频片段和二进制标签组成的数据集，并给出了如何训练模型来近似从片段到分类的映射。这种问题，我们试图根据已知的输入来预测一个指定的未知标签，给定一个由已知标签组成的数据集，称为监督学习。这只是许多机器学习问题中的一个。稍后我们将深入研究不同的机器学习问题。首先，我们想让大家更清楚地了解一些核心组件，无论我们遇到什么类型的机器学习问题：

1. 我们可以从中学习的数据。
1. 如何转换数据的模型。
1. 一个*目标函数*量化模型做得好（或不好）。
1. 调整模型参数以优化目标函数的*算法。

### 数据

不言而喻，没有数据就无法进行数据科学。我们可能会失去数百页的思考数据的确切组成，但目前，我们将错误的实际方面，并集中在关键属性的关注。一般来说，我们关注的是一组例子。为了有效地处理数据，我们通常需要想出一个合适的数值表示法。每个*示例*（或*数据点*、*数据实例*、*样本*）通常由一组称为*特征*（或*协变量*）的属性组成，模型必须从中进行预测。在上面的监督学习问题中，要预测的是一个特殊的属性，它被指定为*标签*（或*目标*）。

如果我们处理的是图像数据，每一张单独的照片可能构成一个例子，每一张照片都由与每个像素亮度相对应的数字值的有序列表表示。$200\times 200$彩色照片由$200\times200\times3=120000$个数值组成，对应于每个空间位置的红、绿、蓝通道的亮度。在另一个传统的任务中，我们可能试图预测一个病人是否能存活下来，给出一组标准的特征，如年龄、生命体征和诊断。

当每个例子的特征值数目相同时，我们说数据由固定长度的向量组成，我们把向量的恒定长度描述为数据的维数。正如您所想象的，固定长度可以是一个方便的属性。如果我们想训练一个在显微镜图像中识别癌症的模型，固定长度的输入意味着我们可以少担心一件事。

然而，并不是所有的数据都可以很容易地表示为
*固定长度*向量。
虽然我们可能期望显微镜图像来自标准设备，但我们不能期望从互联网上采集的图像都以相同的分辨率或形状出现。对于图像，我们可以考虑将它们全部裁剪成标准尺寸，但这种策略只能让我们走到目前为止。我们冒着丢失信息的风险。此外，文本数据更顽固地抵制固定长度表示。考虑一下亚马逊、IMDB和TripAdvisor等电子商务网站上留下的客户评论。有些是简短的：“真臭！”。其他人漫无目的地翻着书。与传统方法相比，深度学习的一个主要优势是现代模型可以处理不同长度的数据。

一般来说，我们拥有的数据越多，我们的工作就越容易。当我们有了更多的数据，我们就可以训练出更强大的模型，减少对预先设想的假设的依赖。从（相对地）小数据到大数据的体制变化是现代深度学习成功的主要贡献者。为了让我们明白这一点，在深度学习中，许多最令人兴奋的模型在没有大数据集的情况下是行不通的。其他一些在小数据领域工作，但并不比传统方法好。

最后，仅仅拥有大量的数据并进行巧妙的处理是不够的。我们需要正确的数据。如果数据中充满了错误，或者选择的特征不能预测目标的兴趣量，那么学习就会失败。陈词滥调很好地反映了这种情况：
*垃圾进，垃圾出*。
此外，预测性能差并不是唯一的潜在后果。在机器学习的敏感应用中，如预测性政策、简历筛选和贷款风险模型，我们必须特别警惕垃圾数据的后果。一种常见的故障模式发生在数据集中，其中一些人在训练数据中没有出现。想象一下，在野外应用一个从未见过黑皮肤的皮肤癌识别系统。当数据不仅低估了某些群体，而且反映了社会偏见时，也可能出现失败。例如，如果过去的招聘决策被用来训练一个预测模型，用于筛选简历，那么机器学习模型可能会无意中捕捉到历史上的不公正现象并使之自动化。请注意，这一切都可能发生在没有数据科学家积极共谋，甚至没有意识到的情况下。

### 模型

大多数机器学习在某种意义上涉及到数据的转换。我们可能想建立一个系统，摄取照片和预测笑脸。或者，我们可能需要摄取一组传感器读数，并预测读数的正常与异常程度。通过*模型*，我们表示摄取一种类型的数据并输出可能不同类型的预测的计算机器。特别是，我们对可以从数据中估计的统计模型感兴趣。虽然简单的模型完全能够解决适当的简单问题，但本书中我们关注的问题扩展了经典方法的局限性。深度学习与经典方法的区别主要在于它所关注的一系列强大的模型。这些模型由许多连续的数据转换组成，这些数据从上到下链接在一起，因此被称为“深度学习”。在讨论深层模型的过程中，我们还将讨论一些更传统的方法。

### 目标函数

早些时候，我们介绍了机器学习作为从经验中学习。这里所说的“学习”，是指随着时间的推移，在某些任务上有所提高。但谁能说什么是进步呢？您可能认为我们可以建议更新我们的模型，而有些人可能不同意建议的更新是改进还是下降。

为了开发学习机器的正式数学系统，我们需要对我们的模型有多好（或坏）有正式的度量。在机器学习和优化中，我们称之为目标函数。按照惯例，我们通常定义目标函数，这样越低越好。这只是一个惯例。你可以取任何一个函数，它的值越高越好，然后把它变成一个新的函数，这个函数在性质上是相同的，但是对于哪个函数来说，越低越好。因为越低越好，这些函数有时被调用
*损失函数*。

当试图预测数值时，最常见的损失函数是*平方误差*，即预测值与实际值之差的平方。对于分类，最常见的目标是最小化错误率，也就是说，我们的预测与实际情况不符的部分示例。有些目标（如平方误差）很容易优化。由于不可微性或其他复杂性，其他（如错误率）难以直接优化。在这些情况下，通常会优化*替代目标*。

通常，损失函数是根据模型参数定义的，并取决于数据集。我们通过最小化在一个集合上产生的损失来学习模型参数的最佳值，该集合由一些为训练而收集的例子组成。然而，在训练数据上做得好并不能保证我们能在看不见的数据上做得好。因此，我们通常希望将可用数据分成两个分区：*training dataset*（或*training set*，用于拟合模型参数）和*test dataset*（或*test set*，用于评估），报告模型如何在这两个分区上执行。你可以把训练表现看作是一个学生在为真正的期末考试做准备的练习考试中的分数。即使成绩令人鼓舞，也不能保证期末考试能成功。换言之，测试表现可能会明显偏离训练表现。当一个模型在训练集上表现良好，但不能推广到看不见的数据时，我们说它是“过度拟合”。在现实生活中，这就像是尽管在实践考试中做得很好，但却没有通过真正的考试。

### 优化算法

一旦我们得到了一些数据源和表示，一个模型，和一个定义良好的目标函数，我们需要一个算法，能够搜索最佳的可能参数，以最小化损失函数。用于深度学习的流行优化算法基于一种称为“梯度下降”的方法。简而言之，在每一步中，这个方法都会检查每个参数，看看如果你稍微扰动那个参数，训练集丢失会以什么方式移动。然后在可能减少损失的方向上更新参数。

## 各种机器学习问题

在我们的激励例子中，唤醒词问题只是机器学习可以解决的众多问题中的一个。为了进一步激励读者，并在本书中讨论更多问题时为我们提供一些通用语言，下面我们列出了一些机器学习问题的示例。我们将不断引用前面提到的概念，如数据、模型和培训技术。

### 监督学习

监督学习解决的任务是预测给定输入特征的标签。每个feature-label对称为示例。有时，当上下文清楚时，我们可以使用术语*示例*来指输入的集合，即使相应的标签是未知的。我们的目标是生成一个模型，将任何输入映射到标签预测。

以一个具体的例子来说明这一点，如果我们从事医疗保健工作，那么我们可能需要预测患者是否会心脏病发作。这个观察结果，“心脏病发作”或“没有心脏病发作”，将是我们的标签。输入特征可能是生命体征，如心率、舒张压和收缩压。

监督之所以发挥作用，是因为在选择参数时，我们（监督者）为模型提供了一个数据集，其中每个示例都与基本真实标签相匹配。在概率方面，我们通常感兴趣的是估计给定输入特征的标签的条件概率。尽管监督学习只是机器学习的几种范式之一，但监督学习在工业上的成功应用占了机器学习的绝大多数。在一定程度上，这是因为许多重要任务可以简单地描述为在给定一组特定的可用数据的情况下估计未知事物的概率：

* 根据计算机断层扫描图像预测癌症与非癌症。
* 给出一个英语句子，预测正确的法语翻译。
* 根据本月的财务报告数据预测下个月股票的价格。

即使使用简单的描述“预测给定输入特征的标签”，监督学习也可以采取多种形式，并且需要大量的建模决策，这取决于（除其他考虑外）输入和输出的类型、大小和数量。例如，我们使用不同的模型来处理任意长度的序列和处理固定长度的向量表示。我们将在本书中深入探讨这些问题。

非正式地说，学习过程如下所示。首先，从已知特征的大量例子中选取一个随机子集，为每个样本获取基本的真实标签。有时这些标签可能是已经收集到的可用数据（例如，患者是否在下一年内死亡？）其他时候，我们可能需要使用人工注释器来标记数据（例如，将图像分配给类别）。这些输入和相应的标签一起构成了训练集。我们将训练数据集输入一个有监督的学习算法，这个函数将一个数据集作为输入，并输出另一个函数：学习模型。最后，我们可以将以前看不见的输入输入输入到学习的模型中，使用它的输出作为相应标签的预测。整个过程在:numref:`fig_supervised_learning`中绘制。

![Supervised learning.](../img/supervised-learning.svg)
:label:`fig_supervised_learning`

#### 回归

也许最简单的监督学习任务就是回归。例如，考虑从房屋销售数据库中获取的一组数据。我们可以构造一个表，其中每一行对应一个不同的房子，每一列对应一些相关的属性，例如房子的平方英尺、卧室的数量、浴室的数量以及到市中心的分钟数（步行）。在这个数据集中，每个例子都是一个特定的房子，对应的特征向量将是表中的一行。如果你住在纽约或旧金山，而你不是亚马逊、谷歌、微软或Facebook的首席执行官，那么你家中的（平方英尺、卧室数量、浴室数量、步行距离）特征向量可能类似于：$[600, 1, 1, 60]$。然而，如果你住在匹兹堡，它可能看起来更像$[3000, 4, 3, 10]$。像这样的特征向量对于大多数经典的机器学习算法是必不可少的。

使问题成为回归的实际上是输出。说你想买一个新家。考虑到上述的一些特征，你可能需要估计一栋房子的公平市场价值。标签，销售价格，是一个数值。当标签具有任意数值时，我们称之为*回归*问题。我们的目标是产生一个模型，其预测值接近实际标签值。

许多实际问题都是描述得很好的回归问题。预测一个用户将分配给一部电影的评分可以被认为是一个回归问题，如果你在2009年设计了一个很好的算法来完成这一壮举，你可能会赢得[1-million-dollar Netflix prize](https://en.wikipedia.org/wiki/Netflix_Prize)。预测病人在医院的住院时间也是一个回归问题。一个好的经验法则是多少？*或者*多少？*问题应建议回归，例如：

* 这个手术需要多少小时？
* 未来六小时这个镇会有多少雨量？

即使您以前从未使用过机器学习，您也可能非正式地解决过回归问题。例如，想象一下，你的排水管修好了，你的承包商花了3个小时从你的污水管里清除污垢。然后他寄给你一张350美元的账单。现在想象一下，你的朋友雇佣了同一个承包商两个小时，他收到了一张250美元的账单。如果有人要求你在未来的工作时间里多做一些合理的假设。你也可以假设有一些基本费用，然后承包商每小时收费。如果这些假设成立，那么给出这两个数据示例，你就可以确定承包商的定价结构：每小时100美元外加50美元到你家来。如果你遵循了那么多，那么你已经理解了线性回归背后的高级概念。

在这种情况下，我们可以生成与承包商价格完全匹配的参数。有时这是不可能的，例如，如果一些差异是由于除了你的两个特征之外的一些因素造成的。在这些情况下，我们将尝试学习使我们的预测与观测值之间的距离最小化的模型。在我们的大部分章节中，我们将着重于最小化平方误差损失函数。正如我们稍后将看到的，这种损失对应于我们的数据被高斯噪声破坏的假设。

#### 分类

回归模型很适合解决问题，有多少？*问题，很多问题都不适合这个模板。例如，一家银行希望将支票扫描添加到其移动应用程序中。这需要用户用智能手机的摄像头拍摄支票的照片，应用程序需要能够自动理解图像中显示的文本。具体地说，它还需要理解手写文本以使其更加健壮，例如将手写字符映射到已知字符之一。这种，哪一种？*这个问题叫做分类。虽然许多技术都会继续使用，但它使用的算法集与用于回归的算法集不同。

在*分类*中，我们希望我们的模型能够查看特征，例如，图像中的像素值，然后预测某个离散选项集中的*类别*（正式称为*类*），例如。对于手写数字，我们可能有10个类，对应于数字0到9。最简单的分类形式是只有两个类，这个问题我们称之为“二进制分类”。例如，我们的数据集可以包含动物的图像，我们的标签可能是$\mathrm{\{cat, dog\}}$类。在回归中，我们寻找一个回归器来输出一个数值；在分类中，我们寻找一个分类器，其输出是预测的类赋值。

由于本书的技术含量越来越高，因此很难优化只能输出硬分类任务的模型，例如“猫”或“狗”。在这些情况下，用概率语言来表达我们的模型通常要容易得多。给定一个例子的特征，我们的模型为每个可能的类分配一个概率。回到我们的动物分类示例，其中类是$\mathrm{\{cat, dog\}}$，分类器可能会看到一个图像，并将图像是猫的概率输出为0.9。我们可以通过说分类器90%确定图像描绘了一只猫来解释这个数字。预测类的概率大小传达了一个不确定性的概念。这不是不确定性的唯一概念，我们将在更高级的章节中讨论其他概念。

当我们有两个以上的类时，我们把这个问题称为多类分类。常见的例子包括手写字符识别$\mathrm{\{0, 1, 2, ... 9, a, b, c, ...\}}$。当我们试图最小化平方误差损失函数来解决回归问题时，分类问题的常见损失函数被称为*交叉熵*，其名称可以通过后面章节中对信息理论的介绍来揭开。

请注意，最有可能的类不一定是您将用于决策的类。假设你在后院发现了一个美丽的蘑菇，如:numref:`fig_death_cap`所示。

![Death cap---do not eat!](../img/death-cap.jpg)
:width:`200px`
:label:`fig_death_cap`

现在，假设您构建了一个分类器并训练它根据照片预测蘑菇是否有毒。假设我们的毒物检测分类器输出:numref:`fig_death_cap`包含死亡上限的概率是0.2。换句话说，分类器80%确定蘑菇不是死亡帽。你还得吃它。那是因为一顿美味的晚餐所带来的某些好处不值得冒20%的死亡风险。换句话说，不确定风险的影响远远大于收益。因此，我们需要计算预期风险，作为损失函数，也就是说，我们需要将结果的概率乘以与之相关的收益（或伤害）。在这种情况下，食用蘑菇造成的损失为$0.2 \times \infty + 0.8 \times 0 = \infty$，而丢弃蘑菇的损失为$0.2 \times 0 + 0.8 \times 1 = 0.8$。我们的谨慎是有道理的：正如任何真菌学家都会告诉我们的那样，:numref:`fig_death_cap`中的蘑菇实际上是一个死亡帽。

分类比二进制、多类甚至多标签分类要复杂得多。例如，有一些用于寻址层次结构的分类变体。层次结构假设许多类之间存在某种关系。因此，并非所有的错误都是相等的——如果我们一定要犯错，我们宁愿把错误分类到一个相关的类，而不是一个遥远的类。通常，这被称为“层次分类”。一个早期的例子是[Linnaeus](https://en.wikipedia.org/wiki/Carl_Linnaeus)，他把动物组织成一个等级。

在动物分类的例子中，把一只狮子狗（一种狗的品种）误认为雪纳瑞（另一种狗的品种）可能不会太糟糕，但如果我们的模型将狮子狗与恐龙混淆，将付出巨大的代价。哪个层次结构相关，可能取决于您计划如何使用模型。例如，响尾蛇和吊袜带蛇可能在系统进化树上很接近，但是把响尾蛇误认为吊袜带可能是致命的。

#### 标记

有些分类问题很适合于二进制或多类分类设置。例如，我们可以训练一个普通的二元分类器来区分猫和狗。鉴于目前计算机视觉的现状，我们可以用现成的工具轻松地做到这一点。尽管如此，无论我们的模型有多精确，当分类器遇到一个名为“不来梅的城市音乐家”的图像时，我们可能会发现自己陷入困境，这是一个流行的德国童话故事，讲述了:numref:`fig_stackedanimals`中的四只动物。

![A donkey, a dog, a cat, and a rooster.](../img/stackedanimals.png)
:width:`300px`
:label:`fig_stackedanimals`

如你所见，:numref:`fig_stackedanimals`有一只猫，一只公鸡，一只狗，一头驴，背景是一些树。取决于我们最终想用我们的模型做什么，把它当作一个二元分类问题可能没有什么意义。取而代之的是，我们可以给模型一个选项，说图像描绘了一只猫，一只狗，一头驴，
*还有一只公鸡。

学习预测非互斥类的问题称为“多标签分类”。自动标记问题通常最好描述为多标签分类问题。想想人们在技术博客上贴的标签，比如“机器学习”、“技术”、“小工具”、“编程语言”、“Linux”、“云计算”、“AWS”。一篇典型的文章可能会应用5-10个标记，因为这些概念是相互关联的。关于“云计算”的帖子可能会提到“AWS”，而关于“机器学习”的帖子也可能涉及“编程语言”。

在处理生物医学文献时，我们也必须处理这类问题，正确地标记文章很重要，因为它允许研究人员对文献进行详尽的回顾。在国家医学图书馆，许多专业的注释者会仔细阅读PubMed索引的每一篇文章，并将其与MeSH的相关术语联系起来，MeSH是一个大约28000个标签的集合。这是一个耗时的过程，注释器通常在归档和标记之间有一年的延迟。机器学习在这里可以用来提供临时标签，直到每一篇文章都可以有一个适当的手动审查。事实上，几年来，BioASQ组织已经有[hosted competitions](http://bioasq.org/)来完成这项工作。

#### 搜索

有时，我们不只是想将每个示例分配给一个bucket或一个实际值。在信息检索领域，我们希望对一组项目进行排序。以网络搜索为例。我们的目标不是确定特定页面是否与查询相关，而是确定大量搜索结果中哪一个与特定用户最相关。我们真正关心的是相关搜索结果的排序，我们的学习算法需要从更大的集合中产生有序的元素子集。换句话说，如果我们被要求从字母表中产生前5个字母，那么返回“a B C D D E”和“C a B E D D”是不同的。即使结果集是相同的，集合内的顺序也很重要。

这个问题的一个可能的解决方案是，首先为集合中的每个元素分配一个相应的相关性分数，然后检索排名靠前的元素。[PageRank](https://en.wikipedia.org/wiki/PageRank)，谷歌搜索引擎背后最初的秘密调料就是这样一个评分系统的早期例子，但它的特殊之处在于它不依赖于实际的查询。在这里，他们依赖于一个简单的相关性过滤器来识别相关项集，然后在PageRank上对包含查询项的结果进行排序。目前，搜索引擎使用机器学习和行为模型来获取与查询相关的分数。整个学术会议都在讨论这个问题。

#### 推荐系统
:label:`subsec_recommender_systems`

推荐系统是另一个与搜索和排名相关的问题设置。问题是相似的，因为目标是向用户显示一组相关的项目。主要区别在于强调
*个性化*
推荐系统中的特定用户。例如，对于电影推荐，科幻迷的结果页面和彼得·塞勒斯喜剧鉴赏家的结果页面可能会有很大不同。类似的问题也会出现在其他推荐设置中，例如零售产品、音乐和新闻推荐。

在某些情况下，客户会提供明确的反馈，传达他们对特定产品的喜爱程度（例如，亚马逊、IMDb和GoodReads上的产品评级和评论）。在其他一些情况下，他们会提供隐性反馈，例如，跳过播放列表上的标题，这可能表示不满，但可能只是表明歌曲在上下文中不合适。在最简单的公式中，这些系统被训练来估计某些分数，例如给定用户和物品的估计评级或购买概率。

给定这样一个模型，对于任何给定的用户，我们都可以检索得分最大的对象集，然后将其推荐给用户。生产系统是相当先进的，并考虑到详细的用户活动和项目特征时，计算这样的分数。:numref:`fig_deeplearning_amazon`是亚马逊推荐的深度学习书籍的一个例子，它基于个性化算法，调整以捕捉个人偏好。

![Deep learning books recommended by Amazon.](../img/deeplearning-amazon.jpg)
:label:`fig_deeplearning_amazon`

尽管它们具有巨大的经济价值，但单纯建立在预测模型之上的推荐系统仍存在一些严重的概念缺陷。首先，我们只观察“审查后的反馈”：用户更倾向于给他们感觉强烈的电影打分。例如，在5分制中，您可能会注意到，项目获得了许多五星级和一星级评级，但三星级评级却明显很少。此外，目前的购买习惯往往是由于目前的推荐算法，但学习算法并不总是考虑到这一细节。因此，有可能形成反馈循环，推荐系统会优先推送一个被认为更好（由于购买量较大）的商品，进而更频繁地被推荐。关于如何处理审查、激励和反馈循环的许多问题，都是重要的开放性研究问题。

#### 序列学习

到目前为止，我们已经研究了输入数量固定、输出数量固定的问题。例如，我们考虑根据一组固定的特征来预测房价：面积、卧室数量、浴室数量、步行到市中心的时间。我们还讨论了从一个图像（固定维度）到它属于固定数量的类中的每一个的预测概率的映射，或者获取一个用户ID和一个产品ID，并预测一个星级。在这些情况下，一旦我们将固定长度的输入输入输入到模型中以生成输出，模型会立即忘记它刚刚看到的内容。

如果我们的输入确实都具有相同的维度，并且连续的输入确实彼此无关，那么这可能是好的。但是我们如何处理视频片段呢？在这种情况下，每个片段可能由不同数量的帧组成。如果我们考虑到前一帧或后续帧，我们对每一帧中发生的事情的猜测可能会更强。语言也是如此。一个流行的深度学习问题是机器翻译：一个任务是吸收一些源语言的句子，并预测它们在另一种语言中的翻译。

这些问题也发生在医学上。我们可能需要一个模型来监控重症监护病房的病人，如果他们在未来24小时内死亡的风险超过某个阈值，就会发出警报。我们绝对不希望这个模型抛弃它每小时所知道的关于病人病史的所有信息，而仅仅根据最近的测量结果做出预测。

这些问题是机器学习最令人兴奋的应用之一，它们是序列学习的实例。它们需要一个模型来摄取输入序列或发出输出序列（或两者兼而有之）。明确地，
*顺序对顺序学习*考虑问题
其中输入和输出都是可变长度的序列，例如机器翻译和从口语中抄写文本。虽然不可能考虑所有类型的序列转换，但以下特殊情况值得一提。

**标记和解析**。这涉及到用属性注释文本序列。
换句话说，输入和输出的数量基本上是相同的。例如，我们可能想知道动词和主语在哪里。或者，我们可能想知道哪些单词是命名实体。一般来说，目标是基于结构和语法假设对文本进行分解和注释，以获得一些注释。这听起来比实际情况更复杂。下面是一个非常简单的例子，它用标记来说明哪些单词引用了命名的实体（标记为“Ent”）。

```text
Tom has dinner in Washington with Sally
Ent  -    -    -     Ent      -    Ent
```

**自动语音识别**。在语音识别中，输入序列
是说话人的录音（如:numref:`fig_speech`所示），输出是说话人所说内容的文本记录。挑战在于，与文本相比，音频帧（声音通常以8kHz或16kHz采样）多得多，也就是说，音频和文本之间没有1:1的对应关系，因为数千个样本可能对应于一个单独的单词。这些是序列到序列的学习问题，其中输出比输入短得多。

![`-D-e-e-p- L-ea-r-ni-ng-` in an audio recording.](../img/speech.png)
:width:`700px`
:label:`fig_speech`

**文本到语音**。这与自动语音识别相反。
换句话说，输入是文本，输出是音频文件。在这种情况下，输出比输入长得多。虽然人类很容易识别出一个坏的音频文件，但这对计算机来说并不是那么简单。

**机器翻译**。与语音识别不同的是
输入和输出以相同的顺序出现（对齐后），在机器翻译中，顺序反转非常重要。换言之，当我们仍在将一个序列转换成另一个序列时，输入和输出的数量和相应数据示例的顺序都不假定相同。考虑下面这个例子，说明德国人把动词放在句尾的特殊倾向。

```text
German:           Haben Sie sich schon dieses grossartige Lehrwerk angeschaut?
English:          Did you already check out this excellent tutorial?
Wrong alignment:  Did you yourself already this excellent tutorial looked-at?
```

在其他学习任务中会出现许多相关的问题。例如，确定用户阅读网页的顺序是一个二维布局分析问题。对话问题表现出各种额外的复杂性，决定下一步说什么需要考虑到现实世界的知识和跨长时间距离对话的先前状态。这些都是研究的活跃领域。

### 无监督学习

到目前为止，所有的例子都与监督学习有关，也就是说，我们向模型提供一个包含特征和相应标签值的巨大数据集。你可以把受监督的学习者看作是一个非常专业的工作和一个非常平庸的老板。老板会站在你身后，告诉你在每种情况下该做什么，直到你学会从各种情况到行动。为这样一个老板工作听起来很蹩脚。另一方面，取悦这位老板很容易。你只要尽快识别出模式并模仿他们的动作。

相反，为一个不知道自己想让你做什么的老板工作会让人沮丧。然而，如果你打算成为一名数据科学家，你最好习惯它。老板可能会给你一大堆数据，然后让你用它做一些数据科学研究！*这听起来很模糊，因为确实如此。我们称这类问题为“无监督学习”，我们可以问的问题的类型和数量只受我们的创造力的限制。我们将在后面的章节中讨论无监督的学习技巧。为了激起你现在的食欲，我们将介绍以下几个你可能会问的问题。

* 我们能找到少量的原型吗
准确地总结了数据？给定一组照片，我们能把它们分成风景照片、狗、婴儿、猫和山峰的照片吗？同样，给定一组用户的浏览活动，我们能将他们分组为具有相似行为的用户吗？这个问题通常被称为*集群*。
* 我们能找到一些参数吗
准确捕捉数据的相关属性？球的运动轨迹可以用球的速度、直径和质量来描述。裁缝们已经开发出了一小部分参数，这些参数相当准确地描述了人体的形状，以适应衣服的需要。这些问题被称为*子空间估计*。如果相关性是线性的，则称为主成分分析。
* 是否有（任意结构）对象的表示
在欧几里德空间中这样的符号性质可以很好地匹配吗？这可以用来描述实体及其关系，例如“罗马”$-$“意大利”$+$“法国”$=$“巴黎”。
* 是否有对根本原因的描述
我们观察到的大部分数据？例如，如果我们有关于房价、污染、犯罪、地理位置、教育和工资的人口统计数据，我们能否简单地根据经验数据发现它们之间的关系？与因果关系有关的领域和
*概率图形模型解决了这个问题。
* 无监督学习的另一个重要而令人兴奋的最新发展
是“生成性对抗性网络”的出现。这为我们提供了一种程序化的方法来合成数据，甚至是复杂的结构化数据，如图像和音频。基本的统计机制是检验真假数据是否相同的测试。

### 与环境互动

到目前为止，我们还没有讨论数据实际来自哪里，或者机器学习模型生成输出时实际发生了什么。这是因为有监督的学习和无监督的学习并没有以非常复杂的方式解决这些问题。不管是哪种情况，我们都会预先获取大量数据，然后启动模式识别机器，而不再与环境交互。因为所有的学习都是在算法与环境断开连接后进行的，这有时被称为离线学习。对于监督学习，考虑从环境中收集数据的过程类似于:numref:`fig_data_collection`。

![Collecting data for supervised learning from an environment.](../img/data-collection.svg)
:label:`fig_data_collection`

这种简单的离线学习有它的魅力。好的一面是，我们可以孤立地担心模式识别，而不必分心于其他问题。但缺点是，问题的表述是相当有限的。如果你更有雄心壮志，或者你是在读阿西莫夫的机器人系列小说长大的，那么你可能会想象，人工智能机器人不仅能够做出预测，而且能够在世界上采取行动。我们要考虑智能代理，而不仅仅是预测模型。这意味着我们需要考虑选择行动，而不仅仅是做出预测。此外，与预测不同，行动实际上会影响环境。如果我们想训练一个智能代理，我们必须考虑到它的行为可能会影响代理将来的观察结果。

考虑到与环境的交互将打开一整套新的建模问题。以下只是几个例子。

* 环境还记得我们以前做过什么吗？
* 环境是否有助于我们，例如，用户将文本读入语音识别器？
* 环境是否想要打败我们，例如，一个对抗性的设置，如垃圾邮件过滤（针对垃圾邮件发送者）或玩游戏（对对手）？
* 环境不在乎吗？
* 环境是否有变化的动力？例如，未来的数据是否总是与过去相似，还是随着时间的推移模式会发生变化，是自然变化还是响应我们的自动化工具？

最后一个问题提出了当训练和测试数据不同时*分布转移*的问题。最有经验的是我们讲师在考试时写的一道题。接下来，我们将简要介绍强化学习，这是一种明确考虑与环境交互作用的设置。

### 强化学习

如果您对使用机器学习开发一个与环境交互并采取行动的代理感兴趣，那么您可能最终会专注于*强化学习*。这可能包括机器人技术、对话系统、甚至是为视频游戏开发人工智能（AI）的应用。
*深度强化学习*，适用于
以深度学习来强化学习问题，已经风靡一时。突破性的深度Q网络在雅达利游戏中仅使用视觉输入就击败了人类，以及AlphaGo程序在棋盘游戏围棋中击败了世界冠军，是两个突出的例子。

强化学习给出了一个问题的非常一般的描述，在这个问题中，一个agent通过一系列的时间步与一个环境进行交互。在每一个时间步，代理从环境中接收到一些*观察*，并且必须选择一个*动作*，该动作随后通过某种机制（有时称为执行器）传回环境。最后，代理从环境中获得奖励。:numref:`fig_rl-environment`中说明了这一过程。然后代理接收后续观察，并选择后续操作，依此类推。强化学习代理的行为受策略控制。简而言之，*policy*只是一个从环境观察到动作的映射函数。强化学习的目标是制定一个好的政策。

![The interaction between reinforcement learning and an environment.](../img/rl-environment.svg)
:label:`fig_rl-environment`

强化学习框架的普遍性是很难夸大的。例如，我们可以将任何有监督学习问题归结为强化学习问题。假设我们遇到了分类问题。我们可以创建一个强化学习代理，每个类对应一个动作。然后我们就可以创造一个奖励的环境，这个奖励正好等于原始监督学习问题的损失函数。

也就是说，强化学习还可以解决许多监督学习无法解决的问题。例如，在监督学习中，我们总是期望训练输入与正确的标签相关联。但是在强化学习中，我们并不认为环境会告诉我们最佳的行为。一般来说，我们会得到一些奖励。此外，环境甚至可能不会告诉我们是什么行为导致了回报。

以国际象棋为例。唯一真正的奖励信号出现在游戏结束时，我们要么赢了，我们可以分配1的奖励，要么我们输了，我们可以分配-1的奖励。因此，强化学习者必须处理“学分分配”问题：决定哪些行为是值得表扬的，哪些行为是导致结果的罪魁祸首。10月11日升职的员工也是如此。这一提升可能反映了过去一年中大量精心挑选的行动。要想在未来获得更多的晋升，就需要弄清楚这一过程中哪些行为导致了晋升。

强化学习者可能还需要处理部分可观察性的问题。也就是说，目前的观察结果可能无法告诉你你当前的状态。比如说，一个清洁机器人发现自己被困在一所房子里许多相同的壁橱中的一个。推断机器人的精确位置（从而推断其状态）可能需要在进入壁橱之前考虑其先前的观察结果。

最后，在任何给定的点上，强化学习者可能知道一个好的策略，但是可能还有许多其他更好的策略，代理从未尝试过。强化学习者必须不断地选择是将目前最为人熟知的策略作为一项策略加以利用，还是探索策略的空间，从而有可能放弃一些短期回报来换取知识。

一般的强化学习问题是一个非常普遍的设置。行动影响后续观察。奖励只会根据所选择的行动来观察。可以完全或部分观察环境。同时解释所有这些复杂性可能会对研究人员提出太多的要求。此外，并不是每个实际问题都表现出这种复杂性。因此，研究者们研究了一些特殊情况下的强化学习问题。

当环境完全被观察到时，我们将强化学习问题称为马尔可夫决策过程。当国家不依赖于先前的行动时，我们称之为“情境强盗问题”。当没有状态时，只有一组初始未知奖励的可用动作，这个问题就是经典的“多武装强盗问题”。

## 根

我们刚刚回顾了机器学习可以解决的一小部分问题。对于一系列不同的机器学习问题，深度学习为解决这些问题提供了强大的工具。虽然许多深度学习方法都是最近才发明的，但使用数据和神经网络编程（许多深度学习模型的名称）的核心思想已经研究了几个世纪。事实上，人类对分析数据和预测未来结果的渴望由来已久，许多自然科学都植根于此。例如，伯努利分布是以[Jacob Bernoulli（1655--1705）](https://en.wikipedia.org/wiki/Jacob\u Bernoulli)，而高斯分布是[Carl Friedrich Gauss（1777—1855）](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss). 例如，他发明了最小均方算法，至今仍用于解决从保险计算到医疗诊断的无数问题。这些工具在自然科学中产生了一种实验方法——例如，电阻中电流和电压的欧姆定律可以用线性模型完美地描述。

即使在中世纪，数学家对估计也有敏锐的直觉。例如，雅各布·克贝尔（Jacob Köbel，1460--1533）的几何书(https://www.maa.org/press/journals/convergeneration/mathematic-burses-jacob-kobels-geometry)举例说明平均16个成年男子的脚的长度，以获得平均脚长。

![Estimating the length of a foot.](../img/koebel.jpg)
:width:`500px`
:label:`fig_koebel`

:numref:`fig_koebel`说明了这个估计器是如何工作的。16名成年男子在离开教堂时被要求排成一行。然后将它们的总长度除以16，得到一个现在达到1英尺的估计值。这个“算法”后来被改进以处理畸形的脚——两个脚最短和最长的人被送走，平均只超过其余的人。这是修剪平均估计的最早例子之一。

随着数据的收集和可用性的提高，统计数据真正起飞了。它的巨人之一，[罗纳德·费舍尔（1890-1962）](https://en.wikipedia.org/wiki/Ronald_-Fisher)对其理论及其在遗传学中的应用做出了重要贡献。他的许多算法（如线性判别分析）和公式（如Fisher信息矩阵）至今仍在频繁使用。事实上，甚至Fisher在1936年发布的Iris数据集有时也被用来说明机器学习算法。他也是优生学的倡导者，优生学应该提醒我们，在道德上可疑地使用数据科学与它在工业和自然科学中的生产性应用一样，有着悠久而持久的历史。

第二个对机器学习的影响来自信息理论[Claude Shannon（1916-2001）](https://en.wikipedia.org/wiki/Claude\u Shannon)和计算理论通过[Alan Turing（1912-1954）](https://en.wikipedia.org/wiki/Alan_Turing). 图灵提出了一个问题：“机器能思考吗？在他著名的论文《计算机机械与智能》:cite:`Turing.1950`中。在他所描述的图灵测试中，如果人类评估者很难根据文本交互来区分机器和人类的回答，那么机器可以被认为是智能的。

另一个影响可以在神经科学和心理学中找到。毕竟，人类显然表现出智能行为。因此，只有问一个人是否能够解释并可能对这种能力进行逆向工程才是合理的。最古老的算法之一是由[Donald Hebb（1904--1985）](https://en.wikipedia.org/wiki/Donald_O.\u Hebb). 在他开创性的著作《行为的组织》:cite:`Hebb.Hebb.1949`中，他提出神经元通过积极强化学习。这被称为赫比学习规则。它是Rosenblatt的感知器学习算法的原型，它奠定了许多支持当今深度学习的随机梯度下降算法的基础：强化期望行为，减少不期望行为，从而在神经网络中获得良好的参数设置。

生物灵感是神经网络得名的原因。一个多世纪以来（可以追溯到1873年的亚历山大·贝恩和1890年的詹姆斯·舍林顿的模型），研究人员一直试图组装类似相互作用神经元网络的计算电路。随着时间的推移，对生物学的解释已经不再那么字面意义上了，但这个名字却一直存在。在它的核心，有几个关键的原则，可以找到在大多数网络今天：

* 线性和非线性处理单元的交替，通常称为*层*。
* 使用链式规则（也称为*反向传播*）一次性调整整个网络中的参数。

在最初的快速发展之后，神经网络的研究从1995年到2005年一直萎靡不振。这主要有两个原因。首先，训练一个网络在计算上是非常昂贵的。虽然随机存取存储器在上个世纪末很丰富，但计算能力却很有限。其次，数据集相对较小。事实上，1932年的Fisher虹膜数据集是测试算法有效性的流行工具。拥有60000个手写数字的MNIST数据集被认为是巨大的。

考虑到数据和计算的稀缺性，强大的统计工具如核方法、决策树和图形模型被证明在经验上更优越。与神经网络不同的是，他们不需要几周的时间来训练，而且在理论上有很强的保证，可以提供可预测的结果。

## 深入学习之路

由于万维网、在线服务数亿用户的公司的出现、廉价、高质量传感器的传播、廉价的数据存储（克莱德定律）和廉价计算（摩尔定律），特别是GPU的形式，这些都改变了这一点，最初设计用于电脑游戏。突然之间，似乎在计算上不可行的算法和模型变得相关（反之亦然）。这在:numref:`tab_intro_decade`中得到了最好的说明。

：数据集与计算机内存和计算能力

|Decade|Dataset|Memory|Floating point calculations per second|
|:--|:-|:-|:-|
|1970|100 (Iris)|1 KB|100 KF (Intel 8080)|
|1980|1 K (House prices in Boston)|100 KB|1 MF (Intel 80186)|
|1990|10 K (optical character recognition)|10 MB|10 MF (Intel 80486)|
|2000|10 M (web pages)|100 MB|1 GF (Intel Core)|
|2010|10 G (advertising)|1 GB|1 TF (Nvidia C2050)|
|2020|1 T (social network)|100 GB|1 PF (Nvidia DGX-2)|
:label:`tab_intro_decade`

很明显，随机存取存储器没有跟上数据增长的步伐。同时，计算能力的增长已经超过了现有数据的增长速度。这意味着，由于计算预算的增加，统计模型需要提高内存效率（这通常通过添加非线性来实现），同时能够花费更多时间优化这些参数。因此，机器学习和统计学的研究热点从（广义的）线性模型和核方法转移到深层神经网络。这也是为什么许多深度学习的支柱，如多层感知器:cite:`McCulloch.Pitts.1943`、卷积神经网络:cite:`LeCun.Bottou.Bengio.ea.1998`、长短时记忆:cite:`Watkins.Dayan.1992`和Q学习:cite:`Watkins.Dayan.1992`，在相当长一段时间处于相对休眠状态之后，在过去十年中基本上被“重新发现”的原因之一。

统计模型、应用和算法的最新进展有时被比作寒武纪大爆发：物种进化的一个快速发展时期。事实上，最先进的技术不仅仅是可用资源的结果，应用于几十年前的算法。请注意，下面的列表仅仅触及了过去十年帮助研究人员取得巨大进展的想法的表面。

* 新的容量控制方法，如*dropout*:cite:`Srivastava.Hinton.Krizhevsky.ea.2014`，有助于减轻过度装配的危险。这是通过在整个神经网络中应用噪声注入:cite:`Bishop.1995`来实现的，为了训练目的，用随机变量代替权重。
* 注意力机制解决了困扰统计学一个多世纪的第二个问题：如何在不增加可学习参数的情况下增加系统的内存和复杂性。研究人员通过使用只能被视为可学习的指针结构:cite:`Bahdanau.Cho.Bengio.2014`找到了一个优雅的解决方案。不必记住整个文本序列，例如，对于固定维表示的机器翻译，需要存储的只是指向翻译过程中间状态的指针。这使得长序列的精确度显著提高，因为在开始生成新序列之前，模型不再需要记住整个序列。
* 例如，通过存储器网络:cite:`Sukhbaatar.Weston.Fergus.ea.2015`和神经程序解释器:cite:`Reed.De-Freitas.2015`的多阶段设计允许统计建模者描述推理的迭代方法。这些工具允许对深层神经网络的内部状态进行反复修改，从而在推理链中执行后续步骤，类似于处理器如何为计算修改内存。
* 另一个关键的发展是产生式对抗网络:cite:`Goodfellow.Pouget-Abadie.Mirza.ea.2014`的发明。传统上，密度估计和生成模型的统计方法侧重于寻找合适的概率分布和（通常是近似的）从中取样的算法。因此，这些算法在很大程度上受限于统计模型固有的灵活性。生成对抗网络的关键创新是用参数可微的任意算法代替采样器。然后对这些数据进行调整，使鉴别器（实际上是两个样本测试）无法区分假数据和真实数据。通过使用任意算法生成数据的能力，它为各种各样的技术打开了密度估计的大门。驰骋的斑马:cite:`Zhu.Park.Isola.ea.2017`和假名人脸:cite:`Karras.Aila.Laine.ea.2017`的例子都证明了这一进展。即使是业余的涂鸦者也可以根据描述场景布局的草图（:cite:`Park.Liu.Wang.ea.2019`）生成照片级真实感图像。
* 在许多情况下，单个GPU不足以处理大量可用于培训的数据。在过去的十年里，构建并行和分布式训练算法的能力有了显著的提高。设计可伸缩算法的一个关键挑战是，深度学习优化的主力军随机梯度下降依赖于相对较小的小批量数据进行处理。同时，小批量限制了gpu的效率。因此，在1024个gpu上进行训练，小批量大小为，比如说每批32个图像，相当于总共有32000个图像。最近的工作，首先由Li :cite:`Li.2017`，随后由:cite:`You.Gitman.Ginsburg.2017`和:cite:`Jia.Song.He.ea.2018`将尺寸提高到64000个观察值，从而将ResNet-50模型在ImageNet数据集上的训练时间缩短到不到7分钟。作为比较---最初的训练时间以天为单位。
* 并行计算的能力也对强化学习的进展做出了相当重要的贡献，至少在模拟是一种选择的情况下。这使得计算机在围棋、雅达利游戏、星际争霸和物理模拟（例如使用MuJoCo）方面取得了重大进展。有关如何在AlphaGo中实现这一点的说明，请参见:cite:`Silver.Huang.Maddison.ea.2016`。简言之，如果有足够多的（状态、行动、奖励）三元组可用，强化学习效果最好，也就是说，只要有可能尝试很多东西来学习它们之间的关系。模拟提供了这样一种途径。
* 深度学习框架在传播思想方面发挥了至关重要的作用。允许简单建模的第一代框架包括[Caffe](https://github.com/BVLC/caffe)、[Torch](https://github.com/torch)和[Theano](https://github.com/Theano/Theano)。许多开创性的论文都是用这些工具写的。到目前为止，它们已经被[TensorFlow](https://github.com/tensorflow/tensorflow)（通常通过其高级API [Keras](https://github.com/keras-team/keras)使用）、[CNTK](https://github.com/Microsoft/CNTK)、[Caffe 2](https://github.com/caffe2/caffe2)和[Apache MXNet](https://github.com/apache/incubator-mxnet)所取代。第三代工具，即用于深度学习的命令式工具，可以说是由[Chainer](https://github.com/chainer/chainer)率先开发的，它使用类似于pythonnumpy的语法来描述模型。这个想法被[PyTorch](https://github.com/pytorch/pytorch)、MXNet的[Gluon API](https://github.com/apache/incubator-mxnet)和[Jax](https://github.com/google/jax)采用。

系统研究人员构建更好的工具和统计建模人员构建更好的神经网络之间的分工大大简化了事情。例如，训练一个线性logistic回归模型曾经是一个非常重要的家庭作业问题，值得2014年卡内基梅隆大学（Carnegie Mellon University）的新机器学习博士生学习。到目前为止，这个任务只需不到10行代码就可以完成，这让程序员牢牢掌握了它。

## 成功案例

人工智能在交付成果方面有着悠久的历史，否则很难实现这些成果。例如，使用光学字符识别的邮件分拣系统从20世纪90年代开始部署，这毕竟是著名的MNIST手写数字数据集的来源。申请人的存款与银行存款的信用度相同。自动检查财务交易是否存在欺诈。这构成了许多电子商务支付系统的支柱，如PayPal、Stripe、支付宝、微信、苹果、Visa和万事达卡。国际象棋的计算机程序已经竞争了几十年。机器学习在互联网上提供搜索、推荐、个性化和排名。换言之，机器学习是无处不在的，尽管它常常隐藏在视线之外。

直到最近，人工智能才成为人们关注的焦点，这主要是因为解决了以前认为难以解决的问题，而且这些问题与消费者直接相关。许多这样的进步都归功于深度学习。

* 智能助理，如苹果的Siri、亚马逊的Alexa和谷歌的助手，都能够以合理的准确度回答口头问题。这包括一些琐碎的工作，比如打开电灯开关（对残疾人来说是个福音）到预约理发师和提供电话支持对话。这可能是人工智能正在影响我们生活的最明显的迹象。
* 数字助理的一个关键因素是能够准确识别语音。渐渐地，这种系统的精度已经提高到了在某些应用中达到人类平等的程度:cite:`Xiong.Wu.Alleva.ea.2018`。
* 物体识别同样也取得了长足的进步。估计图片中的物体在2010年是一项相当具有挑战性的任务。在ImageNet基准上，来自NEC实验室和伊利诺伊大学香槟分校的研究人员获得了28%的前5位错误率:cite:`Lin.Lv.Zhu.ea.2010`。到2017年，这一错误率降低到2.25%:cite:`Hu.Shen.Sun.2018`。同样，在鉴别鸟类或诊断皮肤癌方面也取得了惊人的成果。
* 游戏曾经是人类智慧的堡垒。从TD-Gammon开始，一个使用时间差分强化学习的双陆棋程序，算法和计算的进步导致了算法的广泛应用。与双陆棋不同，国际象棋的状态空间和动作集要复杂得多。深蓝击败了加里卡斯帕罗夫使用大规模并行，专用硬件和高效搜索通过游戏树:cite:`Campbell.Hoane-Jr.Hsu.2002`。由于其巨大的状态空间，Go更为困难。AlphaGo在2015年达到人类平价，使用深度学习结合蒙特卡罗树抽样:cite:`Silver.Huang.Maddison.ea.2016`。扑克的挑战是状态空间很大，而且没有被完全观察到（我们不知道对手的牌）。Libratus使用高效结构化的策略:cite:`Brown.Sandholm.2017`在扑克中超越了人类的表现。这说明了在游戏中令人印象深刻的进步，以及高级算法在其中扮演了关键角色的事实。
* 另一个显示人工智能进步的迹象是自动驾驶汽车和卡车的出现。虽然完全的自主性还没有完全实现，但是在这个方向上已经取得了巨大的进步，比如特斯拉、英伟达和威莫船运产品公司至少实现了部分自治。使完全自主性如此具有挑战性的是，正确的驾驶要求具备感知、推理和将规则纳入系统的能力。目前，深度学习主要用于这些问题的计算机视觉方面。其余部分则由工程师进行了大量调整。

同样，上面的列表几乎没有触及机器学习对实际应用的影响。例如，机器人学、物流学、计算生物学、粒子物理学和天文学等领域的最新进展都归功于机器学习。因此，机器学习正成为工程师和科学家普遍使用的工具。

关于人工智能的非技术性文章中经常提到人工智能的启示或人工智能奇点的问题。令人担心的是，机器学习系统会变得有知觉，并独立于程序员（和主人）来决定那些直接影响人类生计的事情。在某种程度上，人工智能已经直接影响到人类的生计：信誉度是自动评估的，自动驾驶仪主要驾驶车辆，决定是否准予保释使用统计数据作为输入。更轻浮的是，我们可以让亚历克斯打开咖啡机。

幸运的是，我们还远没有一个有知觉的人工智能系统准备操纵它的人类创造者（或烧掉他们的咖啡）。首先，人工智能系统是以一种特定的、面向目标的方式设计、训练和部署的。虽然他们的行为可能会给人一种普遍智能的错觉，但设计的基础是规则、启发式和统计模型的结合。第二，目前人工通用智能的工具根本不存在，它们能够自我改进、自我推理，并且能够在尝试解决一般任务的同时修改、扩展和改进自己的体系结构。

一个更紧迫的问题是人工智能在我们日常生活中的应用。卡车司机和店员完成的许多琐碎的工作很可能也将是自动化的。农场机器人可能会降低有机农业的成本，但它们也会自动收割作业。工业革命的这一阶段可能对社会的大部分地区产生深远的影响，因为卡车司机和店员是许多国家最常见的工作之一。此外，统计模型在不加注意地应用时，可能会导致种族、性别或年龄偏见，如果自动驱动相应的决策，则会引起对程序公平性的合理关注。重要的是要确保小心使用这些算法。就我们今天所知，这比恶意的超级智能毁灭人类的潜力更为紧迫。

## 特点

到目前为止，我们已经广泛地讨论了机器学习，它既是人工智能的一个分支，也是人工智能的一种方法。虽然深度学习是机器学习的一个子集，但是令人眼花缭乱的算法和应用使我们很难评估深度学习的具体成分。这和试图确定比萨饼所需的配料一样困难，因为几乎每种成分都是可替代的。

如前所述，机器学习可以使用数据来学习输入和输出之间的转换，例如在语音识别中将音频转换为文本。在这样做时，通常需要以适合算法的方式表示数据，以便将这种表示转换为输出。
*深度学习正是这个意思
它的模型学习许多转换层，每一层都提供一个层次的表示。例如，靠近输入的层可以表示数据的低级细节，而接近分类输出的层可以表示用于区分的更抽象的概念。由于表征学习的目的是寻找表征本身，因此深层学习可以称为多级表征学习。

到目前为止，我们讨论的问题，例如从原始音频信号中学习，图像的原始像素值，或者任意长度的句子与外语中的对应句子之间的映射，都是深度学习优于传统机器学习方法的问题。事实证明，这些多层模型能够以以前的工具所不能的方式处理低级的感知数据。可以说，深度学习方法中最显著的共同点是使用端到端培训。也就是说，与其基于单独调整的组件组装系统，不如构建系统，然后联合调整它们的性能。例如，在计算机视觉中，科学家们习惯于将特征工程的过程与建立机器学习模型的过程分开。Canny边缘检测器:cite:`Canny.1987`和Lowe's SIFT特征提取器:cite:`Lowe.2004`作为将图像映射到特征向量的算法，在过去的十年里占据了至高无上的地位。在过去的日子里，将机器学习应用于这些问题的关键部分是提出人工设计的方法，将数据转换为某种适合于浅层模型的形式。不幸的是，与一个算法自动执行的数百万个选择的一致性评估相比，人类通过独创性所能完成的事情很少。当深度学习开始时，这些特征抽取器被自动调谐的过滤器所取代，产生了更高的精确度。

因此，深度学习的一个关键优势是它不仅取代了传统学习管道末端的浅层模型，而且还取代了特征工程的劳动密集型过程。此外，通过替换许多特定领域的预处理，深度学习消除了许多以前将计算机视觉、语音识别、自然语言处理、医学信息学和其他应用领域分开的界限，为解决各种问题提供了一套统一的工具。

除了端到端的培训，我们正在经历从参数统计描述到完全非参数模型的转变。当数据稀缺时，人们需要依靠简化对现实的假设来获得有用的模型。当数据丰富时，可以用更准确地拟合实际情况的非参数模型来代替。在某种程度上，这反映了物理学在上个世纪中叶随着计算机的出现所经历的进步。现在人们可以求助于相关偏微分方程的数值模拟，而不是用手来求解电子行为的参数近似。这导致了更精确的模型，尽管常常以牺牲可解释性为代价。

另一个不同于以往的工作是接受次优解，处理非凸非线性优化问题，并愿意在证明之前尝试。这种新发现的处理统计问题的经验主义，加上人才的迅速涌入，导致了实用算法的快速发展，尽管在许多情况下是以修改和重新发明已经存在了几十年的工具为代价的。

最终，deeplearning社区以跨学术界和企业界共享工具、发布许多优秀的库、统计模型和经过培训的开放源码网络而自豪。正是本着这种精神，形成这本书的笔记本可以免费分发和使用。我们努力降低每个人学习深度学习的障碍，希望我们的读者能从中受益。

## 摘要

* 机器学习研究计算机系统如何利用经验（通常是数据）来提高特定任务的性能。它结合了统计学、数据挖掘和优化的思想。通常，它被用作实现人工智能解决方案的一种手段。
* 表征学习作为机器学习的一类，其研究的重点是如何自动找到合适的数据表示方式。深度学习是通过学习多层次的转换来进行的多层次的表征学习。
* 深度学习不仅取代了传统机器学习管道末端的浅层模型，而且取代了特征工程的劳动密集型过程。
* 最近在深度学习方面取得的许多进展都是由廉价传感器和互联网规模的应用所产生的大量数据以及计算方面的重大进展（主要是通过gpu）来触发的。
* 整个系统的优化是获得高性能的关键环节。有效的深度学习框架的可用性使得设计和实施这一方法变得非常容易。

## 练习

1. 您当前正在编写的代码的哪些部分可以“学习”，即通过学习和自动确定代码中所做的设计选择来改进？你的代码是否包含启发式设计选择？
1. 您遇到的哪些问题有许多解决它们的例子，但没有具体的方法来自动化它们？这些可能是使用深度学习的主要候选者。
1. 把人工智能的发展看作一场新的工业革命，算法和数据之间的关系是什么？它类似于蒸汽机和煤吗？根本区别是什么？
1. 你还可以在哪里应用端到端的培训方法，比如:numref:`fig_ml_loop`，物理，工程和计量经济学？

[Discussions](https://discuss.d2l.ai/t/22)
