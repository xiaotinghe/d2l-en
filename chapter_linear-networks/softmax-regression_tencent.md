# SoftMax回归
:label:`sec_softmax`

在:numref:`sec_linear_regression`中，我们引入了线性回归，在:numref:`sec_linear_scratch`中从头开始实现，在:numref:`sec_linear_concise`中再次使用深度学习框架的高级API来完成繁重的任务。

当我们想要回答*多少？或者*多少？*问题时，回归就是我们伸手去拿的锤子。如果你想预测一所房子将以多少美元(价格)出售，或者一支棒球队可能会赢得多少场比赛，或者一个病人在出院前将继续住院的天数，那么你可能正在寻找一个回归模型。

在实践中，我们更感兴趣的是*分类*：问的不是“多少”，而是“哪一个”：

* 此电子邮件属于垃圾邮件文件夹还是收件箱？
* 该客户更有可能*注册*还是*不注册*订阅服务？
* 这幅图描绘的是驴、狗、猫还是公鸡？
* 阿斯顿接下来最有可能看哪部电影？

通俗地说，机器学习从业者过载了*分类*这个词来描述两个微妙的不同问题：(I)那些我们只对类别(类)的硬分配感兴趣的问题；(Ii)那些我们希望进行软分配的问题，即评估每个类别适用的概率。这一区别往往会变得模糊，部分原因是，通常情况下，即使我们只关心硬分配，我们仍然使用进行软分配的模型。

## 分类问题
:label:`subsec_classification-problem`

为了熟悉一下，让我们从一个简单的图像分类问题开始。这里，每个输入由$2\times2$灰度图像组成。我们可以用单个标量表示每个像素值，从而得到四个特征$x_1, x_2, x_3, x_4$。此外，让我们假设每个图像属于类别“猫”、“鸡”和“狗”中的一个。

接下来，我们必须选择如何表示标签。我们有两个明显的选择。也许最自然的冲动是选择$y \in \{1, 2, 3\}$，其中整数分别表示$\{\text{dog}, \text{cat}, \text{chicken}\}$。这是在计算机上“存储”这类信息的一种很好的方式。如果这些类别之间有某种自然的顺序，例如，如果我们试图预测$\{\text{baby}, \text{toddler}, \text{adolescent}, \text{young adult}, \text{adult}, \text{geriatric}\}$，那么将这个问题转换为回归并保持这种格式的标签可能是有意义的。

但是，一般的分类问题并不具有类之间的自然顺序。幸运的是，统计学家很久以前就发明了一种表示分类数据的简单方式：“一热编码”。一次热编码是一个矢量，它的分量和我们的类别一样多。与特定实例类别对应的组件设置为1，所有其他组件设置为0。在我们的例子中，标签$y$将是一个三维矢量，其中$(1, 0, 0)$对应于“猫”，$(0, 1, 0)$对应于“鸡”，$(0, 0, 1)$对应于“狗”：

$$y \in \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}.$$

## 网络架构

为了估计与所有可能的类相关联的条件概率，我们需要一个具有多个输出的模型，每个类一个输出。为了解决线性模型的分类问题，我们需要与输出一样多的仿射函数。每个输出将对应于它自己的仿射函数。在我们的示例中，因为我们有4个特征和3个可能的输出类别，所以我们将需要12个标量来表示权重($w$带下标)，3个标量来表示偏差($b$带下标)。对于每个输入，我们计算这三个*logit*、$o_1, o_2$和$o_3$：

$$
\begin{aligned}
o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\
o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\
o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.
\end{aligned}
$$

我们可以用:numref:`fig_softmaxreg`中所示的神经网络图来描述这个计算。与线性回归一样，Softmax回归也是单层神经网络。由于每个输出($o_1, o_2$和$o_3$)的计算依赖于所有输入($x_1$、$x_2$、$x_3$和$x_4$)，因此SoftMAX回归的输出层也可以描述为完全连接层。

![Softmax regression is a single-layer neural network.](../img/softmaxreg.svg)
:label:`fig_softmaxreg`

为了更简洁地表示模型，我们可以使用线性代数表示法。在矢量形式中，我们得到$\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$，这是一种更适合数学和编写代码的形式。注意，我们已经将我们的所有权重收集到$3 \times 4$矩阵中，并且对于给定数据示例$\mathbf{x}$的特征，我们的输出由我们的权重乘以我们的输入特征加上我们的偏差$\mathbf{b}$的矩阵向量乘积给出。

## SoftMax操作
:label:`subsec_softmax_operation`

我们在这里要采取的主要方法是将我们模型的输出解释为概率。我们将优化参数以产生最大化观测数据可能性的概率。然后，为了生成预测，我们将设置一个阈值，例如，选择具有最大预测概率的标签。

正式地说，我们希望任何输出$\hat{y}_j$被解释为给定项目属于类$j$的概率。然后我们可以选择具有最大产值的类别作为我们的预测$\operatorname*{argmax}_j y_j$。例如，如果$\hat{y}_1$、$\hat{y}_2$和$\hat{y}_3$分别为0.1、0.8和0.1，则我们预测类别2(在我们的示例中)表示“鸡”。

您可能会建议我们将LOGITS $o$直接解释为我们感兴趣的输出。然而，将线性层的输出直接解释为概率存在一些问题。一方面，没有任何因素限制这些数字的总和为1。另一方面，根据输入的不同，它们可以取负值。这些违反了:numref:`sec_prob`中提出的概率基本公理

要将我们的输出解释为概率，我们必须保证(即使在新数据上)它们是非负的，并且总和为1。此外，我们需要一个鼓励模型忠实估计概率的训练目标。在分类器输出0.5的所有实例中，我们希望这些示例中有一半实际上属于预测类。这是一个称为*校准*的属性。

1959年，社会科学家R.Duncan Luce在“选择模型”的背景下发明了“Softmax函数”，它正是做到了这一点。要将我们的逻辑转换为非负数并求和为1，同时要求模型保持可微，我们首先对每个Logit求幂(确保非负性)，然后除以它们的和(确保它们的和为1)：

$$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{where}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}. $$
:eqlabel:`eq_softmax_y_and_o`

很容易看到$\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1$,$0 \leq \hat{y}_j \leq 1$代表全部$j$。因此，$\hat{\mathbf{y}}$是适当的概率分布，其元素值可以相应地解释。注意，软最大操作不改变逻辑$\mathbf{o}$之间的排序，其仅仅是确定分配给每个类别的概率的软最大之前的值。因此，在预测过程中，我们仍然可以通过以下方式挑选出最有可能的类别

$$
\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.
$$

尽管Softmax是一个非线性函数，但Softmax回归的输出仍然由输入特征的仿射变换确定；因此，Softmax回归是线性模型。

## 小批量问题的矢量化研究
:label:`subsec_softmax_vectorization`

为了提高计算效率并利用GPU，我们通常对小批量数据执行向量计算。假设给我们一个小批次$\mathbf{X}$的示例，其特征维度(输入数量)为$d$，批次大小为$n$。此外，假设输出中有$q$个类别。则小批量特征$\mathbf{X}$在$\mathbb{R}^{n \times d}$，加权$\mathbf{W} \in \mathbb{R}^{d \times q}$，并且偏差满足$\mathbf{b} \in \mathbb{R}^{1\times q}$。

$$ \begin{aligned} \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O}). \end{aligned} $$
:eqlabel:`eq_minibatch_softmax_reg`

这将主导运算加速为矩阵-矩阵乘积$\mathbf{X} \mathbf{W}$，而不是如果我们一次处理一个示例，我们将执行的矩阵-向量乘积。由于$\mathbf{X}$中的每一行代表一个数据示例，因此SOFTMAX操作本身可以*按行*计算：对于$\mathbf{O}$的每一行，对所有条目求幂，然后按总和对它们进行归一化。在:eqref:`eq_minibatch_softmax_reg`的求和$\mathbf{X} \mathbf{W} + \mathbf{b}$期间触发广播，小批量记录$\mathbf{O}$和输出概率$\hat{\mathbf{Y}}$都是$n \times q$矩阵。

## 损失函数

接下来，我们需要一个损失函数来度量我们预测的概率的质量。我们将依赖最大似然估计，这与我们在线性回归(:numref:`subsec_normal_distribution_and_squared_loss`)中为均方误差目标提供概率证明时遇到的概念完全相同。

### 对数似然

SOFTMAX函数向我们提供向量$\hat{\mathbf{y}}$，我们可以将其解释为给定任何输入$\mathbf{x}$(例如，$\hat{y}_1$=$P(y=\text{cat} \mid \mathbf{x})$)的每个类别的估计条件概率。假设整个数据集$\{\mathbf{X}, \mathbf{Y}\}$具有$n$个示例，其中由$i$索引的示例由特征向量$\mathbf{x}^{(i)}$和一个热点标签向量$\mathbf{y}^{(i)}$组成。在给定以下特征的情况下，我们可以通过检查根据我们的模型的实际类的概率来将估计值与实际值进行比较：

$$
P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).
$$

根据最大似然估计，我们最大化$P(\mathbf{Y} \mid \mathbf{X})$，这相当于最小化负对数似然：

$$
-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
= \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}),
$$

其中，对于超过$q$个类别的任何一对标签$\mathbf{y}$和模型预测$\hat{\mathbf{y}}$，损失函数$l$是

$$ l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j. $$
:eqlabel:`eq_l_cross_entropy`

由于后面解释的原因，:eqref:`eq_l_cross_entropy`中的损失函数通常被称为“交叉熵损失”。由于$\mathbf{y}$是长度为$q$的一热向量，因此其所有坐标$j$上的和对于除一项之外的所有项都为零。因为所有$\hat{y}_j$都是预测概率，所以它们的对数永远不会大于$0$。因此，如果我们以*确定性*正确地预测实际标签，即如果实际标签$\mathbf{y}$的预测概率为$P(\mathbf{y} \mid \mathbf{x}) = 1$，则损失函数不能进一步最小化。请注意，这通常是不可能的。例如，数据集中可能存在标签噪声(某些示例可能标记错误)。当输入特征没有足够的信息来完美地对每个示例进行分类时，也可能是不可能的。

### Softmax及其导数
:label:`subsec_softmax_and_derivatives`

由于软最大值和相应的损耗是如此常见，因此有必要更好地了解它是如何计算的。将:eqref:`eq_softmax_y_and_o`插入:eqref:`eq_l_cross_entropy`中的损失定义，并使用最大软值的定义，我们得到：

$$
\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\
&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
\end{aligned}
$$

为了更好地理解所发生的事情，请考虑关于任何logit $o_j$的导数。我们会得到

$$
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
$$

换句话说，导数是由我们的模型分配的概率(由Softmax操作表示)与实际发生的概率(由One-hotLabel向量中的元素表示)之间的差值。从这个意义上说，它与我们在回归中看到的非常相似，在回归中，梯度是观察值$y$和估计值$\hat{y}$之间的差值。这不是巧合。在任何指数族(见[online appendix on distributions](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/distributions.html))模型中，对数似然的梯度正好由这一项给出。这一事实使得在实践中计算梯度变得容易。

### 交叉熵损失

现在考虑这样的情况，我们观察的不只是单个结果，而是整个结果的分布。我们可以对标签$\mathbf{y}$使用与之前相同的表示。唯一的区别是，我们现在有一个通用的概率向量，比方说$(0.1, 0.2, 0.7)$，而不是只包含二进制条目的向量，比方说$(0, 0, 1)$。我们之前用来定义:eqref:`eq_l_cross_entropy`年度损失$l$的数学仍然很好，只是解释稍微普遍一些。它是标签上分配的损失的期望值。这种损失被称为“交叉熵损失”，它是分类问题中最常用的损失之一。我们可以通过仅仅介绍信息论的基础知识来揭开这个名字的神秘面纱。如果你想了解信息论的更多细节，你可以进一步参考[online appendix on information theory](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html)。

## 信息论基础
:label:`subsec_info_theory_basics`

*信息论*处理编码、解码、传输、
并以尽可能简洁的形式处理信息(也称为数据)。

### 熵

信息论的核心思想是将数据中的信息量量化。这个数量对我们压缩数据的能力造成了严格的限制。在信息论中，这个量被称为分布$P$的*熵*，并且它由下面的方程式捕获：

$$H[P] = \sum_j - P(j) \log P(j).$$
:eqlabel:`eq_softmax_reg_entropy`

信息论的一个基本定理指出，为了对从分布$P$随机抽取的数据进行编码，我们需要至少$H[P]$个“NAT”来对其进行编码。如果你想知道什么是“NAT”，它等同于比特，但是当使用基数为$e$的代码而不是基数为2的代码时，它等同于比特。因此，一个NAT是$\frac{1}{\log(2)} \approx 1.44$比特。

### 叛乱分子

您可能想知道压缩与预测有什么关系。假设我们有一个想要压缩的数据流。如果我们总是很容易预测下一个令牌，那么这个数据就很容易压缩！以流中的每个令牌始终采用相同值的极端示例为例。那是一条非常无聊的数据流！而且它不仅无聊，而且也很容易预测。因为它们总是相同的，所以我们不必传输任何信息来传递流的内容。易于预测，易于压缩。

然而，如果我们不能完美地预测每一件事，那么我们有时可能会感到惊讶。当我们分配一个概率较低的事件时，我们的惊讶会更大。克劳德·香农选择了$\log \frac{1}{P(j)} = -\log P(j)$来量化一个人在观察事件时的“惊讶”程度$j$，并将其(主观)概率定为$P(j)$。那么，当一个人分配了与数据生成过程真正匹配的正确概率时，:eqref:`eq_softmax_reg_entropy`中定义的熵就是“预期的惊喜”。

### 交叉熵再认识

所以，如果熵是知道真实概率的人所经历的惊讶程度，那么你可能会想，什么是交叉熵？从*$P$*到*$Q$的交叉熵*表示为$H(P, Q)$，是具有主观概率$Q$的观察者在看到根据概率$P$实际生成的数据时的预期惊喜。当交叉熵为$P=Q$时，可能达到最低交叉熵。在这种情况下，从$P$到$Q$的交叉熵是$H(P, P)= H(P)$。

简而言之，我们可以用两种方式来考虑交叉熵分类目标：(I)最大化观察数据的可能性；以及(Ii)最小化传送标签所需的惊喜(从而减少比特数)。

## 模型预测与评价

在对Softmax回归模型进行训练后，给定任何示例特征，就可以预测每个输出类的概率。通常，我们使用预测概率最高的类作为输出类。如果预测与实际类别(标签)一致，则预测是正确的。在实验的下一部分中，我们将使用*准确性*来评估模型的性能。这等于正确预测数与总预测数之比。

## 摘要

* Softmax操作获取一个向量，并将其映射为概率。
* SoftMax回归适用于分类问题。它在Softmax操作中使用输出类的概率分布。
* 交叉熵是衡量两个概率分布之间差异的一个很好的量度。它测量在给定模型的情况下对数据进行编码所需的位数。

## 练习

1. 我们可以更深入地探讨指数族和Softmax之间的联系。
    1. 计算软最大值的交叉熵损失$l(\mathbf{y},\hat{\mathbf{y}})$的二阶导数。
    1. 计算$\mathrm{softmax}(\mathbf{o})$给出的分布的方差，并表明它与上面计算的二阶导数相匹配。
1. 假设我们有三个概率相等的类别，即概率向量为$(\frac{1}{3}, \frac{1}{3}, \frac{1}{3})$。
    1. 如果我们试图为它设计一个二进制代码，会有什么问题呢？
    1. 你能设计出更好的代码吗？提示：如果我们尝试对两个独立的观察结果进行编码，会发生什么情况？如果我们将$n$个观测数据联合编码，会怎么样？
1. 对于上面介绍的映射，Softmax是一个用词不当的词(但深度学习领域的每个人都在使用它)。实际的软最大值定义为$\mathrm{RealSoftMax}(a, b) = \log (\exp(a) + \exp(b))$。
    1. 证明那$\mathrm{RealSoftMax}(a, b) > \mathrm{max}(a, b)$。
    1. 证明这一点在$\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b)$的情况下是成立的，前提是$\lambda > 0$。
    1. 显示出$\lambda \to \infty$美元我们有$\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b) \to \mathrm{max}(a, b)$美元。
    1. 软敏是什么样子的？
    1. 将其扩展到两个以上的数字。

[Discussions](https://discuss.d2l.ai/t/46)
