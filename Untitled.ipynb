{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "repo = git.Repo('.')#.branch()\n",
    "#repository.commit(branch='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['another-new-one', 'master', 'translate']\n"
     ]
    }
   ],
   "source": [
    "print([str(b) for b in repo.branches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.Head \"refs/heads/translate\">"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.branches[2].checkout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "git = repo.git\n",
    "result = git.diff('upstream/release','translate','--stat=120').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'diff --git a/chapter_convolutional-modern/alexnet_baidu.md b/chapter_convolutional-modern/alexnet_baidu.md\\nnew file mode 100644\\nindex 00000000..e4f62e67\\n--- /dev/null\\n+++ b/chapter_convolutional-modern/alexnet_baidu.md\\n@@ -0,0 +1,278 @@\\n+# 深卷积神经网络\\n+:label:`sec_alexnet`\\n+\\n+尽管cnn在LeNet的引入后在计算机视觉和机器学习领域中很有名，但它们并没有立即主导这个领域。虽然LeNet在早期的小数据集上取得了很好的效果，但是在更大、更真实的数据集上训练cnn的性能和可行性还有待于确定。事实上，在上世纪90年代初到2012年分水岭结果之间的大部分时间里，神经网络往往被其他机器学习方法所超越，比如支持向量机。\\n+\\n+对于计算机视觉来说，这种比较也许不公平。也就是说，尽管卷积网络的输入由原始像素值或经过轻微处理（例如通过居中）的像素值组成，但从业者永远不会将原始像素输入到传统模型中。相反，典型的计算机视觉管道由人工工程特征提取管道组成。这些功能不是*学习功能*而是*精心设计的*。大部分的进步来自于对特性有了更聪明的想法，学习算法常常被后置之脑后。\\n+\\n+虽然上世纪90年代就有了一些神经网络加速器，但它们还不足以制造出具有大量参数的深层多通道多层cnn。此外，数据集仍然相对较小。除了这些障碍，训练神经网络的关键技巧，包括参数初始化启发式、随机梯度下降的巧妙变体、非压缩激活函数和有效的正则化技术仍然缺失。\\n+\\n+因此，与训练*端到端*（像素到分类）系统不同，经典管道看起来更像这样：\\n+\\n+1. 获取一个有趣的数据集。在早期，这些数据集需要昂贵的传感器（当时，100万像素的图像是最先进的）。\\n+2. 根据光学、几何学和其他分析工具的一些知识，以及偶尔对幸运的研究生的偶然发现，用手工制作的特征对数据集进行预处理。\\n+3. 通过一组标准的特征提取程序（如SIFT（标度不变特征变换）:cite:`Lowe.2004`、SURF（加速鲁棒特征）:cite:`Bay.Tuytelaars.Van-Gool.2006`或任何数量的其他手动调节的管道来输入数据。\\n+4. 将结果表示转储到您最喜欢的分类器中，例如线性模型或内核方法，以训练分类器。\\n+\\n+如果你和机器学习研究人员交谈，他们相信机器学习既重要又美丽。优雅的理论证明了各种量词的性质。机器学习是一个蓬勃发展、严谨且非常有用的领域。然而，如果你和计算机视觉研究人员交谈，你会听到一个完全不同的故事。他们会告诉你，图像识别的肮脏事实是，推动进步的是特征，而不是学习算法。计算机视觉研究人员有理由相信，稍微大一点或更干净一点的数据集或稍微改进的特征提取管道比任何学习算法对最终精度的影响要大得多。\\n+\\n+## 学习表征\\n+\\n+另一种预测事态发展的方法是，管道最重要的部分是代表性。直到2012年，这种代表性都是机械计算出来的。事实上，设计一套新的特征函数，改进结果，并编写方法是一种突出的论文体裁。SIFT :cite:`Lowe.2004`、SURF :cite:`Bay.Tuytelaars.Van-Gool.2006`、HOG（定向梯度直方图）:cite:`Dalal.Triggs.2005`、[bags of visual words](https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision)和类似的特征提取程序占据了主导地位。\\n+\\n+另一组研究人员，包括Yann LeCun、Geoff Hinton、Yoshua Bengio、Andrew Ng、Shun ichi Amari和Juergen Schmidhuber，有不同的计划。他们认为特征本身应该被学习。此外，他们还认为，为了合理地复杂化，特征应该由多个共同学习的层组成，每个层都有可学习的参数。在图像的情况下，最低层可能检测边缘、颜色和纹理。事实上，亚历克斯·克里兹夫斯基、伊利亚·萨茨克弗和杰夫·辛顿提出了一种新的CNN变体，\\n+*亚历克内特*，\\n+在2012年ImageNet挑战赛中取得了优异的表现。AlexNet以Alex Krizhevsky的名字命名，他是ImageNet分类论文:cite:`Krizhevsky.Sutskever.Hinton.2012`的第一作者。\\n+\\n+有趣的是，在网络的最底层，模型学习了一些类似于传统滤波器的特征抽取器。:numref:`fig_filters`是从AlexNet论文:cite:`Krizhevsky.Sutskever.Hinton.2012`复制的，描述了低级图像描述符。\\n+\\n+![Image filters learned by the first layer of AlexNet.](../img/filters.png)\\n+:width:`400px`\\n+:label:`fig_filters`\\n+\\n+网络中的更高层可能建立在这些表示的基础上，以表示更大的结构，如眼睛、鼻子、草叶等等。更高的层次可能代表整个物体，如人、飞机、狗或飞盘。最终，最终的隐藏状态学习图像的紧凑表示，该图像概括了其内容，从而使属于不同类别的数据易于分离。\\n+\\n+虽然多层次cnn的最终突破出现在2012年，但一组核心研究人员致力于这一想法，多年来一直试图学习视觉数据的分层表示。2012年的最终突破可归因于两个关键因素。\\n+\\n+### 缺失成分：数据\\n+\\n+具有多层的深层模型需要大量的数据才能进入这样一种状态：它们显著优于基于凸优化的传统方法（如线性和核方法）。然而，考虑到计算机的存储容量有限，传感器的相对开销，以及90年代相对紧张的研究预算，大多数研究都依赖于微小的数据集。许多论文涉及UCI收集的数据集，其中许多只包含数百或（少数）数千幅在非自然环境下以低分辨率拍摄的图像。\\n+\\n+2009年，ImageNet数据集发布，向研究人员提出了挑战，要求他们从100万个样本中学习模型，其中1000个样本来自1000个不同类别的对象。由李飞飞（Fei-Fei-Li）领导的研究人员介绍了这一数据集，利用谷歌图像搜索（Google Image Search）对每一类图像进行预筛选，并利用Amazon-Mechanical-Turk众包管道来确认每张图片是否属于相关类别。这种规模是前所未有的。这项被称为ImageNet挑战赛的相关竞赛推动了计算机视觉和机器学习研究的发展，挑战研究人员确定哪些模型在更大的范围内表现最好，而不是学者们之前所认为的。\\n+\\n+### 缺少的成分：硬件\\n+\\n+深度学习模型是计算周期的贪婪消费者。训练可能需要数百个时期，每次迭代都需要通过计算代价高昂的线性代数操作的许多层传递数据。这也是为什么在20世纪90年代和21世纪初，基于更有效优化凸目标的简单算法成为首选的主要原因之一。\\n+\\n+*图形处理单元*（GPU）被证明是一个游戏规则的改变者\\n+使深度学习成为可能。这些芯片早就被开发用来加速图形处理，从而使电脑游戏受益。特别是，它们被优化为高吞吐量$4 \\\\times 4$矩阵向量产品，这是许多计算机图形任务所需要的。幸运的是，这个数学与计算卷积层所需的数学惊人地相似。大约在那个时候，NVIDIA和ATI已经开始为通用计算操作优化gpu，甚至把它们作为通用gpu*来销售。\\n+\\n+为了提供一些直觉，考虑一下现代微处理器（CPU）的核心。每一个核心都是相当强大的运行在一个高时钟频率和运动大型缓存（高达数兆字节的L3）。每个内核都非常适合执行各种指令，具有分支预测器、深管道和其他使其能够运行各种程序的各种各样的功能。然而，这种明显的优势也是它的致命弱点：通用核心的制造成本非常高。它们需要大量的芯片面积、复杂的支持结构（内存接口、内核之间的缓存逻辑、高速互连等等），而且它们在任何单个任务上都相对较差。现代笔记本电脑最多有4核，即使是高端服务器也很少超过64核，仅仅是因为它的性价比不高。\\n+\\n+相比之下，gpu由$100 \\\\sim 1000$个小的处理元素组成（NVIDIA、ATI、ARM和其他芯片供应商之间的细节有所不同），通常被分成更大的组（NVIDIA称之为翘曲）。虽然每个内核都相对较弱，有时甚至以低于1GHz的时钟频率运行，但正是这些内核的总数使GPU比CPU快几个数量级。例如，NVIDIA最近一代的Volta为每个芯片提供了高达120 TFlop的专用指令（对于更通用的指令，最高可达24 TFlop），而cpu的浮点性能到目前为止还没有超过1 TFlop。之所以可以这样做，原因其实很简单：首先，功耗往往会随时钟频率呈二次方增长。因此，对于一个运行速度快4倍的CPU内核（一个典型的数字），您可以使用16个GPU内核，其速度是$1/4$，其性能是$16 \\\\times 1/4 = 4$倍。此外，GPU内核要简单得多（事实上，在很长一段时间内，它们甚至不能执行通用代码），这使得它们更节能。最后，深度学习中的许多操作需要高内存带宽。再说一次，gpu在这里闪耀着至少是cpu宽度10倍的总线。\\n+\\n+回到2012年。当亚历克斯·克里兹夫斯基（Alex Krizhevsky）和伊利亚·萨茨克弗（Ilya Sutskever）实现了一个可以在GPU硬件上运行的深度CNN时，一个重大突破出现了。他们意识到cnn中的计算瓶颈，卷积和矩阵乘法，都是可以在硬件上并行化的操作。使用两个nvidiagtx580和3GB内存，他们实现了快速卷积。代码[cuda-convnet](https://code.google.com/archive/p/cuda-convnet/)已经足够好了，几年来它一直是行业标准，并推动了深度学习热潮的头几年。\\n+\\n+## 亚历克斯内特\\n+\\n+采用8层CNN的AlexNet以惊人的优势赢得了2012年ImageNet大规模视觉识别挑战赛。该网络首次表明，通过学习获得的特征可以超越人工设计的特征，打破了以往计算机视觉的范式。\\n+\\n+AlexNet和LeNet的架构非常相似，如:numref:`fig_alexnet`所示。请注意，我们提供了一个稍微精简的版本，消除了2012年需要的一些设计怪癖，使模型适合两个小型gpu。\\n+\\n+![From LeNet (left) to AlexNet (right).](../img/alexnet.svg)\\n+:label:`fig_alexnet`\\n+\\n+AlexNet和LeNet的设计理念非常相似，但也存在显著差异。首先，AlexNet比相对较小的LeNet5要深得多。AlexNet由八层组成：五个卷积层，两个完全连接的隐藏层和一个完全连接的输出层。其次，AlexNet使用ReLU而不是sigmoid作为其激活函数。让我们深入研究下面的细节。\\n+\\n+### 建筑\\n+\\n+在AlexNet的第一层，卷积窗口的形状是$11\\\\times11$。由于ImageNet中的大多数图像比MNIST图像高10倍以上，因此ImageNet数据中的对象往往占据更多的像素。因此，需要一个更大的卷积窗口来捕获目标。第二层中的卷积窗形状被缩减为$5\\\\times5$，然后是$3\\\\times3$。此外，在第一层、第二层和第五层之后，网络增加了最大的池层，窗口形状为$3\\\\times3$，步长为2。此外，AlexNet的卷积通道是LeNet的10倍。\\n+\\n+在最后一个卷积层之后有两个完全连接的层，有4096个输出。这两个巨大的完全连接层产生了将近1GB的模型参数。由于早期gpu内存有限，原有的AlexNet采用了双数据流设计，使得每个gpu只负责存储和计算模型的一半。幸运的是，现在GPU内存相对充裕，所以我们现在很少需要跨GPU分解模型（我们版本的AlexNet模型在这方面偏离了原始论文）。\\n+\\n+### 激活函数\\n+\\n+此外，AlexNet将sigmoid激活函数改为更简单的ReLU激活函数。一方面，ReLU激活函数的计算更简单。例如，它没有在sigmoid激活函数中找到的求幂运算。另一方面，当使用不同的参数初始化方法时，ReLU激活函数使模型训练更加容易。这是因为，当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。相反，ReLU激活函数在正区间的梯度总是1。因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。\\n+\\n+### 容量控制和预处理\\n+\\n+AlexNet通过dropout（:numref:`sec_dropout`）控制全连接层的模型复杂度，而LeNet只使用权重衰减。为了进一步扩充数据，AlexNet的训练循环增加了大量的图像增强，如翻转、剪切和颜色变化。这使得模型更健壮，更大的样本量有效地减少了过度拟合。我们将在:numref:`sec_image_augmentation`中更详细地讨论数据扩充。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+net = nn.Sequential()\\n+# Here, we use a larger 11 x 11 window to capture objects. At the same time,\\n+# we use a stride of 4 to greatly reduce the height and width of the output.\\n+# Here, the number of output channels is much larger than that in LeNet\\n+net.add(nn.Conv2D(96, kernel_size=11, strides=4, activation=\\'relu\\'),\\n+        nn.MaxPool2D(pool_size=3, strides=2),\\n+        # Make the convolution window smaller, set padding to 2 for consistent\\n+        # height and width across the input and output, and increase the\\n+        # number of output channels\\n+        nn.Conv2D(256, kernel_size=5, padding=2, activation=\\'relu\\'),\\n+        nn.MaxPool2D(pool_size=3, strides=2),\\n+        # Use three successive convolutional layers and a smaller convolution\\n+        # window. Except for the final convolutional layer, the number of\\n+        # output channels is further increased. Pooling layers are not used to\\n+        # reduce the height and width of input after the first two\\n+        # convolutional layers\\n+        nn.Conv2D(384, kernel_size=3, padding=1, activation=\\'relu\\'),\\n+        nn.Conv2D(384, kernel_size=3, padding=1, activation=\\'relu\\'),\\n+        nn.Conv2D(256, kernel_size=3, padding=1, activation=\\'relu\\'),\\n+        nn.MaxPool2D(pool_size=3, strides=2),\\n+        # Here, the number of outputs of the fully-connected layer is several\\n+        # times larger than that in LeNet. Use the dropout layer to mitigate\\n+        # overfitting\\n+        nn.Dense(4096, activation=\\'relu\\'), nn.Dropout(0.5),\\n+        nn.Dense(4096, activation=\\'relu\\'), nn.Dropout(0.5),\\n+        # Output layer. Since we are using Fashion-MNIST, the number of\\n+        # classes is 10, instead of 1000 as in the paper\\n+        nn.Dense(10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+\\n+net = nn.Sequential(\\n+    # Here, we use a larger 11 x 11 window to capture objects. At the same\\n+    # time, we use a stride of 4 to greatly reduce the height and width of the\\n+    # output. Here, the number of output channels is much larger than that in\\n+    # LeNet\\n+    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\\n+    nn.MaxPool2d(kernel_size=3, stride=2),\\n+    # Make the convolution window smaller, set padding to 2 for consistent\\n+    # height and width across the input and output, and increase the number of\\n+    # output channels\\n+    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\\n+    nn.MaxPool2d(kernel_size=3, stride=2),\\n+    # Use three successive convolutional layers and a smaller convolution\\n+    # window. Except for the final convolutional layer, the number of output\\n+    # channels is further increased. Pooling layers are not used to reduce the\\n+    # height and width of input after the first two convolutional layers\\n+    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\\n+    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\\n+    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\\n+    nn.MaxPool2d(kernel_size=3, stride=2),\\n+    nn.Flatten(),\\n+    # Here, the number of outputs of the fully-connected layer is several\\n+    # times larger than that in LeNet. Use the dropout layer to mitigate\\n+    # overfitting\\n+    nn.Linear(6400, 4096), nn.ReLU(),\\n+    nn.Dropout(p=0.5),\\n+    nn.Linear(4096, 4096), nn.ReLU(),\\n+    nn.Dropout(p=0.5),\\n+    # Output layer. Since we are using Fashion-MNIST, the number of classes is\\n+    # 10, instead of 1000 as in the paper\\n+    nn.Linear(4096, 10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+def net():\\n+    return tf.keras.models.Sequential([\\n+        # Here, we use a larger 11 x 11 window to capture objects. At the same\\n+        # time, we use a stride of 4 to greatly reduce the height and width of\\n+        # the output. Here, the number of output channels is much larger than\\n+        # that in LeNet\\n+        tf.keras.layers.Conv2D(filters=96, kernel_size=11, strides=4,\\n+                               activation=\\'relu\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\\n+        # Make the convolution window smaller, set padding to 2 for consistent\\n+        # height and width across the input and output, and increase the\\n+        # number of output channels\\n+        tf.keras.layers.Conv2D(filters=256, kernel_size=5, padding=\\'same\\',\\n+                               activation=\\'relu\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\\n+        # Use three successive convolutional layers and a smaller convolution\\n+        # window. Except for the final convolutional layer, the number of\\n+        # output channels is further increased. Pooling layers are not used to\\n+        # reduce the height and width of input after the first two\\n+        # convolutional layers\\n+        tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding=\\'same\\',\\n+                               activation=\\'relu\\'),\\n+        tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding=\\'same\\',\\n+                               activation=\\'relu\\'),\\n+        tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding=\\'same\\',\\n+                               activation=\\'relu\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\\n+        tf.keras.layers.Flatten(),\\n+        # Here, the number of outputs of the fully-connected layer is several\\n+        # times larger than that in LeNet. Use the dropout layer to mitigate\\n+        # overfitting\\n+        tf.keras.layers.Dense(4096, activation=\\'relu\\'),\\n+        tf.keras.layers.Dropout(0.5),\\n+        tf.keras.layers.Dense(4096, activation=\\'relu\\'),\\n+        tf.keras.layers.Dropout(0.5),\\n+        # Output layer. Since we are using Fashion-MNIST, the number of\\n+        # classes is 10, instead of 1000 as in the paper\\n+        tf.keras.layers.Dense(10)\\n+    ])\\n+```\\n+\\n+我们构造了一个高度和宽度都为224的单通道数据实例来观察每一层的输出形状。它与:numref:`fig_alexnet`中的AlexNet架构相匹配。\\n+\\n+```{.python .input}\\n+X = np.random.uniform(size=(1, 1, 224, 224))\\n+net.initialize()\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.name, \\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+X = torch.randn(1, 1, 224, 224)\\n+for layer in net:\\n+    X=layer(X)\\n+    print(layer.__class__.__name__,\\'Output shape:\\\\t\\',X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.random.uniform((1, 224, 224, 1))\\n+for layer in net().layers:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__, \\'Output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+## 读取数据集\\n+\\n+尽管本文中AlexNet是在ImageNet上进行训练的，但我们在这里使用的是时尚MNIST，因为即使在现代GPU上，训练ImageNet模型以使其收敛可能需要数小时或数天的时间。将AlexNet直接应用于Fashion MNIST的一个问题是，它的图像分辨率（$28 \\\\times 28$像素）低于ImageNet图像。为了使工作正常，我们将它们增加到$224 \\\\times 224$（通常不是一个明智的做法，但我们在这里这样做是为了忠实于AlexNet架构）。我们使用`d2l.load_data_fashion_mnist`函数中的`resize`参数执行此调整。\\n+\\n+```{.python .input}\\n+#@tab all\\n+batch_size = 128\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\\n+```\\n+\\n+## 培训\\n+\\n+现在，我们可以开始训练亚历克内特了。与:numref:`sec_lenet`中的LeNet相比，这里的主要变化是使用更小的学习速率和更慢的训练，这是因为网络更深更广，图像分辨率更高，卷积更昂贵。\\n+\\n+```{.python .input}\\n+#@tab all\\n+lr, num_epochs = 0.01, 10\\n+d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+## 摘要\\n+\\n+* AlexNet的结构与LeNet相似，但使用了更多的卷积层和更大的参数空间来拟合大规模的ImageNet数据集。\\n+* 今天，AlexNet已经被更有效的体系结构所超越，但它是当今从浅层到深层网络的关键一步。\\n+* 尽管AlexNet的实现似乎只比LeNet多出几行，但学术界花了很多年才接受这一概念转变，并利用其出色的实验结果。这也是由于缺乏有效的计算工具。\\n+* Dropout、ReLU和预处理是实现计算机视觉任务出色性能的其他关键步骤。\\n+\\n+## 练习\\n+\\n+1. 试着增加纪元的数量。与LeNet相比，结果有什么不同？为什么？\\n+1. AlexNet对于时尚MNIST数据集来说可能太复杂了。\\n+    1. 尝试简化模型以加快训练速度，同时确保准确性不会显著下降。\\n+    1. 设计一个更好的模型，直接在$28 \\\\times 28$图像上工作。\\n+1. 修改批处理大小，并观察精度和GPU内存的变化。\\n+1. 分析了AlexNet的计算性能。\\n+    1. AlexNet的内存占用占主导地位的是什么？\\n+    1. 在AlexNet中计算的主导部分是什么？\\n+    1. 计算结果时内存带宽如何？\\n+1. 将dropout和ReLU应用于LeNet-5。改善了吗？预处理怎么样？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/75)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/76)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/276)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-modern/alexnet_tencent.md b/chapter_convolutional-modern/alexnet_tencent.md\\nnew file mode 100644\\nindex 00000000..f3e30a6e\\n--- /dev/null\\n+++ b/chapter_convolutional-modern/alexnet_tencent.md\\n@@ -0,0 +1,278 @@\\n+# 深卷积神经网络(AlexNet)\\n+:label:`sec_alexnet`\\n+\\n+虽然在引入LeNet之后，CNN在计算机视觉和机器学习领域广为人知，但它们并没有立即在该领域占据主导地位。虽然LENet在早期的小数据集上取得了很好的效果，但在更大、更现实的数据集上训练CNN的性能和可行性尚未确定。事实上，在20世纪90年代初和2012年分水岭结果之间的大部分时间里，神经网络经常被其他机器学习方法超越，比如支持向量机。\\n+\\n+对于计算机视觉来说，这种比较可能是不公平的。也就是说，尽管卷积网络的输入由原始或轻微处理(例如，通过居中)的像素值组成，但从业者永远不会将原始像素馈送到传统模型中。取而代之的是，典型的计算机视觉流水线由人工设计的特征提取流水线组成。这些功能不是“学习功能”，而是“精心制作”的。大部分的进步来自于对功能有了更聪明的想法，而学习算法往往被放在了事后的考虑中。\\n+\\n+虽然在20世纪90年代已经有了一些神经网络加速器，但它们还不足以制造具有大量参数的深部多通道、多层CNN。此外，数据集仍然相对较小。除了这些障碍之外，训练神经网络的关键技巧，包括参数初始化启发式、随机梯度下降的巧妙变体、非挤压激活函数和有效的正则化技术，仍然缺乏。\\n+\\n+因此，与训练“端到端”(像素到分类)系统不同，经典管道看起来更像这样：\\n+\\n+1. 获取有趣的数据集。在早期，这些数据集需要昂贵的传感器(当时，100万像素的图像是最先进的)。\\n+2. 根据光学、几何学、其他分析工具的一些知识，偶尔根据幸运的研究生的偶然发现，用手工制作的特征对数据集进行预处理。\\n+3. 通过一组标准的特征提取器，例如SIFT(比例不变特征变换):cite:`Lowe.2004`、冲浪(加速稳健特征):cite:`Bay.Tuytelaars.Van-Gool.2006`或任何数量的其他手动调谐管道来馈送数据。\\n+4. 将生成的表示形式转储到您最喜欢的分类器(可能是线性模型或内核方法)中，以训练分类器。\\n+\\n+如果你与机器学习研究人员交谈，他们认为机器学习既重要又美好。精妙的理论证明了各种量词的性质。机器学习领域欣欣向荣，严谨，而且非常有用。然而，如果你与计算机视觉研究人员交谈，你会听到一个非常不同的故事。他们会告诉你，图像识别的肮脏真相是，推动进步的是特征，而不是学习算法。计算机视觉研究人员理所当然地认为，比起任何学习算法，稍微更大或更干净的数据集或稍微改进的特征提取管道对最终精度的影响要大得多。\\n+\\n+## 学习表征\\n+\\n+另一种预测现状的方法是，管道中最重要的部分是代表性。直到2012年，这个表示都是机械计算的。事实上，设计一组新的特征函数，改进结果，并撰写方法是一种突出的论文体裁。SIFT :cite:`Lowe.2004`、冲浪:cite:`Bay.Tuytelaars.Van-Gool.2006`、HOG(定向梯度直方图):cite:`Dalal.Triggs.2005`、[bags of visual words](https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision)和类似的特征提取器占据了主导地位。\\n+\\n+另一组研究人员，包括Yann LeCun，Geoff Hinton，Yoshua Bengio，Andrew Ng，Shun-ichi Amari和Juergen Schmidhuber，有不同的计划。他们认为特征本身应该被学习。此外，他们认为，要想变得相当复杂，这些特征应该由多个共同学习的层分层组成，每个层都有可学习的参数。在图像的情况下，最低层可能会检测边缘、颜色和纹理。事实上，Alex Krizhevsky，Ilya Sutskever和Geoff Hinton提出了CNN的新变体，\\n+*AlexNet*，\\n+在2012 ImageNet挑战赛中取得优异表现。Alexnet是以亚历克斯·克里日夫斯基(Alex Krizhevsky)的名字命名的，他是突破性的图像网分类论文:cite:`Krizhevsky.Sutskever.Hinton.2012`的第一作者。\\n+\\n+有趣的是，在网络的最低层，该模型学习了类似于一些传统过滤器的特征提取器。:numref:`fig_filters`是从ALEXNET纸:cite:`Krizhevsky.Sutskever.Hinton.2012`再现的，并且描述了较低级别的图像描述符。\\n+\\n+![Image filters learned by the first layer of AlexNet.](../img/filters.png)\\n+:width:`400px`\\n+:label:`fig_filters`\\n+\\n+网络中的更高层可能建立在这些表示的基础上，以表示更大的结构，如眼睛、鼻子、草叶等。甚至更高层可能代表整个物体，如人、飞机、狗或飞盘。最终，最终的隐藏状态学习图像的紧凑表示，该图像汇总其内容，以便容易地分离属于不同类别的数据。\\n+\\n+虽然多层CNN的最终突破出现在2012年，但一个核心的研究小组一直致力于这一想法，多年来一直试图学习视觉数据的分层表示法。2012年的最终突破可以归因于两个关键因素。\\n+\\n+### 缺少配料：数据\\n+\\n+具有多层的深层模型需要大量数据才能进入其性能明显优于基于凸优化的传统方法(例如，线性方法和核方法)的区域。然而，考虑到计算机的存储能力有限，传感器的相对费用，以及20世纪90年代相对紧缩的研究预算，大多数研究都依赖于微小的数据集。许多论文介绍了UCI数据集的收集，其中许多只包含数百或(几个)数千张在非自然环境下拍摄的低分辨率图像。\\n+\\n+2009年，ImageNet数据集发布，挑战研究人员从100万个例子中学习模型，每个例子来自1000个不同类别的物体。由引入这一数据集的李飞飞(Fei-fei Li)领导的研究人员利用谷歌图像搜索(Google Image Search)对每个类别的大型候选集进行预过滤，并使用亚马逊机械土耳其(Amazon Mechanical Turk)众包管道来确认每张图像是否属于相关的类别。这个规模是史无前例的。这项名为ImageNet Challenges的相关比赛推动了计算机视觉和机器学习的研究，挑战研究人员找出哪些模型在比学者之前认为的规模更大的范围内表现最好。\\n+\\n+### 缺少的成分：硬件\\n+\\n+深度学习模型是计算周期的贪婪消费者。训练可能需要数百个时代，并且每次迭代都需要通过计算代价高昂的多层线性代数运算来传递数据。这就是为什么在20世纪90年代和21世纪初，基于更高效的优化凸面目标的简单算法被首选的主要原因之一。\\n+\\n+*事实证明，图形处理器*(GPU)改变了游戏规则\\n+使深度学习变得可行。这些芯片长期以来一直是为了加速图形处理而开发的，以使计算机游戏受益。特别是，它们针对高吞吐量的$4 \\\\times 4$矩阵矢量产品进行了优化，这些产品是许多计算机图形任务所需的。幸运的是，这个数学与计算卷积层所需的数学惊人地相似。大约在那个时候，NVIDIA和ATI已经开始针对一般计算操作优化GPU，甚至将其作为“通用GPU”(GPGPU)进行市场推广。\\n+\\n+为了提供一些直观的信息，考虑一下现代微处理器(CPU)的内核。每个内核都相当强大，以很高的时钟频率运行，并且具有大容量缓存(高达几兆字节的L3)。每个内核都非常适合执行范围广泛的指令，具有分支预测器、深度流水线和其他使其能够运行各种程序的功能。然而，这种明显的优势也是它的致命弱点：通用内核的建造成本非常高。它们需要大量的芯片面积、复杂的支持结构(内存接口、核心之间的缓存逻辑、高速互连等等)，并且它们在任何单一任务中都相对较差。现代笔记本电脑最多有4个核心，即使是高端服务器也很少超过64个核心，原因很简单，因为它不划算。\\n+\\n+相比之下，图形处理器由$100 \\\\sim 1000$个小的处理单元组成(细节在英伟达、ATI、ARM和其他芯片供应商之间略有不同)，通常被分成更大的组(英伟达称之为WARPS)。虽然每个内核都相对较弱，有时甚至以低于1 GHz的时钟频率运行，但正是这些内核的总数使GPU比CPU快了几个数量级。例如，NVIDIA最新一代的Volta为专用指令提供了每个芯片高达120TFlops的性能(对于更通用的指令提供了高达24TFlops)，而CPU的浮点性能到目前为止还没有超过1TFLOP。实现这一目标的原因其实很简单：首先，功耗往往会随着时钟频率呈“二次曲线”增长。因此，对于运行速度快4倍(一个典型数字)的cpu核心的功率预算，您可以使用速度为$1/4$的16个gpu核心，这将产生$16 \\\\times 1/4 = 4$倍的性能。此外，GPU内核要简单得多(事实上，在很长一段时间内，它们甚至不能*执行通用代码)，这使得它们更节能。最后，深度学习中的许多操作都需要很高的存储带宽。同样，GPU的总线宽度至少是CPU的10倍，在这里大放异彩。\\n+\\n+回到2012年。当Alex Krizhevsky和Ilya Sutskever实现了可以在GPU硬件上运行的深度CNN时，一个重大突破出现了。他们意识到，CNN中的计算瓶颈，卷积和矩阵乘法，都是可以在硬件上并行化的操作。使用两台内存为3 GB的NVIDIA GTX 580，他们实现了快速卷积。代码[cuda-convnet](https://code.google.com/archive/p/cuda-convnet/)足够好了，几年来它一直是行业标准，并在深度学习热潮的头几年提供了动力。\\n+\\n+## AlexNet\\n+\\n+AlexNet使用了一个8层的CNN，以惊人的优势赢得了ImageNet 2012大型视觉识别挑战赛(ImageNet Large Scale Visual Recognition Challest 2012)。这个网络首次表明，通过学习获得的特征可以超越人工设计的特征，打破了以前计算机视觉的范式。\\n+\\n+如:numref:`fig_alexnet`所示，AlexNet和LeNet的架构非常相似。请注意，我们提供了略微简化的AlexNet版本，消除了2012年需要的一些设计怪癖，以使该模型适合两个小型GPU。\\n+\\n+![From LeNet (left) to AlexNet (right).](../img/alexnet.svg)\\n+:label:`fig_alexnet`\\n+\\n+AlexNet和LeNet的设计理念非常相似，但也有重大差异。首先，AlexNet比相对较小的LeNet5要深得多。AlexNet由八个层组成：五个卷积层、两个完全连接的隐藏层和一个完全连接的输出层。第二，AlexNet使用RELU而不是Sigmoid作为其激活函数。让我们深入研究下面的细节。\\n+\\n+### 架构\\n+\\n+在Alexnet的第一层中，卷积窗口形状为$11\\\\times11$。由于ImageNet中的大多数图像都比MNIST图像高和宽十倍以上，因此ImageNet数据中的对象往往占用更多的像素。因此，需要更大的卷积窗口来捕获对象。第二层中的卷积窗口形状减小到$5\\\\times5$，然后是$3\\\\times3$。此外，在第一卷积层、第二卷积层和第五卷积层之后，网络增加了最大汇聚层，窗口形状为$3\\\\times3$，步长为2，而且ALEXNET的卷积信道比LENet多10倍。\\n+\\n+在最后一个卷积层之后，有两个具有4096个输出的完全连接层。这两个巨大的全连接层产生了近1 GB的模型参数。由于早期GPU的内存有限，最初的AlexNet采用了双数据流设计，因此它们的两个GPU中的每一个都只能负责存储和计算模型的一半。幸运的是，GPU内存现在相对充足，所以我们现在很少需要在GPU之间拆分模型(我们的AlexNet模型在这方面与最初的论文有所不同)。\\n+\\n+### 激活函数\\n+\\n+此外，AlexNet将Sigmoid激活功能改为更简单的REU激活功能。一方面，REU激活函数的计算更简单。例如，它没有在Sigmoid激活函数中找到的求幂运算。另一方面，当使用不同的参数初始化方法时，RELU激活功能使模型训练变得更容易。这是因为，当Sigmoid激活函数的输出非常接近0或1时，这些区域的梯度几乎为0，因此反向传播不能继续更新一些模型参数。相反，REU激活函数在正区间的梯度始终为1，因此，如果模型参数初始化不当，则S型函数在正区间可能获得几乎为0的梯度，从而不能有效地训练模型。\\n+\\n+### 容量控制和预处理\\n+\\n+AlexNet通过丢弃(:numref:`sec_dropout`)来控制完全连接层的模型复杂性，而LeNet仅使用权重衰减。为了进一步增加数据，AlexNet的训练循环添加了大量的图像增强功能，如翻转、裁剪和颜色更改。这使得模型更加稳健，更大的样本量有效地减少了过拟合。我们将在:numref:`sec_image_augmentation`中更详细地讨论数据增强。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+net = nn.Sequential()\\n+# Here, we use a larger 11 x 11 window to capture objects. At the same time,\\n+# we use a stride of 4 to greatly reduce the height and width of the output.\\n+# Here, the number of output channels is much larger than that in LeNet\\n+net.add(nn.Conv2D(96, kernel_size=11, strides=4, activation=\\'relu\\'),\\n+        nn.MaxPool2D(pool_size=3, strides=2),\\n+        # Make the convolution window smaller, set padding to 2 for consistent\\n+        # height and width across the input and output, and increase the\\n+        # number of output channels\\n+        nn.Conv2D(256, kernel_size=5, padding=2, activation=\\'relu\\'),\\n+        nn.MaxPool2D(pool_size=3, strides=2),\\n+        # Use three successive convolutional layers and a smaller convolution\\n+        # window. Except for the final convolutional layer, the number of\\n+        # output channels is further increased. Pooling layers are not used to\\n+        # reduce the height and width of input after the first two\\n+        # convolutional layers\\n+        nn.Conv2D(384, kernel_size=3, padding=1, activation=\\'relu\\'),\\n+        nn.Conv2D(384, kernel_size=3, padding=1, activation=\\'relu\\'),\\n+        nn.Conv2D(256, kernel_size=3, padding=1, activation=\\'relu\\'),\\n+        nn.MaxPool2D(pool_size=3, strides=2),\\n+        # Here, the number of outputs of the fully-connected layer is several\\n+        # times larger than that in LeNet. Use the dropout layer to mitigate\\n+        # overfitting\\n+        nn.Dense(4096, activation=\\'relu\\'), nn.Dropout(0.5),\\n+        nn.Dense(4096, activation=\\'relu\\'), nn.Dropout(0.5),\\n+        # Output layer. Since we are using Fashion-MNIST, the number of\\n+        # classes is 10, instead of 1000 as in the paper\\n+        nn.Dense(10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+\\n+net = nn.Sequential(\\n+    # Here, we use a larger 11 x 11 window to capture objects. At the same\\n+    # time, we use a stride of 4 to greatly reduce the height and width of the\\n+    # output. Here, the number of output channels is much larger than that in\\n+    # LeNet\\n+    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\\n+    nn.MaxPool2d(kernel_size=3, stride=2),\\n+    # Make the convolution window smaller, set padding to 2 for consistent\\n+    # height and width across the input and output, and increase the number of\\n+    # output channels\\n+    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\\n+    nn.MaxPool2d(kernel_size=3, stride=2),\\n+    # Use three successive convolutional layers and a smaller convolution\\n+    # window. Except for the final convolutional layer, the number of output\\n+    # channels is further increased. Pooling layers are not used to reduce the\\n+    # height and width of input after the first two convolutional layers\\n+    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\\n+    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\\n+    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\\n+    nn.MaxPool2d(kernel_size=3, stride=2),\\n+    nn.Flatten(),\\n+    # Here, the number of outputs of the fully-connected layer is several\\n+    # times larger than that in LeNet. Use the dropout layer to mitigate\\n+    # overfitting\\n+    nn.Linear(6400, 4096), nn.ReLU(),\\n+    nn.Dropout(p=0.5),\\n+    nn.Linear(4096, 4096), nn.ReLU(),\\n+    nn.Dropout(p=0.5),\\n+    # Output layer. Since we are using Fashion-MNIST, the number of classes is\\n+    # 10, instead of 1000 as in the paper\\n+    nn.Linear(4096, 10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+def net():\\n+    return tf.keras.models.Sequential([\\n+        # Here, we use a larger 11 x 11 window to capture objects. At the same\\n+        # time, we use a stride of 4 to greatly reduce the height and width of\\n+        # the output. Here, the number of output channels is much larger than\\n+        # that in LeNet\\n+        tf.keras.layers.Conv2D(filters=96, kernel_size=11, strides=4,\\n+                               activation=\\'relu\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\\n+        # Make the convolution window smaller, set padding to 2 for consistent\\n+        # height and width across the input and output, and increase the\\n+        # number of output channels\\n+        tf.keras.layers.Conv2D(filters=256, kernel_size=5, padding=\\'same\\',\\n+                               activation=\\'relu\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\\n+        # Use three successive convolutional layers and a smaller convolution\\n+        # window. Except for the final convolutional layer, the number of\\n+        # output channels is further increased. Pooling layers are not used to\\n+        # reduce the height and width of input after the first two\\n+        # convolutional layers\\n+        tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding=\\'same\\',\\n+                               activation=\\'relu\\'),\\n+        tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding=\\'same\\',\\n+                               activation=\\'relu\\'),\\n+        tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding=\\'same\\',\\n+                               activation=\\'relu\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\\n+        tf.keras.layers.Flatten(),\\n+        # Here, the number of outputs of the fully-connected layer is several\\n+        # times larger than that in LeNet. Use the dropout layer to mitigate\\n+        # overfitting\\n+        tf.keras.layers.Dense(4096, activation=\\'relu\\'),\\n+        tf.keras.layers.Dropout(0.5),\\n+        tf.keras.layers.Dense(4096, activation=\\'relu\\'),\\n+        tf.keras.layers.Dropout(0.5),\\n+        # Output layer. Since we are using Fashion-MNIST, the number of\\n+        # classes is 10, instead of 1000 as in the paper\\n+        tf.keras.layers.Dense(10)\\n+    ])\\n+```\\n+\\n+我们构造了一个高度和宽度均为224的单通道数据示例，以观察各层的输出形状。它与:numref:`fig_alexnet`中的Alexnet架构相匹配。\\n+\\n+```{.python .input}\\n+X = np.random.uniform(size=(1, 1, 224, 224))\\n+net.initialize()\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.name, \\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+X = torch.randn(1, 1, 224, 224)\\n+for layer in net:\\n+    X=layer(X)\\n+    print(layer.__class__.__name__,\\'Output shape:\\\\t\\',X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.random.uniform((1, 224, 224, 1))\\n+for layer in net().layers:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__, \\'Output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+## 正在读取数据集\\n+\\n+虽然本文中对AlexNet进行了ImageNet方面的培训，但我们在这里使用的是Fashion-MNIST，因为即使在现代GPU上训练ImageNet模型进行聚合也可能需要数小时或数天的时间。将Alexnet直接应用于Fashion-MNIST的问题之一是其图像的分辨率($28 \\\\times 28$像素)低于ImageNet图像。为了使其正常工作，我们将其上采样到$224 \\\\times 224$(通常不是一个明智的做法，但是我们在这里这样做是为了忠实于Alexnet架构)。我们使用`resize`函数中的`d2l.load_data_fashion_mnist`参数执行此大小调整。\\n+\\n+```{.python .input}\\n+#@tab all\\n+batch_size = 128\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\\n+```\\n+\\n+## 培训\\n+\\n+现在，我们可以开始训练AlexNet了。与:numref:`sec_lenet`的LENET相比，这里的主要变化是使用了更小的学习率和更慢的训练速度，这是因为网络更深更广，图像分辨率更高，卷积的成本更高。\\n+\\n+```{.python .input}\\n+#@tab all\\n+lr, num_epochs = 0.01, 10\\n+d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+## 摘要\\n+\\n+* AlexNet具有与LeNet相似的结构，但使用了更多的卷积层和更大的参数空间来适应大规模的ImageNet数据集。\\n+* 今天，AlexNet已经被更有效的体系结构超越，但它是当今使用的从浅层网络到深层网络的关键一步。\\n+* 虽然AlexNet的实现似乎只比LeNet多几行，但学术界花了很多年才接受这一概念变化，并利用其出色的实验结果。这也是因为缺乏有效的计算工具。\\n+* 辍学、重修和预处理是在计算机视觉任务中取得优异性能的其他关键步骤。\\n+\\n+## 练习\\n+\\n+1. 尝试增加纪元数。与乐网相比，结果有何不同？为什么？\\n+1. 对于Fashion-MNIST数据集来说，AlexNet可能太复杂了。\\n+    1. 尝试简化模型以使训练更快，同时确保准确度不会显著下降。\\n+    1. 设计一个更好的模型，可以直接处理$28 \\\\times 28$张图像。\\n+1. 修改批量大小，观察精度和GPU内存的变化。\\n+1. 分析了AlexNet的计算性能。\\n+    1. AlexNet内存占用的主要部分是什么？\\n+    1. 在AlexNet中，计算的主要部分是什么？\\n+    1. 计算结果时内存带宽如何？\\n+1. 将辍学和重修应用到LENet-5。情况好转了吗？预处理怎么样？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/75)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/76)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/276)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-modern/batch-norm_baidu.md b/chapter_convolutional-modern/batch-norm_baidu.md\\nnew file mode 100644\\nindex 00000000..4a5260f8\\n--- /dev/null\\n+++ b/chapter_convolutional-modern/batch-norm_baidu.md\\n@@ -0,0 +1,477 @@\\n+# 批次标准化\\n+:label:`sec_batch_norm`\\n+\\n+训练深层神经网络是困难的。而让它们在合理的时间内收敛可能会很棘手。在本节中，我们将介绍*批处理规范化*，这是一种流行而有效的技术，它可以持续加速深度网络:cite:`Ioffe.Szegedy.2015`的收敛。再加上剩余块（稍后将在:numref:`sec_resnet`中介绍），批处理规范化使从业者能够常规地训练超过100层的网络。\\n+\\n+## 训练深层网络\\n+\\n+为了激励批处理规范化，让我们回顾一下在训练机器学习模型和神经网络时出现的一些实际挑战。\\n+\\n+首先，关于数据预处理的选择通常会对最终结果产生巨大的影响。回想一下我们应用MLPs预测房价（:numref:`sec_kaggle_house`）。处理真实数据时，我们的第一步是标准化输入特征，使其平均值为零，方差为1。直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它将参数*一个先验的*放在一个相似的尺度上。\\n+\\n+第二，对于典型的MLP或CNN，在我们训练的过程中，中间层中的变量（例如MLP中的仿射变换输出）可能具有广泛变化的大小：沿着从输入到输出的层，跨同一层中的单元，以及随着时间的推移，由于我们对模型参数的更新。批处理标准化的发明者非正式地假设，这些变量分布中的这种偏移可能会阻碍网络的收敛。直观地说，我们可能会猜想，如果一个层的可变值是另一层的100倍，这可能需要对学习率进行补偿调整。\\n+\\n+第三，更深层次的网络很复杂，很容易过度拟合。这意味着正则化变得更加重要。\\n+\\n+批处理规范化应用于各个层（可选地，适用于所有层），其工作原理如下：在每个训练迭代中，我们首先通过减去它们的平均值并除以它们的标准差来规范化输入（批处理规范化的输入），其中两者都是基于当前小批量的统计数据进行估计的。接下来，我们应用比例系数和比例偏移。正是由于基于*batch*统计的*normalization*，才有了它的名字。\\n+\\n+请注意，如果我们尝试对大小为1的小批量应用批处理规范化，我们将无法了解任何内容。这是因为减去平均值后，每个隐藏单元的值都是0！正如您可能猜到的那样，由于我们花了一整节时间讨论批处理规范化，并且有足够大的小批量，因此该方法被证明是有效和稳定的。这里的一个要点是，当应用批处理规范化时，批大小的选择可能比没有批处理规范化更重要。\\n+\\n+在形式上，通过$\\\\mathbf{x} \\\\in \\\\mathcal{B}$表示批次标准化（$\\\\mathrm{BN}$）的输入，即来自小批次$\\\\mathcal{B}$的输入，批次标准化根据以下表达式转换$\\\\mathbf{x}$：\\n+\\n+$$\\\\mathrm{BN}(\\\\mathbf{x}) = \\\\boldsymbol{\\\\gamma} \\\\odot \\\\frac{\\\\mathbf{x} - \\\\hat{\\\\boldsymbol{\\\\mu}}_\\\\mathcal{B}}{\\\\hat{\\\\boldsymbol{\\\\sigma}}_\\\\mathcal{B}} + \\\\boldsymbol{\\\\beta}.$$\\n+:eqlabel:`eq_batchnorm`\\n+\\n+在:eqref:`eq_batchnorm`中，$\\\\hat{\\\\boldsymbol{\\\\mu}}_\\\\mathcal{B}$是样品平均值，$\\\\hat{\\\\boldsymbol{\\\\sigma}}_\\\\mathcal{B}$是小批量$\\\\mathcal{B}$的样品标准差。应用标准化后，得到的小批量产品的平均值和单位方差为零。因为单位方差的选择（相对于其他一些幻数）是任意选择，我们通常包括元素\\n+*刻度参数*$\\\\boldsymbol{\\\\gamma}$和*移位参数*$\\\\boldsymbol{\\\\beta}$\\n+与$\\\\mathbf{x}$形状相同。请注意，$\\\\boldsymbol{\\\\gamma}$和$\\\\boldsymbol{\\\\beta}$是需要与其他模型参数一起学习的参数。\\n+\\n+因此，在训练期间，中间层的变量大小不能发散，因为批量标准化会主动地将其集中并重新调整到给定的平均值和大小（通过$\\\\hat{\\\\boldsymbol{\\\\mu}}_\\\\mathcal{B}$和${\\\\hat{\\\\boldsymbol{\\\\sigma}}_\\\\mathcal{B}}$）。从业者的一个直觉或智慧是批量标准化似乎允许更积极的学习率。\\n+\\n+形式上，我们计算$\\\\hat{\\\\boldsymbol{\\\\mu}}_\\\\mathcal{B}$和:eqref:`eq_batchnorm`中的$\\\\hat{\\\\boldsymbol{\\\\mu}}_\\\\mathcal{B}$，如下所示：\\n+\\n+$$\\\\begin{aligned} \\\\hat{\\\\boldsymbol{\\\\mu}}_\\\\mathcal{B} &= \\\\frac{1}{|\\\\mathcal{B}|} \\\\sum_{\\\\mathbf{x} \\\\in \\\\mathcal{B}} \\\\mathbf{x},\\\\\\\\\\n+\\\\hat{\\\\boldsymbol{\\\\sigma}}_\\\\mathcal{B}^2 &= \\\\frac{1}{|\\\\mathcal{B}|} \\\\sum_{\\\\mathbf{x} \\\\in \\\\mathcal{B}} (\\\\mathbf{x} - \\\\hat{\\\\boldsymbol{\\\\mu}}_{\\\\mathcal{B}})^2 + \\\\epsilon.\\\\end{aligned}$$\\n+\\n+请注意，我们在方差估计中添加了一个小常数$\\\\epsilon > 0$，以确保我们从不尝试除以零，即使在经验方差估计可能会消失的情况下。估计值$\\\\hat{\\\\boldsymbol{\\\\mu}}_\\\\mathcal{B}$和${\\\\hat{\\\\boldsymbol{\\\\sigma}}_\\\\mathcal{B}}$通过使用均值和方差的噪声估计来抵消标度问题。你可能认为这种噪音应该是个问题。事实证明，这实际上是有益的。\\n+\\n+这在深度学习中是一个反复出现的主题。由于理论上还没有很好描述的原因，优化中的各种噪声源通常导致更快的训练和更少的过度拟合：这种变化似乎是一种正则化形式。在一些初步研究中，:cite:`Teye.Azizpour.Smith.2018`和:cite:`Luo.Wang.Shao.ea.2018`分别将批规范化的性质与贝叶斯先验和惩罚相关联。特别是，这揭示了为什么批量标准化最适合$50 \\\\sim 100$范围内的中等小批量的难题。\\n+\\n+修正一个经过训练的模型，您可能会认为我们更喜欢使用整个数据集来估计均值和方差。一旦训练完成，为什么我们要根据同一图像所处的批次对同一图像进行不同的分类？在训练过程中，这种精确的计算是不可行的，因为每次我们更新模型时，所有数据示例的中间变量都会发生变化。然而，一旦模型被训练，我们就可以根据整个数据集计算每一层变量的均值和方差。事实上，这是使用批处理规范化的模型的标准实践，因此批处理规范化层在*训练模式*（通过小批量统计进行规范化）和在*预测模式*（通过数据集统计进行规范化）中的功能不同。\\n+\\n+我们现在准备看看批量标准化在实践中是如何工作的。\\n+\\n+## 批处理规范化层\\n+\\n+完全连接层和卷积层的批处理规范化实现略有不同。我们在下面讨论这两种情况。回想一下，批处理规范化与其他层之间的一个关键区别是，因为批处理规范化一次只能在一个完整的小批量上运行，所以我们不能像以前介绍其他层时那样忽略批处理维度。\\n+\\n+### 全连接层\\n+\\n+当对完全连接的层应用批处理规范化时，原始文件在仿射变换之后和非线性激活函数之前插入批处理规范化（以后的应用程序可能在激活函数之后插入批处理规范化）:cite:`Ioffe.Szegedy.2015`。用$\\\\mathbf{x}$表示对全连通层的输入，用$\\\\mathbf{x}$表示仿射变换（使用权参数$\\\\mathbf{W}$和偏置参数$\\\\phi$），用$\\\\phi$表示激活函数，我们可以将启用批量标准化的全连接层输出$\\\\mathbf{h}$的计算表示如下：\\n+\\n+$$\\\\mathbf{h} = \\\\phi(\\\\mathrm{BN}(\\\\mathbf{W}\\\\mathbf{x} + \\\\mathbf{b}) ).$$\\n+\\n+回想一下，均值和方差是在应用转换的*同一*小批量上计算的。\\n+\\n+### 卷积层\\n+\\n+类似地，对于卷积层，我们可以在卷积之后和非线性激活函数之前应用批标准化。当卷积有多个输出通道时，我们需要对这些通道的*每个*输出进行批量归一化，每个通道都有自己的尺度和移位参数，这两个参数都是标量。假设我们的小批量包含$m$个示例，对于每个通道，卷积的输出高度为$p$，宽度为$q$。对于卷积层，我们同时对每个输出通道的$m \\\\cdot p \\\\cdot q$个元素进行批处理归一化。因此，我们在计算平均值和方差时收集所有空间位置的值，然后在给定通道内应用相同的平均值和方差来规范化每个空间位置的值。\\n+\\n+### 预测过程中的批量标准化\\n+\\n+正如我们前面提到的，批处理规范化在训练模式和预测模式中的行为通常是不同的。首先，一旦我们训练了模型，样本均值和样本方差中的噪声就不再是理想的了。第二，我们可能没有计算每批标准化统计数据的奢侈。例如，我们可能需要应用我们的模型一次做出一个预测。\\n+\\n+通常，在训练之后，我们使用整个数据集来计算变量统计的稳定估计，然后在预测时修正它们。因此，批量标准化在培训和测试期间的表现是不同的。回想一下，辍学也表现出这一特点。\\n+\\n+## 从头开始实施\\n+\\n+下面，我们从零开始使用张量实现一个批处理规范化层。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import autograd, np, npx, init\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\\n+    # Use `autograd` to determine whether the current mode is training mode or\\n+    # prediction mode\\n+    if not autograd.is_training():\\n+        # If it is prediction mode, directly use the mean and variance\\n+        # obtained by moving average\\n+        X_hat = (X - moving_mean) / np.sqrt(moving_var + eps)\\n+    else:\\n+        assert len(X.shape) in (2, 4)\\n+        if len(X.shape) == 2:\\n+            # When using a fully-connected layer, calculate the mean and\\n+            # variance on the feature dimension\\n+            mean = X.mean(axis=0)\\n+            var = ((X - mean) ** 2).mean(axis=0)\\n+        else:\\n+            # When using a two-dimensional convolutional layer, calculate the\\n+            # mean and variance on the channel dimension (axis=1). Here we\\n+            # need to maintain the shape of `X`, so that the broadcasting\\n+            # operation can be carried out later\\n+            mean = X.mean(axis=(0, 2, 3), keepdims=True)\\n+            var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True)\\n+        # In training mode, the current mean and variance are used for the\\n+        # standardization\\n+        X_hat = (X - mean) / np.sqrt(var + eps)\\n+        # Update the mean and variance using moving average\\n+        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\\n+        moving_var = momentum * moving_var + (1.0 - momentum) * var\\n+    Y = gamma * X_hat + beta  # Scale and shift\\n+    return Y, moving_mean, moving_var\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+\\n+def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\\n+    # Use `is_grad_enabled` to determine whether the current mode is training\\n+    # mode or prediction mode\\n+    if not torch.is_grad_enabled():\\n+        # If it is prediction mode, directly use the mean and variance\\n+        # obtained by moving average\\n+        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\\n+    else:\\n+        assert len(X.shape) in (2, 4)\\n+        if len(X.shape) == 2:\\n+            # When using a fully-connected layer, calculate the mean and\\n+            # variance on the feature dimension\\n+            mean = X.mean(dim=0)\\n+            var = ((X - mean) ** 2).mean(dim=0)\\n+        else:\\n+            # When using a two-dimensional convolutional layer, calculate the\\n+            # mean and variance on the channel dimension (axis=1). Here we\\n+            # need to maintain the shape of `X`, so that the broadcasting\\n+            # operation can be carried out later\\n+            mean = X.mean(dim=(0, 2, 3), keepdim=True)\\n+            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\\n+        # In training mode, the current mean and variance are used for the\\n+        # standardization\\n+        X_hat = (X - mean) / torch.sqrt(var + eps)\\n+        # Update the mean and variance using moving average\\n+        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\\n+        moving_var = momentum * moving_var + (1.0 - momentum) * var\\n+    Y = gamma * X_hat + beta  # Scale and shift\\n+    return Y, moving_mean.data, moving_var.data\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+def batch_norm(X, gamma, beta, moving_mean, moving_var, eps):\\n+    # Compute reciprocal of square root of the moving variance element-wise\\n+    inv = tf.cast(tf.math.rsqrt(moving_var + eps), X.dtype)\\n+    # Scale and shift\\n+    inv *= gamma\\n+    Y = X * inv + (beta - moving_mean * inv)\\n+    return Y\\n+```\\n+\\n+我们现在可以创建一个适当的`BatchNorm`层。我们的层将保持适当的参数为规模`gamma`和班次`beta`，这两个将在训练过程中更新。此外，我们的层将保持均值和方差的移动平均值，以便在模型预测期间使用。\\n+\\n+抛开算法细节不谈，注意我们实现层的设计模式。通常，我们在一个单独的函数中定义数学，比如`batch_norm`。然后我们将这个功能集成到一个定制层中，该层的代码主要处理簿记事务，例如将数据移动到正确的设备上下文，分配和初始化任何必需的变量，跟踪移动平均值（这里是均值和方差），等等。这种模式可以将数学与样板代码完全分离。另外请注意，为了方便起见，我们不担心在这里自动推断输入形状，因此我们需要指定整个特征的数量。不用担心，深度学习框架中的高级批处理规范化api将为我们处理这些问题，我们稍后将对此进行演示。\\n+\\n+```{.python .input}\\n+class BatchNorm(nn.Block):\\n+    # `num_features`: the number of outputs for a fully-connected layer\\n+    # or the number of output channels for a convolutional layer. `num_dims`:\\n+    # 2 for a fully-connected layer and 4 for a convolutional layer\\n+    def __init__(self, num_features, num_dims, **kwargs):\\n+        super().__init__(**kwargs)\\n+        if num_dims == 2:\\n+            shape = (1, num_features)\\n+        else:\\n+            shape = (1, num_features, 1, 1)\\n+        # The scale parameter and the shift parameter (model parameters) are\\n+        # initialized to 1 and 0, respectively\\n+        self.gamma = self.params.get(\\'gamma\\', shape=shape, init=init.One())\\n+        self.beta = self.params.get(\\'beta\\', shape=shape, init=init.Zero())\\n+        # The variables that are not model parameters are initialized to 0\\n+        self.moving_mean = np.zeros(shape)\\n+        self.moving_var = np.zeros(shape)\\n+\\n+    def forward(self, X):\\n+        # If `X` is not on the main memory, copy `moving_mean` and\\n+        # `moving_var` to the device where `X` is located\\n+        if self.moving_mean.ctx != X.ctx:\\n+            self.moving_mean = self.moving_mean.copyto(X.ctx)\\n+            self.moving_var = self.moving_var.copyto(X.ctx)\\n+        # Save the updated `moving_mean` and `moving_var`\\n+        Y, self.moving_mean, self.moving_var = batch_norm(\\n+            X, self.gamma.data(), self.beta.data(), self.moving_mean,\\n+            self.moving_var, eps=1e-12, momentum=0.9)\\n+        return Y\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class BatchNorm(nn.Module):\\n+    # `num_features`: the number of outputs for a fully-connected layer\\n+    # or the number of output channels for a convolutional layer. `num_dims`:\\n+    # 2 for a fully-connected layer and 4 for a convolutional layer\\n+    def __init__(self, num_features, num_dims):\\n+        super().__init__()\\n+        if num_dims == 2:\\n+            shape = (1, num_features)\\n+        else:\\n+            shape = (1, num_features, 1, 1)\\n+        # The scale parameter and the shift parameter (model parameters) are\\n+        # initialized to 1 and 0, respectively\\n+        self.gamma = nn.Parameter(torch.ones(shape))\\n+        self.beta = nn.Parameter(torch.zeros(shape))\\n+        # The variables that are not model parameters are initialized to 0\\n+        self.moving_mean = torch.zeros(shape)\\n+        self.moving_var = torch.zeros(shape)\\n+\\n+    def forward(self, X):\\n+        # If `X` is not on the main memory, copy `moving_mean` and\\n+        # `moving_var` to the device where `X` is located\\n+        if self.moving_mean.device != X.device:\\n+            self.moving_mean = self.moving_mean.to(X.device)\\n+            self.moving_var = self.moving_var.to(X.device)\\n+        # Save the updated `moving_mean` and `moving_var`\\n+        Y, self.moving_mean, self.moving_var = batch_norm(\\n+            X, self.gamma, self.beta, self.moving_mean,\\n+            self.moving_var, eps=1e-5, momentum=0.9)\\n+        return Y\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class BatchNorm(tf.keras.layers.Layer):\\n+    def __init__(self, **kwargs):\\n+        super(BatchNorm, self).__init__(**kwargs)\\n+\\n+    def build(self, input_shape):\\n+        weight_shape = [input_shape[-1], ]\\n+        # The scale parameter and the shift parameter (model parameters) are\\n+        # initialized to 1 and 0, respectively\\n+        self.gamma = self.add_weight(name=\\'gamma\\', shape=weight_shape,\\n+            initializer=tf.initializers.ones, trainable=True)\\n+        self.beta = self.add_weight(name=\\'beta\\', shape=weight_shape,\\n+            initializer=tf.initializers.zeros, trainable=True)\\n+        # The variables that are not model parameters are initialized to 0\\n+        self.moving_mean = self.add_weight(name=\\'moving_mean\\',\\n+            shape=weight_shape, initializer=tf.initializers.zeros,\\n+            trainable=False)\\n+        self.moving_variance = self.add_weight(name=\\'moving_variance\\',\\n+            shape=weight_shape, initializer=tf.initializers.zeros,\\n+            trainable=False)\\n+        super(BatchNorm, self).build(input_shape)\\n+\\n+    def assign_moving_average(self, variable, value):\\n+        momentum = 0.9\\n+        delta = variable * momentum + value * (1 - momentum)\\n+        return variable.assign(delta)\\n+\\n+    @tf.function\\n+    def call(self, inputs, training):\\n+        if training:\\n+            axes = list(range(len(inputs.shape) - 1))\\n+            batch_mean = tf.reduce_mean(inputs, axes, keepdims=True)\\n+            batch_variance = tf.reduce_mean(tf.math.squared_difference(\\n+                inputs, tf.stop_gradient(batch_mean)), axes, keepdims=True)\\n+            batch_mean = tf.squeeze(batch_mean, axes)\\n+            batch_variance = tf.squeeze(batch_variance, axes)\\n+            mean_update = self.assign_moving_average(\\n+                self.moving_mean, batch_mean)\\n+            variance_update = self.assign_moving_average(\\n+                self.moving_variance, batch_variance)\\n+            self.add_update(mean_update)\\n+            self.add_update(variance_update)\\n+            mean, variance = batch_mean, batch_variance\\n+        else:\\n+            mean, variance = self.moving_mean, self.moving_variance\\n+        output = batch_norm(inputs, moving_mean=mean, moving_var=variance,\\n+            beta=self.beta, gamma=self.gamma, eps=1e-5)\\n+        return output\\n+```\\n+\\n+## 批标准化在LeNet中的应用\\n+\\n+为了了解如何在上下文中应用`BatchNorm`，下面我们将其应用于传统的LeNet模型（:numref:`sec_lenet`）。回想一下，批量规范化是在卷积层或完全连接层之后，但在相应的激活函数之前应用的。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nn.Conv2D(6, kernel_size=5),\\n+        BatchNorm(6, num_dims=4),\\n+        nn.Activation(\\'sigmoid\\'),\\n+        nn.MaxPool2D(pool_size=2, strides=2),\\n+        nn.Conv2D(16, kernel_size=5),\\n+        BatchNorm(16, num_dims=4),\\n+        nn.Activation(\\'sigmoid\\'),\\n+        nn.MaxPool2D(pool_size=2, strides=2),\\n+        nn.Dense(120),\\n+        BatchNorm(120, num_dims=2),\\n+        nn.Activation(\\'sigmoid\\'),\\n+        nn.Dense(84),\\n+        BatchNorm(84, num_dims=2),\\n+        nn.Activation(\\'sigmoid\\'),\\n+        nn.Dense(10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(\\n+    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(),\\n+    nn.MaxPool2d(kernel_size=2, stride=2),\\n+    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(),\\n+    nn.MaxPool2d(kernel_size=2, stride=2), nn.Flatten(),\\n+    nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),\\n+    nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),\\n+    nn.Linear(84, 10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+# Recall that this has to be a function that will be passed to `d2l.train_ch6`\\n+# so that model building or compiling need to be within `strategy.scope()` in\\n+# order to utilize the CPU/GPU devices that we have\\n+def net():\\n+    return tf.keras.models.Sequential([\\n+        tf.keras.layers.Conv2D(filters=6, kernel_size=5,\\n+                               input_shape=(28, 28, 1)),\\n+        BatchNorm(),\\n+        tf.keras.layers.Activation(\\'sigmoid\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\\n+        tf.keras.layers.Conv2D(filters=16, kernel_size=5),\\n+        BatchNorm(),\\n+        tf.keras.layers.Activation(\\'sigmoid\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\\n+        tf.keras.layers.Flatten(),\\n+        tf.keras.layers.Dense(120),\\n+        BatchNorm(),\\n+        tf.keras.layers.Activation(\\'sigmoid\\'),\\n+        tf.keras.layers.Dense(84),\\n+        BatchNorm(),\\n+        tf.keras.layers.Activation(\\'sigmoid\\'),\\n+        tf.keras.layers.Dense(10)]\\n+    )\\n+```\\n+\\n+我们会像以前一样在我们的网络上训练数据集。这个代码实际上与我们第一次训练LeNet（:numref:`sec_lenet`）时的代码完全相同。主要的区别是学习率大得多。\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+lr, num_epochs, batch_size = 1.0, 10, 256\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\\n+d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+lr, num_epochs, batch_size = 1.0, 10, 256\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\\n+net = d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+让我们看看从第一批处理规范化层学习到的scale参数`gamma`和shift参数`beta`。\\n+\\n+```{.python .input}\\n+net[1].gamma.data().reshape(-1,), net[1].beta.data().reshape(-1,)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net[1].gamma.reshape((-1,)), net[1].beta.reshape((-1,))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+tf.reshape(net.layers[1].gamma, (-1,)), tf.reshape(net.layers[1].beta, (-1,))\\n+```\\n+\\n+## 简明实施\\n+\\n+与我们刚刚定义的`BatchNorm`类相比，我们可以直接从深度学习框架中使用高级api中定义的`BatchNorm`类。我们的应用程序与上面的代码几乎相同。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nn.Conv2D(6, kernel_size=5),\\n+        nn.BatchNorm(),\\n+        nn.Activation(\\'sigmoid\\'),\\n+        nn.MaxPool2D(pool_size=2, strides=2),\\n+        nn.Conv2D(16, kernel_size=5),\\n+        nn.BatchNorm(),\\n+        nn.Activation(\\'sigmoid\\'),\\n+        nn.MaxPool2D(pool_size=2, strides=2),\\n+        nn.Dense(120),\\n+        nn.BatchNorm(),\\n+        nn.Activation(\\'sigmoid\\'),\\n+        nn.Dense(84),\\n+        nn.BatchNorm(),\\n+        nn.Activation(\\'sigmoid\\'),\\n+        nn.Dense(10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(\\n+    nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(),\\n+    nn.MaxPool2d(kernel_size=2, stride=2),\\n+    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(),\\n+    nn.MaxPool2d(kernel_size=2, stride=2), nn.Flatten(),\\n+    nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(),\\n+    nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),\\n+    nn.Linear(84, 10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def net():\\n+    return tf.keras.models.Sequential([\\n+        tf.keras.layers.Conv2D(filters=6, kernel_size=5,\\n+                               input_shape=(28, 28, 1)),\\n+        tf.keras.layers.BatchNormalization(),\\n+        tf.keras.layers.Activation(\\'sigmoid\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\\n+        tf.keras.layers.Conv2D(filters=16, kernel_size=5),\\n+        tf.keras.layers.BatchNormalization(),\\n+        tf.keras.layers.Activation(\\'sigmoid\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\\n+        tf.keras.layers.Flatten(),\\n+        tf.keras.layers.Dense(120),\\n+        tf.keras.layers.BatchNormalization(),\\n+        tf.keras.layers.Activation(\\'sigmoid\\'),\\n+        tf.keras.layers.Dense(84),\\n+        tf.keras.layers.BatchNormalization(),\\n+        tf.keras.layers.Activation(\\'sigmoid\\'),\\n+        tf.keras.layers.Dense(10),\\n+    ])\\n+```\\n+\\n+下面，我们使用相同的超参数来训练我们的模型。请注意，通常，高级API变型运行得更快，因为它的代码已经编译成C++或CUDA，而我们的自定义实现必须由Python来解释。\\n+\\n+```{.python .input}\\n+#@tab all\\n+d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+## 争议\\n+\\n+直观地说，批处理规范化被认为可以使优化环境更平滑。然而，在训练深层模型时，我们必须小心区分投机直觉和对我们观察到的现象的真实解释。回想一下，我们甚至不知道为什么更简单的深层神经网络（MLPs和传统cnn）一开始就能很好地推广。即使出现了辍学和权重衰减，它们仍然非常灵活，以至于无法通过传统的学习理论泛化保证来解释它们对未知数据的泛化能力。\\n+\\n+在最初提出批处理标准化的论文中，作者除了介绍一个强大而有用的工具外，还解释了它的工作原理：通过减少*内部协变量偏移*。据推测，作者所说的“内部协变量转移”指的是类似于上述直觉的东西，即变量值的分布在训练过程中会发生变化。然而，这种解释有两个问题：i）这种漂移与协变量漂移非常不同，这使得这个名字用词不当。ii）这种解释提供了一种不明确的直觉，但却留下了一个有待严格解释的问题，即“为什么这项技术准确地起作用”。在这本书中，我们的目的是传达实践者用来指导他们发展深层神经网络的直觉。然而，我们认为，重要的是将这些指导性直觉与既定的科学事实区分开来。最终，当你掌握了这些材料并开始撰写自己的研究论文时，你会想清楚地区分技术主张和直觉。\\n+\\n+在批处理标准化的成功之后，关于如何呈现机器学习研究的技术文献和更广泛的讨论中，关于内部协变量转移的解释一再出现。阿里·拉希米（Ali Rahimi）在接受2017年NeurIPS大会的时间测试奖（Test of Time Award）时发表了一篇令人难忘的演讲，他将“内部协变量转移”（internal covariate shift）作为焦点，将现代深度学习的实践比作炼金术。随后，在一份立场文件中对该示例进行了详细回顾，概述了机器学习中令人不安的趋势:cite:`Lipton.Steinhardt.2018`。另一些作者对批处理规范化的成功提出了另一种解释，一些人声称，尽管批处理规范化的成功表现出与原始论文:cite:`Santurkar.Tsipras.Ilyas.ea.2018`中声称的行为在某些方面是相反的。\\n+\\n+我们注意到，*内部协变量变化*并不比每年在机器学习技术文献中提出的数千个类似模糊的说法更值得批评。很可能，它作为这些辩论焦点的共鸣，要归功于它对目标受众的广泛认可。批处理规范化已经被证明是一种不可或缺的方法，应用于几乎所有已部署的图像分类器中，这篇介绍该技术的论文引文数万篇。\\n+\\n+## 摘要\\n+\\n+* 在模型训练过程中，批量归一化利用小批量的均值和标准差不断调整神经网络的中间输出，使整个神经网络各层的中间输出值更加稳定。\\n+* 完全连接层和卷积层的批处理规范化方法略有不同。\\n+* 批处理规范化层和丢失层一样，在训练模式和预测模式下具有不同的计算结果。\\n+* 批量规范化有许多有益的副作用，主要是正则化。另一方面，减少内部协变量变化的原始动机似乎不是一个有效的解释。\\n+\\n+## 练习\\n+\\n+1. 在批量标准化之前，是否可以从完全连接层或卷积层中删除偏差参数？为什么？\\n+1. 比较LeNet在批处理和不使用批处理标准化的情况下的学习率。\\n+    1. 描绘出训练和测试准确性的提高。\\n+    1. 你的学习率有多高？\\n+1. 我们需要在每一层进行批量标准化吗？尝试一下？\\n+1. 你能用批处理规范化来代替丢失吗？行为如何改变？\\n+1. 固定参数`beta`和`gamma`，观察分析结果。\\n+1. 查看来自高级api的`BatchNorm`的在线文档，以查看用于批处理规范化的其他应用程序。\\n+1. 研究思路：想想其他可以应用的规范化转换？你能应用概率积分变换吗？全秩协方差估计如何？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/83)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/84)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/330)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-modern/batch-norm_tencent.md b/chapter_convolutional-modern/batch-norm_tencent.md\\nnew file mode 100644\\nindex 00000000..8acedcdf\\n--- /dev/null\\n+++ b/chapter_convolutional-modern/batch-norm_tencent.md\\n@@ -0,0 +1,477 @@\\n+# 批量归一化\\n+:label:`sec_batch_norm`\\n+\\n+训练深度神经网络是困难的。而且，让它们在合理的时间内汇聚起来可能是一件棘手的事情。在这一部分中，我们将介绍“批量归一化”，这是一种流行且有效的技术，可持续加速深度网络:cite:`Ioffe.Szegedy.2015`的收敛。连同剩余块-稍后将在:numref:`sec_resnet`中介绍-批量标准化使从业者有可能例行训练百层以上的网络。\\n+\\n+## 训练深度网络\\n+\\n+为了促进批处理标准化，让我们回顾一下在训练机器学习模型和神经网络时出现的一些实际挑战。\\n+\\n+首先，关于数据预处理的选择通常会对最终结果产生巨大的影响。回想一下我们应用MLP来预测房价(:numref:`sec_kaggle_house`)。在处理真实数据时，我们的第一步是将输入要素标准化，使每个要素的平均值为0，方差为1。直观地说，这个标准化与我们的优化器配合得很好，因为它将参数“先验地”置于相似的规模。\\n+\\n+其次，对于典型的MLP或CNN，在我们训练时，中间层中的变量(例如，MLP中的仿射变换输出)可能会采用幅度变化很大的值：既沿着从输入到输出的层，跨同一层中的单元，也随着时间的推移，由于我们对模型参数的更新。批量归一化的发明者非正式地假设，这些变量分布中的这种漂移可能会阻碍网络的收敛。直观地说，我们可能会猜测，如果一层的变量值是另一层的100倍，这可能需要对学习率进行补偿性调整。\\n+\\n+第三，更深的网络很复杂，很容易过度拟合。这意味着正规化变得更加关键。\\n+\\n+批次归一化应用于各个层(可选地，应用于所有层)，其工作方式如下：在每次训练迭代中，我们首先通过减去它们的平均值并除以它们的标准差来归一化(批次归一化的)输入，其中两者都是基于当前小批次的统计来估计的。接下来，我们应用缩放系数和缩放偏移。正是由于这种基于*批量*统计的*归一化*，所以才有了*批量归一化*的名称。\\n+\\n+请注意，如果我们尝试对大小为1的小批应用批标准化，我们将无法了解任何内容。这是因为减去平均值后，每个隐藏单元的值为0！正如您可能猜到的那样，由于我们花了整整一节来进行批次规范化，并且有足够大的小批次，因此该方法被证明是有效和稳定的。这里要说明的一点是，在应用批处理标准化时，批处理大小的选择可能比没有批处理标准化更重要。\\n+\\n+形式上，用$\\\\mathbf{x} \\\\in \\\\mathcal{B}$表示对批次规格化($\\\\mathrm{BN}$)的输入，其来自小批次$\\\\mathcal{B}$，批次规格化根据以下表达式转换$\\\\mathbf{x}$：\\n+\\n+$$\\\\mathrm{BN}(\\\\mathbf{x}) = \\\\boldsymbol{\\\\gamma} \\\\odot \\\\frac{\\\\mathbf{x} - \\\\hat{\\\\boldsymbol{\\\\mu}}_\\\\mathcal{B}}{\\\\hat{\\\\boldsymbol{\\\\sigma}}_\\\\mathcal{B}} + \\\\boldsymbol{\\\\beta}.$$\\n+:eqlabel:`eq_batchnorm`\\n+\\n+在:eqref:`eq_batchnorm`中，$\\\\hat{\\\\boldsymbol{\\\\mu}}_\\\\mathcal{B}$是样本平均值，$\\\\hat{\\\\boldsymbol{\\\\sigma}}_\\\\mathcal{B}$是小批量$\\\\mathcal{B}$的样本标准偏差。在应用标准化之后，所得到的小批量的均值和单位方差为零。因为单位方差的选择(与其他一些幻数)是任意选择的，所以我们通常包括按元素\\n+*比例参数*$\\\\boldsymbol{\\\\gamma}$和*移位参数*$\\\\boldsymbol{\\\\beta}$\\n+它们的形状与$\\\\mathbf{x}$相同。请注意，$\\\\boldsymbol{\\\\gamma}$和$\\\\boldsymbol{\\\\beta}$是需要与其他模型参数一起学习的参数。\\n+\\n+因此，中间层的可变幅度在训练期间不能发散，因为批归一化主动地将它们居中并将它们重新缩放回给定的平均值和大小(通过$\\\\hat{\\\\boldsymbol{\\\\mu}}_\\\\mathcal{B}$和${\\\\hat{\\\\boldsymbol{\\\\sigma}}_\\\\mathcal{B}}$)。实践者的一个直觉或智慧是，批量标准化似乎允许更积极的学习率。\\n+\\n+形式上，我们:eqref:`eq_batchnorm`年度$\\\\hat{\\\\boldsymbol{\\\\mu}}_\\\\mathcal{B}$和${\\\\hat{\\\\boldsymbol{\\\\sigma}}_\\\\mathcal{B}}$计算如下：\\n+\\n+$$\\\\begin{aligned} \\\\hat{\\\\boldsymbol{\\\\mu}}_\\\\mathcal{B} &= \\\\frac{1}{|\\\\mathcal{B}|} \\\\sum_{\\\\mathbf{x} \\\\in \\\\mathcal{B}} \\\\mathbf{x},\\\\\\\\\\n+\\\\hat{\\\\boldsymbol{\\\\sigma}}_\\\\mathcal{B}^2 &= \\\\frac{1}{|\\\\mathcal{B}|} \\\\sum_{\\\\mathbf{x} \\\\in \\\\mathcal{B}} (\\\\mathbf{x} - \\\\hat{\\\\boldsymbol{\\\\mu}}_{\\\\mathcal{B}})^2 + \\\\epsilon.\\\\end{aligned}$$\\n+\\n+请注意，我们在方差估计中添加了一个小常数$\\\\epsilon > 0$，以确保即使在经验方差估计可能消失的情况下，我们也不会尝试除以零。估计$\\\\hat{\\\\boldsymbol{\\\\mu}}_\\\\mathcal{B}$和${\\\\hat{\\\\boldsymbol{\\\\sigma}}_\\\\mathcal{B}}$通过使用均值和方差的噪声估计来抵消缩放问题。你可能认为这种嘈杂应该是个问题。事实证明，这实际上是有益的。\\n+\\n+事实证明，这是深度学习中反复出现的主题。由于理论上还没有很好描述的原因，优化中的各种噪声源通常会导致更快的训练和更少的过度适应：这种变化似乎是正则化的一种形式。在一些初步研究中，:cite:`Teye.Azizpour.Smith.2018`和:cite:`Luo.Wang.Shao.ea.2018`分别将批次归一化的性质与贝叶斯先验和惩罚相关联。特别是，这揭示了为什么批次标准化对于$50 \\\\sim 100$范围内的中等小批量效果最好的谜题。\\n+\\n+固定一个经过训练的模型，您可能会认为我们更喜欢使用整个数据集来估计均值和方差。一旦训练完成，我们为什么要根据图像所在的批次，对相同的图像进行不同的分类呢？在训练过程中，这种精确的计算是不可行的，因为我们每次更新模型时，所有数据示例的中间变量都会发生变化。然而，一旦模型被训练，我们就可以基于整个数据集来计算每一层变量的均值和方差。事实上，这是采用批量归一化的模型的标准实践，因此批归一化层在*训练模式*(通过小批量统计进行归一化)和在*预测模式*(通过数据集统计进行归一化)中的功能不同。\\n+\\n+现在我们准备看看批处理标准化在实践中是如何工作的。\\n+\\n+## 批归一化图层\\n+\\n+完全连接层和卷积层的批归一化实现略有不同。我们将在下面讨论这两种情况。回想一下，批次规格化与其他层之间的一个关键区别是，因为批次规格化一次在一个完整的小批次上操作，所以我们不能像以前在引入其他层时那样忽略批次维度。\\n+\\n+### 完全连接层\\n+\\n+当将批归一化应用于完全连接的层时，原始论文在仿射变换之后并且在非线性激活函数之前插入批归一化(稍后的应用可以恰好在激活函数之后插入批归一化):cite:`Ioffe.Szegedy.2015`。用$\\\\mathbf{x}$表示对全连接层的输入，用$\\\\mathbf{W}\\\\mathbf{x} + \\\\mathbf{b}$表示仿射变换(具有权重参数$\\\\mathbf{W}$和偏置参数$\\\\mathbf{b}$)，用$\\\\phi$表示激活函数，我们可以将启用批归一化的全连接层输出$\\\\mathbf{h}$的计算表示如下：\\n+\\n+$$\\\\mathbf{h} = \\\\phi(\\\\mathrm{BN}(\\\\mathbf{W}\\\\mathbf{x} + \\\\mathbf{b}) ).$$\\n+\\n+回想一下，均值和方差是在应用转换的“相同”小批次上计算的。\\n+\\n+### 卷积层\\n+\\n+类似地，对于卷积层，我们可以在卷积之后和非线性激活函数之前应用批归一化。当卷积有多个输出通道时，我们需要对这些通道的*每个*个输出进行批量归一化，并且每个通道都有自己的标量和移位参数，这两个参数都是标量。假设我们的小批包含$m$个示例，并且对于每个通道，卷积的输出具有高度$p$和宽度$q$。对于卷积层，我们同时对每个输出通道的$m \\\\cdot p \\\\cdot q$个元素执行每批归一化。因此，我们在计算均值和方差时收集所有空间位置上的值，并因此在给定通道内应用相同的均值和方差来归一化每个空间位置的值。\\n+\\n+### 预测过程中的批次归一化\\n+\\n+正如我们前面提到的，批处理标准化在训练模式和预测模式中的行为通常不同。首先，一旦我们训练了模型，样本均值中的噪声和小批量估计每个样本所产生的样本方差就不再是我们想要的了。其次，我们可能没有计算每批归一化统计数据的奢侈。例如，我们可能需要应用我们的模型来一次做出一个预测。\\n+\\n+通常，在训练之后，我们使用整个数据集来计算变量统计的稳定估计，然后将它们固定在预测时间。因此，批处理标准化在训练期间和测试时的行为不同。回想一下，辍学也表现出这一特征。\\n+\\n+## 从头开始实施\\n+\\n+下面，我们从头开始实现一个带有张量的批归一化层。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import autograd, np, npx, init\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\\n+    # Use `autograd` to determine whether the current mode is training mode or\\n+    # prediction mode\\n+    if not autograd.is_training():\\n+        # If it is prediction mode, directly use the mean and variance\\n+        # obtained by moving average\\n+        X_hat = (X - moving_mean) / np.sqrt(moving_var + eps)\\n+    else:\\n+        assert len(X.shape) in (2, 4)\\n+        if len(X.shape) == 2:\\n+            # When using a fully-connected layer, calculate the mean and\\n+            # variance on the feature dimension\\n+            mean = X.mean(axis=0)\\n+            var = ((X - mean) ** 2).mean(axis=0)\\n+        else:\\n+            # When using a two-dimensional convolutional layer, calculate the\\n+            # mean and variance on the channel dimension (axis=1). Here we\\n+            # need to maintain the shape of `X`, so that the broadcasting\\n+            # operation can be carried out later\\n+            mean = X.mean(axis=(0, 2, 3), keepdims=True)\\n+            var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True)\\n+        # In training mode, the current mean and variance are used for the\\n+        # standardization\\n+        X_hat = (X - mean) / np.sqrt(var + eps)\\n+        # Update the mean and variance using moving average\\n+        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\\n+        moving_var = momentum * moving_var + (1.0 - momentum) * var\\n+    Y = gamma * X_hat + beta  # Scale and shift\\n+    return Y, moving_mean, moving_var\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+\\n+def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\\n+    # Use `is_grad_enabled` to determine whether the current mode is training\\n+    # mode or prediction mode\\n+    if not torch.is_grad_enabled():\\n+        # If it is prediction mode, directly use the mean and variance\\n+        # obtained by moving average\\n+        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\\n+    else:\\n+        assert len(X.shape) in (2, 4)\\n+        if len(X.shape) == 2:\\n+            # When using a fully-connected layer, calculate the mean and\\n+            # variance on the feature dimension\\n+            mean = X.mean(dim=0)\\n+            var = ((X - mean) ** 2).mean(dim=0)\\n+        else:\\n+            # When using a two-dimensional convolutional layer, calculate the\\n+            # mean and variance on the channel dimension (axis=1). Here we\\n+            # need to maintain the shape of `X`, so that the broadcasting\\n+            # operation can be carried out later\\n+            mean = X.mean(dim=(0, 2, 3), keepdim=True)\\n+            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\\n+        # In training mode, the current mean and variance are used for the\\n+        # standardization\\n+        X_hat = (X - mean) / torch.sqrt(var + eps)\\n+        # Update the mean and variance using moving average\\n+        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\\n+        moving_var = momentum * moving_var + (1.0 - momentum) * var\\n+    Y = gamma * X_hat + beta  # Scale and shift\\n+    return Y, moving_mean.data, moving_var.data\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+def batch_norm(X, gamma, beta, moving_mean, moving_var, eps):\\n+    # Compute reciprocal of square root of the moving variance element-wise\\n+    inv = tf.cast(tf.math.rsqrt(moving_var + eps), X.dtype)\\n+    # Scale and shift\\n+    inv *= gamma\\n+    Y = X * inv + (beta - moving_mean * inv)\\n+    return Y\\n+```\\n+\\n+我们现在可以创建一个合适的`BatchNorm`层。我们的层将为Scale `gamma`和Shift `beta`维护适当的参数，这两个参数都将在培训过程中更新。此外，我们的层将维护均值和方差的移动平均值，以供随后在模型预测期间使用。\\n+\\n+抛开算法细节不谈，请注意我们实现该层的设计模式。通常，我们在一个单独的函数中定义数学，比如`batch_norm`。然后，我们将此功能集成到一个自定义层中，该层的代码主要解决记账问题，例如将数据移动到正确的设备上下文、分配和初始化任何需要的变量、跟踪移动平均值(这里是平均值和方差)，等等。此模式支持将数学与样板代码完全分离。还要注意，为了方便起见，我们没有担心在这里自动推断输入形状，因此我们需要指定贯穿始终的特征数量。不要担心，深度学习框架中的高级批处理标准化API将为我们解决这一问题，我们稍后将演示这一点。\\n+\\n+```{.python .input}\\n+class BatchNorm(nn.Block):\\n+    # `num_features`: the number of outputs for a fully-connected layer\\n+    # or the number of output channels for a convolutional layer. `num_dims`:\\n+    # 2 for a fully-connected layer and 4 for a convolutional layer\\n+    def __init__(self, num_features, num_dims, **kwargs):\\n+        super().__init__(**kwargs)\\n+        if num_dims == 2:\\n+            shape = (1, num_features)\\n+        else:\\n+            shape = (1, num_features, 1, 1)\\n+        # The scale parameter and the shift parameter (model parameters) are\\n+        # initialized to 1 and 0, respectively\\n+        self.gamma = self.params.get(\\'gamma\\', shape=shape, init=init.One())\\n+        self.beta = self.params.get(\\'beta\\', shape=shape, init=init.Zero())\\n+        # The variables that are not model parameters are initialized to 0\\n+        self.moving_mean = np.zeros(shape)\\n+        self.moving_var = np.zeros(shape)\\n+\\n+    def forward(self, X):\\n+        # If `X` is not on the main memory, copy `moving_mean` and\\n+        # `moving_var` to the device where `X` is located\\n+        if self.moving_mean.ctx != X.ctx:\\n+            self.moving_mean = self.moving_mean.copyto(X.ctx)\\n+            self.moving_var = self.moving_var.copyto(X.ctx)\\n+        # Save the updated `moving_mean` and `moving_var`\\n+        Y, self.moving_mean, self.moving_var = batch_norm(\\n+            X, self.gamma.data(), self.beta.data(), self.moving_mean,\\n+            self.moving_var, eps=1e-12, momentum=0.9)\\n+        return Y\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class BatchNorm(nn.Module):\\n+    # `num_features`: the number of outputs for a fully-connected layer\\n+    # or the number of output channels for a convolutional layer. `num_dims`:\\n+    # 2 for a fully-connected layer and 4 for a convolutional layer\\n+    def __init__(self, num_features, num_dims):\\n+        super().__init__()\\n+        if num_dims == 2:\\n+            shape = (1, num_features)\\n+        else:\\n+            shape = (1, num_features, 1, 1)\\n+        # The scale parameter and the shift parameter (model parameters) are\\n+        # initialized to 1 and 0, respectively\\n+        self.gamma = nn.Parameter(torch.ones(shape))\\n+        self.beta = nn.Parameter(torch.zeros(shape))\\n+        # The variables that are not model parameters are initialized to 0\\n+        self.moving_mean = torch.zeros(shape)\\n+        self.moving_var = torch.zeros(shape)\\n+\\n+    def forward(self, X):\\n+        # If `X` is not on the main memory, copy `moving_mean` and\\n+        # `moving_var` to the device where `X` is located\\n+        if self.moving_mean.device != X.device:\\n+            self.moving_mean = self.moving_mean.to(X.device)\\n+            self.moving_var = self.moving_var.to(X.device)\\n+        # Save the updated `moving_mean` and `moving_var`\\n+        Y, self.moving_mean, self.moving_var = batch_norm(\\n+            X, self.gamma, self.beta, self.moving_mean,\\n+            self.moving_var, eps=1e-5, momentum=0.9)\\n+        return Y\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class BatchNorm(tf.keras.layers.Layer):\\n+    def __init__(self, **kwargs):\\n+        super(BatchNorm, self).__init__(**kwargs)\\n+\\n+    def build(self, input_shape):\\n+        weight_shape = [input_shape[-1], ]\\n+        # The scale parameter and the shift parameter (model parameters) are\\n+        # initialized to 1 and 0, respectively\\n+        self.gamma = self.add_weight(name=\\'gamma\\', shape=weight_shape,\\n+            initializer=tf.initializers.ones, trainable=True)\\n+        self.beta = self.add_weight(name=\\'beta\\', shape=weight_shape,\\n+            initializer=tf.initializers.zeros, trainable=True)\\n+        # The variables that are not model parameters are initialized to 0\\n+        self.moving_mean = self.add_weight(name=\\'moving_mean\\',\\n+            shape=weight_shape, initializer=tf.initializers.zeros,\\n+            trainable=False)\\n+        self.moving_variance = self.add_weight(name=\\'moving_variance\\',\\n+            shape=weight_shape, initializer=tf.initializers.zeros,\\n+            trainable=False)\\n+        super(BatchNorm, self).build(input_shape)\\n+\\n+    def assign_moving_average(self, variable, value):\\n+        momentum = 0.9\\n+        delta = variable * momentum + value * (1 - momentum)\\n+        return variable.assign(delta)\\n+\\n+    @tf.function\\n+    def call(self, inputs, training):\\n+        if training:\\n+            axes = list(range(len(inputs.shape) - 1))\\n+            batch_mean = tf.reduce_mean(inputs, axes, keepdims=True)\\n+            batch_variance = tf.reduce_mean(tf.math.squared_difference(\\n+                inputs, tf.stop_gradient(batch_mean)), axes, keepdims=True)\\n+            batch_mean = tf.squeeze(batch_mean, axes)\\n+            batch_variance = tf.squeeze(batch_variance, axes)\\n+            mean_update = self.assign_moving_average(\\n+                self.moving_mean, batch_mean)\\n+            variance_update = self.assign_moving_average(\\n+                self.moving_variance, batch_variance)\\n+            self.add_update(mean_update)\\n+            self.add_update(variance_update)\\n+            mean, variance = batch_mean, batch_variance\\n+        else:\\n+            mean, variance = self.moving_mean, self.moving_variance\\n+        output = batch_norm(inputs, moving_mean=mean, moving_var=variance,\\n+            beta=self.beta, gamma=self.gamma, eps=1e-5)\\n+        return output\\n+```\\n+\\n+## 批归一化在LENet中的应用\\n+\\n+为了了解如何在上下文中应用`BatchNorm`，下面我们将其应用于传统的LENET模型(:numref:`sec_lenet`)。回想一下，批归一化是在卷积层或完全连接层之后但在相应的激活函数之前应用的。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nn.Conv2D(6, kernel_size=5),\\n+        BatchNorm(6, num_dims=4),\\n+        nn.Activation(\\'sigmoid\\'),\\n+        nn.MaxPool2D(pool_size=2, strides=2),\\n+        nn.Conv2D(16, kernel_size=5),\\n+        BatchNorm(16, num_dims=4),\\n+        nn.Activation(\\'sigmoid\\'),\\n+        nn.MaxPool2D(pool_size=2, strides=2),\\n+        nn.Dense(120),\\n+        BatchNorm(120, num_dims=2),\\n+        nn.Activation(\\'sigmoid\\'),\\n+        nn.Dense(84),\\n+        BatchNorm(84, num_dims=2),\\n+        nn.Activation(\\'sigmoid\\'),\\n+        nn.Dense(10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(\\n+    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(),\\n+    nn.MaxPool2d(kernel_size=2, stride=2),\\n+    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(),\\n+    nn.MaxPool2d(kernel_size=2, stride=2), nn.Flatten(),\\n+    nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),\\n+    nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),\\n+    nn.Linear(84, 10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+# Recall that this has to be a function that will be passed to `d2l.train_ch6`\\n+# so that model building or compiling need to be within `strategy.scope()` in\\n+# order to utilize the CPU/GPU devices that we have\\n+def net():\\n+    return tf.keras.models.Sequential([\\n+        tf.keras.layers.Conv2D(filters=6, kernel_size=5,\\n+                               input_shape=(28, 28, 1)),\\n+        BatchNorm(),\\n+        tf.keras.layers.Activation(\\'sigmoid\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\\n+        tf.keras.layers.Conv2D(filters=16, kernel_size=5),\\n+        BatchNorm(),\\n+        tf.keras.layers.Activation(\\'sigmoid\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\\n+        tf.keras.layers.Flatten(),\\n+        tf.keras.layers.Dense(120),\\n+        BatchNorm(),\\n+        tf.keras.layers.Activation(\\'sigmoid\\'),\\n+        tf.keras.layers.Dense(84),\\n+        BatchNorm(),\\n+        tf.keras.layers.Activation(\\'sigmoid\\'),\\n+        tf.keras.layers.Dense(10)]\\n+    )\\n+```\\n+\\n+像以前一样，我们将根据Fashion-MNIST数据集对我们的网络进行培训。这一代码实际上与我们第一次训练LeNet(:numref:`sec_lenet`)时的代码相同。主要的不同之处在于学习速度要大得多。\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+lr, num_epochs, batch_size = 1.0, 10, 256\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\\n+d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+lr, num_epochs, batch_size = 1.0, 10, 256\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\\n+net = d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+让我们看一下从第一批归一化层学习的比例参数`gamma`和移位参数`beta`。\\n+\\n+```{.python .input}\\n+net[1].gamma.data().reshape(-1,), net[1].beta.data().reshape(-1,)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net[1].gamma.reshape((-1,)), net[1].beta.reshape((-1,))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+tf.reshape(net.layers[1].gamma, (-1,)), tf.reshape(net.layers[1].beta, (-1,))\\n+```\\n+\\n+## 简明实施\\n+\\n+与我们刚刚定义的`BatchNorm`类相比，我们可以直接从深度学习框架中使用高级API中定义的`BatchNorm`类。代码看起来与我们上面实现的应用程序几乎完全相同。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nn.Conv2D(6, kernel_size=5),\\n+        nn.BatchNorm(),\\n+        nn.Activation(\\'sigmoid\\'),\\n+        nn.MaxPool2D(pool_size=2, strides=2),\\n+        nn.Conv2D(16, kernel_size=5),\\n+        nn.BatchNorm(),\\n+        nn.Activation(\\'sigmoid\\'),\\n+        nn.MaxPool2D(pool_size=2, strides=2),\\n+        nn.Dense(120),\\n+        nn.BatchNorm(),\\n+        nn.Activation(\\'sigmoid\\'),\\n+        nn.Dense(84),\\n+        nn.BatchNorm(),\\n+        nn.Activation(\\'sigmoid\\'),\\n+        nn.Dense(10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(\\n+    nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(),\\n+    nn.MaxPool2d(kernel_size=2, stride=2),\\n+    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(),\\n+    nn.MaxPool2d(kernel_size=2, stride=2), nn.Flatten(),\\n+    nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(),\\n+    nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),\\n+    nn.Linear(84, 10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def net():\\n+    return tf.keras.models.Sequential([\\n+        tf.keras.layers.Conv2D(filters=6, kernel_size=5,\\n+                               input_shape=(28, 28, 1)),\\n+        tf.keras.layers.BatchNormalization(),\\n+        tf.keras.layers.Activation(\\'sigmoid\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\\n+        tf.keras.layers.Conv2D(filters=16, kernel_size=5),\\n+        tf.keras.layers.BatchNormalization(),\\n+        tf.keras.layers.Activation(\\'sigmoid\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\\n+        tf.keras.layers.Flatten(),\\n+        tf.keras.layers.Dense(120),\\n+        tf.keras.layers.BatchNormalization(),\\n+        tf.keras.layers.Activation(\\'sigmoid\\'),\\n+        tf.keras.layers.Dense(84),\\n+        tf.keras.layers.BatchNormalization(),\\n+        tf.keras.layers.Activation(\\'sigmoid\\'),\\n+        tf.keras.layers.Dense(10),\\n+    ])\\n+```\\n+\\n+下面，我们使用相同的超参数来训练我们的模型。请注意，与往常一样，高级API变体的运行速度要快得多，因为它的代码已编译为C++或CUDA，而我们的自定义实现必须由Python解释。\\n+\\n+```{.python .input}\\n+#@tab all\\n+d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+## 争议\\n+\\n+直观地说，批量归一化可以使优化后的景观更加平滑。然而，对于我们在训练深层模型时观察到的现象，我们必须小心区分投机性直觉和真实解释。回想一下，我们甚至不知道为什么更简单的深度神经网络(MLP和常规CNN)一开始就具有很好的泛化能力。即使在辍学和重量衰减的情况下，它们仍然非常灵活，以至于它们对看不见的数据进行概括的能力不能通过传统的学习理论概括保证来解释。\\n+\\n+在最初提出批量归一化的论文中，作者除了介绍了一个强大而有用的工具外，还解释了它为什么有效：通过减少“内部协变量偏移”。据推测，作者所说的“内部协变量转移”的意思类似于上面表达的直觉-变量值的分布在训练过程中会发生变化的概念。然而，这种解释有两个问题：i)这种漂移与*协变量漂移*有很大的不同，使得这个名字用词不当。ii)解释提供了一种不明确的直觉，但留下了*这项技术究竟为什么有效*的问题，这是一个需要严格解释的悬而未决的问题。在这本书中，我们的目标是传达实践者用来指导他们发展深层神经网络的直觉。然而，我们认为，将这些指导性直觉与既定的科学事实分开是很重要的。最终，当你掌握了这份材料并开始写你自己的研究论文时，你会想要清楚地在技术主张和预感之间划清界限。\\n+\\n+随着批量归一化的成功，其关于“内部协变量漂移”的解释一再出现在关于如何呈现机器学习研究的技术文献和更广泛的讨论中。在2017年NeurIPS大会上接受时间奖测试时，阿里·拉希米发表了一次令人难忘的演讲，他将*内部协变量转移*作为争论的焦点，将深度学习的现代实践比作炼金术。随后，在一份概述机器学习:cite:`Lipton.Steinhardt.2018`中令人不安的趋势的立场文件中，详细地重新讨论了这个例子。其他作者对批处理标准化的成功提出了另一种解释，一些人声称批处理标准化的成功，尽管在某些方面表现出与原始论文:cite:`Santurkar.Tsipras.Ilyas.ea.2018`中声称的行为相反的行为。\\n+\\n+我们注意到，“内部协变量转移”并不比技术机器学习文献中每年数以千计的类似模糊主张更值得批评。很可能，作为这些辩论的焦点，它的共鸣归功于它对目标受众的广泛认知度。批量归一化已被证明是一种必不可少的方法，几乎应用于所有部署的图像分类器，赢得了介绍该技术的论文的数万条引用。\\n+\\n+## 摘要\\n+\\n+* 在模型训练过程中，批量归一化利用小批量的均值和标准差不断调整神经网络的中间输出，使整个神经网络各层的中间输出值更加稳定。\\n+* 全连通层和卷积层的批量归一化方法略有不同。\\n+* 批归一化层与退出层一样，在训练模式和预测模式下具有不同的计算结果。\\n+* 批量标准化有许多有益的副作用，主要是正规化的副作用。另一方面，减少内部协变量转移的原始动机似乎不是一个有效的解释。\\n+\\n+## 练习\\n+\\n+1. 在批量归一化之前，我们可以从全连通层或卷积层中去掉偏置参数吗？为什么？\\n+1. 比较批归一化和不批归一化的LENet的学习率。\\n+    1. 绘制训练和测试精确度提高的图表。\\n+    1. 你能把学习率调到多大？\\n+1. 我们是否需要在每一层中进行批量规范化？用它做实验？\\n+1. 你能用批量归一化来代替退学吗？这种行为是如何改变的？\\n+1. 确定参数`beta`和`gamma`，观察分析结果。\\n+1. 请从高级API查看`BatchNorm`的在线文档，以了解批处理标准化的其他应用程序。\\n+1. 研究思路：想一想可以应用的其他规范化转换吗？你能应用概率积分变换吗？全秩协方差估计怎么样？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/83)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/84)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/330)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-modern/densenet_baidu.md b/chapter_convolutional-modern/densenet_baidu.md\\nnew file mode 100644\\nindex 00000000..a8343320\\n--- /dev/null\\n+++ b/chapter_convolutional-modern/densenet_baidu.md\\n@@ -0,0 +1,380 @@\\n+# 密集连接网络（DenseNet）\\n+\\n+ResNet极大地改变了如何参数化深层网络中函数的观点。*DenseNet*（稠密卷积网络）在某种程度上是:cite:`Huang.Liu.Van-Der-Maaten.ea.2017`的逻辑扩展。为了理解如何得到它，让我们绕道去数学。\\n+\\n+## 从ResNet到DenseNet\\n+\\n+回想一下函数的泰勒展开式。对于$x = 0$点，可以写为\\n+\\n+$$f(x) = f(0) + f\\'(0) x + \\\\frac{f\\'\\'(0)}{2!}  x^2 + \\\\frac{f\\'\\'\\'(0)}{3!}  x^3 + \\\\ldots.$$\\n+\\n+关键是它把一个函数分解成越来越高阶的项。同样，ResNet将函数分解为\\n+\\n+$$f(\\\\mathbf{x}) = \\\\mathbf{x} + g(\\\\mathbf{x}).$$\\n+\\n+也就是说，ResNet将$f$分解为一个简单的线性项和一个更复杂的非线性项。如果我们想捕获（不一定要添加）超过两个术语的信息呢？一种解决方案是DenseNet :cite:`Huang.Liu.Van-Der-Maaten.ea.2017`。\\n+\\n+![The main difference between ResNet (left) and DenseNet (right) in cross-layer connections: use of addition and use of concatenation. ](../img/densenet-block.svg)\\n+:label:`fig_densenet_block`\\n+\\n+如:numref:`fig_densenet_block`所示，ResNet和DenseNet之间的关键区别在于，在后一种情况下，输出是*连接*（用$[,]$表示）而不是相加的。因此，在应用越来越复杂的函数序列后，我们执行从$\\\\mathbf{x}$到其值的映射：\\n+\\n+$$\\\\mathbf{x} \\\\to \\\\left[\\n+\\\\mathbf{x},\\n+f_1(\\\\mathbf{x}),\\n+f_2([\\\\mathbf{x}, f_1(\\\\mathbf{x})]), f_3([\\\\mathbf{x}, f_1(\\\\mathbf{x}), f_2([\\\\mathbf{x}, f_1(\\\\mathbf{x})])]), \\\\ldots\\\\right].$$\\n+\\n+最后，将这些功能结合到MLP中，再次减少特征的数量。就实现而言，这非常简单：我们不需要添加术语，而是将它们连接起来。DenseNet这个名字是因为变量之间的依赖关系图变得非常密集。这样一个链的最后一层与之前的所有层紧密相连。密集连接如:numref:`fig_densenet`所示。\\n+\\n+![Dense connections in DenseNet.](../img/densenet.svg)\\n+:label:`fig_densenet`\\n+\\n+构成致密网的主要组件是*致密块*和*过渡层*。前者定义如何连接输入和输出，而后者则控制通道的数量，以使其不会太大。\\n+\\n+## 致密块体\\n+\\n+DenseNet使用ResNet的“批处理规范化、激活和卷积”结构（参见:numref:`sec_resnet`中的练习）。首先，我们实现了这种卷积块结构。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+def conv_block(num_channels):\\n+    blk = nn.Sequential()\\n+    blk.add(nn.BatchNorm(),\\n+            nn.Activation(\\'relu\\'),\\n+            nn.Conv2D(num_channels, kernel_size=3, padding=1))\\n+    return blk\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+\\n+def conv_block(input_channels, num_channels):\\n+    return nn.Sequential(\\n+        nn.BatchNorm2d(input_channels), nn.ReLU(),\\n+        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+class ConvBlock(tf.keras.layers.Layer):\\n+    def __init__(self, num_channels):\\n+        super(ConvBlock, self).__init__()\\n+        self.bn = tf.keras.layers.BatchNormalization()\\n+        self.relu = tf.keras.layers.ReLU()\\n+        self.conv = tf.keras.layers.Conv2D(\\n+            filters=num_channels, kernel_size=(3, 3), padding=\\'same\\')\\n+\\n+        self.listLayers = [self.bn, self.relu, self.conv]\\n+\\n+    def call(self, x):\\n+        y = x\\n+        for layer in self.listLayers.layers:\\n+            y = layer(y)\\n+        y = tf.keras.layers.concatenate([x,y], axis=-1)\\n+        return y\\n+```\\n+\\n+一个*密集块*由多个卷积块组成，每个卷积块使用相同数量的输出信道。然而，在前向传播中，我们将每个卷积块的输入和输出连接在信道维上。\\n+\\n+```{.python .input}\\n+class DenseBlock(nn.Block):\\n+    def __init__(self, num_convs, num_channels, **kwargs):\\n+        super().__init__(**kwargs)\\n+        self.net = nn.Sequential()\\n+        for _ in range(num_convs):\\n+            self.net.add(conv_block(num_channels))\\n+\\n+    def forward(self, X):\\n+        for blk in self.net:\\n+            Y = blk(X)\\n+            # Concatenate the input and output of each block on the channel\\n+            # dimension\\n+            X = np.concatenate((X, Y), axis=1)\\n+        return X\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class DenseBlock(nn.Module):\\n+    def __init__(self, num_convs, input_channels, num_channels):\\n+        super(DenseBlock, self).__init__()\\n+        layer = []\\n+        for i in range(num_convs):\\n+            layer.append(conv_block(\\n+                num_channels * i + input_channels, num_channels))\\n+        self.net = nn.Sequential(*layer)\\n+\\n+    def forward(self, X):\\n+        for blk in self.net:\\n+            Y = blk(X)\\n+            # Concatenate the input and output of each block on the channel\\n+            # dimension\\n+            X = torch.cat((X, Y), dim=1)\\n+        return X\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class DenseBlock(tf.keras.layers.Layer):\\n+    def __init__(self, num_convs, num_channels):\\n+        super(DenseBlock, self).__init__()\\n+        self.listLayers = []\\n+        for _ in range(num_convs):\\n+            self.listLayers.append(ConvBlock(num_channels))\\n+\\n+    def call(self, x):\\n+        for layer in self.listLayers.layers:\\n+            x = layer(x)\\n+        return x\\n+```\\n+\\n+在下面的示例中，我们定义了一个`DenseBlock`实例，其中包含10个输出信道的2个卷积块。当使用3个通道的输入时，我们将得到$3+2\\\\times 10=23$个通道的输出。卷积块通道数控制输出通道数相对于输入通道数的增长。这也被称为*增长率*。\\n+\\n+```{.python .input}\\n+blk = DenseBlock(2, 10)\\n+blk.initialize()\\n+X = np.random.uniform(size=(4, 3, 8, 8))\\n+Y = blk(X)\\n+Y.shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+blk = DenseBlock(2, 3, 10)\\n+X = torch.randn(4, 3, 8, 8)\\n+Y = blk(X)\\n+Y.shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+blk = DenseBlock(2, 10)\\n+X = tf.random.uniform((4, 8, 8, 3))\\n+Y = blk(X)\\n+Y.shape\\n+```\\n+\\n+## 过渡层\\n+\\n+由于每一个密集的块体都会增加通道的数量，增加过多的通道会导致模型过于复杂。过渡层*用于控制模型的复杂性。它通过使用$1\\\\times 1$卷积层减少了信道数量，并将平均池层的高度和宽度减半，步长为2，进一步降低了模型的复杂性。\\n+\\n+```{.python .input}\\n+def transition_block(num_channels):\\n+    blk = nn.Sequential()\\n+    blk.add(nn.BatchNorm(), nn.Activation(\\'relu\\'),\\n+            nn.Conv2D(num_channels, kernel_size=1),\\n+            nn.AvgPool2D(pool_size=2, strides=2))\\n+    return blk\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def transition_block(input_channels, num_channels):\\n+    return nn.Sequential(\\n+        nn.BatchNorm2d(input_channels), nn.ReLU(),\\n+        nn.Conv2d(input_channels, num_channels, kernel_size=1),\\n+        nn.AvgPool2d(kernel_size=2, stride=2))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class TransitionBlock(tf.keras.layers.Layer):\\n+    def __init__(self, num_channels, **kwargs):\\n+        super(TransitionBlock, self).__init__(**kwargs)\\n+        self.batch_norm = tf.keras.layers.BatchNormalization()\\n+        self.relu = tf.keras.layers.ReLU()\\n+        self.conv = tf.keras.layers.Conv2D(num_channels, kernel_size=1)\\n+        self.avg_pool = tf.keras.layers.AvgPool2D(pool_size=2, strides=2)\\n+\\n+    def call(self, x):\\n+        x = self.batch_norm(x)\\n+        x = self.relu(x)\\n+        x = self.conv(x)\\n+        return self.avg_pool(x)\\n+```\\n+\\n+将具有10个通道的过渡层应用于上一个示例中密集块的输出。这将输出通道的数量减少到10个，并将高度和宽度减半。\\n+\\n+```{.python .input}\\n+blk = transition_block(10)\\n+blk.initialize()\\n+blk(Y).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+blk = transition_block(23, 10)\\n+blk(Y).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+blk = TransitionBlock(10)\\n+blk(Y).shape\\n+```\\n+\\n+## 登塞纳模型\\n+\\n+接下来，我们将构造一个DenseNet模型。DenseNet首先使用与ResNet中相同的单个卷积层和最大池层。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\\n+        nn.BatchNorm(), nn.Activation(\\'relu\\'),\\n+        nn.MaxPool2D(pool_size=3, strides=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+b1 = nn.Sequential(\\n+    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\\n+    nn.BatchNorm2d(64), nn.ReLU(),\\n+    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def block_1():\\n+    return tf.keras.Sequential([\\n+       tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding=\\'same\\'),\\n+       tf.keras.layers.BatchNormalization(),\\n+       tf.keras.layers.ReLU(),\\n+       tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\\'same\\')])\\n+```\\n+\\n+然后，与ResNet使用的由剩余块组成的四个模块类似，DenseNet使用四个密集块。与ResNet类似，我们可以设置每个密集块中使用的卷积层的数量。在这里，我们将其设置为4，与:numref:`sec_resnet`中的ResNet-18型号一致。此外，我们将密集块中卷积层的通道数（即增长率）设置为32个，这样每个稠密块将增加128个通道。\\n+\\n+在ResNet中，每个模块之间的高度和宽度通过一个步长为2的剩余块来减小。在这里，我们使用过渡层将高度和宽度减半，并将通道数减半。\\n+\\n+```{.python .input}\\n+# `num_channels`: the current number of channels\\n+num_channels, growth_rate = 64, 32\\n+num_convs_in_dense_blocks = [4, 4, 4, 4]\\n+\\n+for i, num_convs in enumerate(num_convs_in_dense_blocks):\\n+    net.add(DenseBlock(num_convs, growth_rate))\\n+    # This is the number of output channels in the previous dense block\\n+    num_channels += num_convs * growth_rate\\n+    # A transition layer that halves the number of channels is added between\\n+    # the dense blocks\\n+    if i != len(num_convs_in_dense_blocks) - 1:\\n+        num_channels //= 2\\n+        net.add(transition_block(num_channels))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+# `num_channels`: the current number of channels\\n+num_channels, growth_rate = 64, 32\\n+num_convs_in_dense_blocks = [4, 4, 4, 4]\\n+blks = []\\n+for i, num_convs in enumerate(num_convs_in_dense_blocks):\\n+    blks.append(DenseBlock(num_convs, num_channels, growth_rate))\\n+    # This is the number of output channels in the previous dense block\\n+    num_channels += num_convs * growth_rate\\n+    # A transition layer that haves the number of channels is added between\\n+    # the dense blocks\\n+    if i != len(num_convs_in_dense_blocks) - 1:\\n+        blks.append(transition_block(num_channels, num_channels // 2))\\n+        num_channels = num_channels // 2\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def block_2():\\n+    net = block_1()\\n+    # `num_channels`: the current number of channels\\n+    num_channels, growth_rate = 64, 32\\n+    num_convs_in_dense_blocks = [4, 4, 4, 4]\\n+\\n+    for i, num_convs in enumerate(num_convs_in_dense_blocks):\\n+        net.add(DenseBlock(num_convs, growth_rate))\\n+        # This is the number of output channels in the previous dense block\\n+        num_channels += num_convs * growth_rate\\n+        # A transition layer that haves the number of channels is added\\n+        # between the dense blocks\\n+        if i != len(num_convs_in_dense_blocks) - 1:\\n+            num_channels //= 2\\n+            net.add(TransitionBlock(num_channels))\\n+    return net\\n+```\\n+\\n+与ResNet类似，一个全局池层和一个完全连接的层在最后连接以产生输出。\\n+\\n+```{.python .input}\\n+net.add(nn.BatchNorm(),\\n+        nn.Activation(\\'relu\\'),\\n+        nn.GlobalAvgPool2D(),\\n+        nn.Dense(10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(\\n+    b1, *blks,\\n+    nn.BatchNorm2d(num_channels), nn.ReLU(),\\n+    nn.AdaptiveMaxPool2d((1, 1)),\\n+    nn.Flatten(),\\n+    nn.Linear(num_channels, 10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def net():\\n+    net = block_2()\\n+    net.add(tf.keras.layers.BatchNormalization())\\n+    net.add(tf.keras.layers.ReLU())\\n+    net.add(tf.keras.layers.GlobalAvgPool2D())\\n+    net.add(tf.keras.layers.Flatten())\\n+    net.add(tf.keras.layers.Dense(10))\\n+    return net\\n+```\\n+\\n+## 培训\\n+\\n+由于我们在这里使用的是一个更深的网络，在本节中，我们将把输入高度和宽度从224减少到96，以简化计算。\\n+\\n+```{.python .input}\\n+#@tab all\\n+lr, num_epochs, batch_size = 0.1, 10, 256\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\\n+d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+## 摘要\\n+\\n+* 在跨层连接方面，与ResNet不同，ResNet将输入和输出相加在一起，DenseNet在通道维度上连接输入和输出。\\n+* 构成致密网的主要成分是致密块体和过渡层。\\n+* 在组成网络时，我们需要通过添加过渡层来控制网络的维数，从而再次减少信道的数量。\\n+\\n+## 练习\\n+\\n+1. 为什么我们在过渡层使用平均池而不是最大池？\\n+1. DenseNet的优点之一是其模型参数比ResNet的小。为什么会这样？\\n+1. DenseNet被批评的一个问题是它的高内存消耗。\\n+    1. 真的是这样吗？尝试将输入形状更改为$224\\\\times 224$以查看实际的GPU内存消耗。\\n+    1. 你能想出另一种方法来减少内存消耗吗？你需要如何改变框架？\\n+1. 实现DenseNet论文:cite:`Huang.Liu.Van-Der-Maaten.ea.2017`表1所示的各种DenseNet版本。\\n+1. 应用DENP的设计思想，设计了基于DENP的模型。应用于:numref:`sec_kaggle_house`中的房价预测任务。\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/87)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/88)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/331)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-modern/densenet_tencent.md b/chapter_convolutional-modern/densenet_tencent.md\\nnew file mode 100644\\nindex 00000000..3d354720\\n--- /dev/null\\n+++ b/chapter_convolutional-modern/densenet_tencent.md\\n@@ -0,0 +1,380 @@\\n+# 密集连接网络(DenseNet)\\n+\\n+RESNET极大地改变了人们对如何将深层网络中的功能参数化的看法。*DENSENET*(密集卷积网络)在某种程度上是这个:cite:`Huang.Liu.Van-Der-Maaten.ea.2017`的逻辑扩展。为了理解如何达到这个目的，让我们稍微绕道数学。\\n+\\n+## 从ResNet到DenseNet\\n+\\n+回想一下函数的泰勒展开。对于点$x = 0$，它可以写为\\n+\\n+$$f(x) = f(0) + f\\'(0) x + \\\\frac{f\\'\\'(0)}{2!}  x^2 + \\\\frac{f\\'\\'\\'(0)}{3!}  x^3 + \\\\ldots.$$\\n+\\n+关键是它将一个函数分解成越来越高的高次项。类似地，ResNet将功能分解为\\n+\\n+$$f(\\\\mathbf{x}) = \\\\mathbf{x} + g(\\\\mathbf{x}).$$\\n+\\n+也就是说，Resnet将$f$分解为简单的线性项和更复杂的非线性项。如果我们希望捕获(不一定要添加)两个术语以外的信息，该怎么办？其中一个解决方案是Densenet :cite:`Huang.Liu.Van-Der-Maaten.ea.2017`。\\n+\\n+![The main difference between ResNet (left) and DenseNet (right) in cross-layer connections: use of addition and use of concatenation. ](../img/densenet-block.svg)\\n+:label:`fig_densenet_block`\\n+\\n+如:numref:`fig_densenet_block`所示，Resnet和Densenet之间的主要区别在于，在后一种情况下，输出是“连接”的*(用$[,]$表示)，而不是相加。因此，在应用越来越复杂的函数序列之后，我们执行从$\\\\mathbf{x}$到其值的映射：\\n+\\n+$$\\\\mathbf{x} \\\\to \\\\left[\\n+\\\\mathbf{x},\\n+f_1(\\\\mathbf{x}),\\n+f_2([\\\\mathbf{x}, f_1(\\\\mathbf{x})]), f_3([\\\\mathbf{x}, f_1(\\\\mathbf{x}), f_2([\\\\mathbf{x}, f_1(\\\\mathbf{x})])]), \\\\ldots\\\\right].$$\\n+\\n+最后，将这些功能组合到MLP中，再次减少特征的数量。就实现而言，这相当简单：我们不是添加术语，而是将它们连接起来。DenseNet这个名字源于变量之间的依赖图变得相当密集这一事实。这样的链的最后一层密集地连接到所有前面的层。密集连接如:numref:`fig_densenet`中所示。\\n+\\n+![Dense connections in DenseNet.](../img/densenet.svg)\\n+:label:`fig_densenet`\\n+\\n+构成DenseNet的主要组件是*密集块*和*过渡层*。前者定义输入和输出如何级联，而后者控制通道的数量，使其不太大。\\n+\\n+## 密集区块\\n+\\n+DenseNet使用修改后的Resnet的“批标准化、激活和卷积”结构(参见:numref:`sec_resnet`中的练习)。首先，我们实现这种卷积挡路结构。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+def conv_block(num_channels):\\n+    blk = nn.Sequential()\\n+    blk.add(nn.BatchNorm(),\\n+            nn.Activation(\\'relu\\'),\\n+            nn.Conv2D(num_channels, kernel_size=3, padding=1))\\n+    return blk\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+\\n+def conv_block(input_channels, num_channels):\\n+    return nn.Sequential(\\n+        nn.BatchNorm2d(input_channels), nn.ReLU(),\\n+        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+class ConvBlock(tf.keras.layers.Layer):\\n+    def __init__(self, num_channels):\\n+        super(ConvBlock, self).__init__()\\n+        self.bn = tf.keras.layers.BatchNormalization()\\n+        self.relu = tf.keras.layers.ReLU()\\n+        self.conv = tf.keras.layers.Conv2D(\\n+            filters=num_channels, kernel_size=(3, 3), padding=\\'same\\')\\n+\\n+        self.listLayers = [self.bn, self.relu, self.conv]\\n+\\n+    def call(self, x):\\n+        y = x\\n+        for layer in self.listLayers.layers:\\n+            y = layer(y)\\n+        y = tf.keras.layers.concatenate([x,y], axis=-1)\\n+        return y\\n+```\\n+\\n+*密集挡路*由多个卷积块组成，每个卷积块使用相同数量的输出通道。然而，在前向传播中，我们将每个卷积挡路的输入和输出串联在信道维度上。\\n+\\n+```{.python .input}\\n+class DenseBlock(nn.Block):\\n+    def __init__(self, num_convs, num_channels, **kwargs):\\n+        super().__init__(**kwargs)\\n+        self.net = nn.Sequential()\\n+        for _ in range(num_convs):\\n+            self.net.add(conv_block(num_channels))\\n+\\n+    def forward(self, X):\\n+        for blk in self.net:\\n+            Y = blk(X)\\n+            # Concatenate the input and output of each block on the channel\\n+            # dimension\\n+            X = np.concatenate((X, Y), axis=1)\\n+        return X\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class DenseBlock(nn.Module):\\n+    def __init__(self, num_convs, input_channels, num_channels):\\n+        super(DenseBlock, self).__init__()\\n+        layer = []\\n+        for i in range(num_convs):\\n+            layer.append(conv_block(\\n+                num_channels * i + input_channels, num_channels))\\n+        self.net = nn.Sequential(*layer)\\n+\\n+    def forward(self, X):\\n+        for blk in self.net:\\n+            Y = blk(X)\\n+            # Concatenate the input and output of each block on the channel\\n+            # dimension\\n+            X = torch.cat((X, Y), dim=1)\\n+        return X\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class DenseBlock(tf.keras.layers.Layer):\\n+    def __init__(self, num_convs, num_channels):\\n+        super(DenseBlock, self).__init__()\\n+        self.listLayers = []\\n+        for _ in range(num_convs):\\n+            self.listLayers.append(ConvBlock(num_channels))\\n+\\n+    def call(self, x):\\n+        for layer in self.listLayers.layers:\\n+            x = layer(x)\\n+        return x\\n+```\\n+\\n+在下面的示例中，我们定义了一个`DenseBlock`实例，该实例具有10个输出通道的2个卷积块。当使用具有3个通道的输入时，我们将获得具有$3+2\\\\times 10=23$个通道的输出。卷积挡路通道数量控制着输出通道数量相对于输入通道数量的增长。这也被称为*增长率*。\\n+\\n+```{.python .input}\\n+blk = DenseBlock(2, 10)\\n+blk.initialize()\\n+X = np.random.uniform(size=(4, 3, 8, 8))\\n+Y = blk(X)\\n+Y.shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+blk = DenseBlock(2, 3, 10)\\n+X = torch.randn(4, 3, 8, 8)\\n+Y = blk(X)\\n+Y.shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+blk = DenseBlock(2, 10)\\n+X = tf.random.uniform((4, 8, 8, 3))\\n+Y = blk(X)\\n+Y.shape\\n+```\\n+\\n+## 过渡层\\n+\\n+由于每个密集的挡路都会增加频道数，加得太多会导致模型过于复杂。使用*过渡层*来控制模型的复杂性。它通过使用$1\\\\times 1$卷积层来减少通道数，并以2的步长将平均汇聚层的高度和宽度减半，进一步降低了模型的复杂性。\\n+\\n+```{.python .input}\\n+def transition_block(num_channels):\\n+    blk = nn.Sequential()\\n+    blk.add(nn.BatchNorm(), nn.Activation(\\'relu\\'),\\n+            nn.Conv2D(num_channels, kernel_size=1),\\n+            nn.AvgPool2D(pool_size=2, strides=2))\\n+    return blk\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def transition_block(input_channels, num_channels):\\n+    return nn.Sequential(\\n+        nn.BatchNorm2d(input_channels), nn.ReLU(),\\n+        nn.Conv2d(input_channels, num_channels, kernel_size=1),\\n+        nn.AvgPool2d(kernel_size=2, stride=2))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class TransitionBlock(tf.keras.layers.Layer):\\n+    def __init__(self, num_channels, **kwargs):\\n+        super(TransitionBlock, self).__init__(**kwargs)\\n+        self.batch_norm = tf.keras.layers.BatchNormalization()\\n+        self.relu = tf.keras.layers.ReLU()\\n+        self.conv = tf.keras.layers.Conv2D(num_channels, kernel_size=1)\\n+        self.avg_pool = tf.keras.layers.AvgPool2D(pool_size=2, strides=2)\\n+\\n+    def call(self, x):\\n+        x = self.batch_norm(x)\\n+        x = self.relu(x)\\n+        x = self.conv(x)\\n+        return self.avg_pool(x)\\n+```\\n+\\n+将具有10个通道的过渡层应用于上一个示例中密集挡路的输出。这会将输出通道的数量减少到10个，并将高度和宽度减半。\\n+\\n+```{.python .input}\\n+blk = transition_block(10)\\n+blk.initialize()\\n+blk(Y).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+blk = transition_block(23, 10)\\n+blk(Y).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+blk = TransitionBlock(10)\\n+blk(Y).shape\\n+```\\n+\\n+## DenseNet模型\\n+\\n+接下来，我们将构建DenseNet模型。DenseNet首次使用与ResNet相同的单卷积层和最大池层。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\\n+        nn.BatchNorm(), nn.Activation(\\'relu\\'),\\n+        nn.MaxPool2D(pool_size=3, strides=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+b1 = nn.Sequential(\\n+    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\\n+    nn.BatchNorm2d(64), nn.ReLU(),\\n+    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def block_1():\\n+    return tf.keras.Sequential([\\n+       tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding=\\'same\\'),\\n+       tf.keras.layers.BatchNormalization(),\\n+       tf.keras.layers.ReLU(),\\n+       tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\\'same\\')])\\n+```\\n+\\n+然后，与ResNet使用的由残留块组成的四个模块类似，DenseNet使用四个密集块。与Resnet类似，我们可以设置每个密集挡路使用的卷积层数。这里，我们将其设置为4，与:numref:`sec_resnet`中的Resnet-18模型一致。此外，我们将密集的挡路中卷积层的通道数(即增长率)设置为32个，因此每个密集的挡路将增加128个通道。\\n+\\n+在ResNet中，每个模块之间的高度和宽度都减少了一个步长为2的剩余挡路，这里我们使用过渡层将高度和宽度减半，将通道数减半。\\n+\\n+```{.python .input}\\n+# `num_channels`: the current number of channels\\n+num_channels, growth_rate = 64, 32\\n+num_convs_in_dense_blocks = [4, 4, 4, 4]\\n+\\n+for i, num_convs in enumerate(num_convs_in_dense_blocks):\\n+    net.add(DenseBlock(num_convs, growth_rate))\\n+    # This is the number of output channels in the previous dense block\\n+    num_channels += num_convs * growth_rate\\n+    # A transition layer that halves the number of channels is added between\\n+    # the dense blocks\\n+    if i != len(num_convs_in_dense_blocks) - 1:\\n+        num_channels //= 2\\n+        net.add(transition_block(num_channels))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+# `num_channels`: the current number of channels\\n+num_channels, growth_rate = 64, 32\\n+num_convs_in_dense_blocks = [4, 4, 4, 4]\\n+blks = []\\n+for i, num_convs in enumerate(num_convs_in_dense_blocks):\\n+    blks.append(DenseBlock(num_convs, num_channels, growth_rate))\\n+    # This is the number of output channels in the previous dense block\\n+    num_channels += num_convs * growth_rate\\n+    # A transition layer that haves the number of channels is added between\\n+    # the dense blocks\\n+    if i != len(num_convs_in_dense_blocks) - 1:\\n+        blks.append(transition_block(num_channels, num_channels // 2))\\n+        num_channels = num_channels // 2\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def block_2():\\n+    net = block_1()\\n+    # `num_channels`: the current number of channels\\n+    num_channels, growth_rate = 64, 32\\n+    num_convs_in_dense_blocks = [4, 4, 4, 4]\\n+\\n+    for i, num_convs in enumerate(num_convs_in_dense_blocks):\\n+        net.add(DenseBlock(num_convs, growth_rate))\\n+        # This is the number of output channels in the previous dense block\\n+        num_channels += num_convs * growth_rate\\n+        # A transition layer that haves the number of channels is added\\n+        # between the dense blocks\\n+        if i != len(num_convs_in_dense_blocks) - 1:\\n+            num_channels //= 2\\n+            net.add(TransitionBlock(num_channels))\\n+    return net\\n+```\\n+\\n+与ResNet类似，全局池层和完全连接层在末端连接以产生输出。\\n+\\n+```{.python .input}\\n+net.add(nn.BatchNorm(),\\n+        nn.Activation(\\'relu\\'),\\n+        nn.GlobalAvgPool2D(),\\n+        nn.Dense(10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(\\n+    b1, *blks,\\n+    nn.BatchNorm2d(num_channels), nn.ReLU(),\\n+    nn.AdaptiveMaxPool2d((1, 1)),\\n+    nn.Flatten(),\\n+    nn.Linear(num_channels, 10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def net():\\n+    net = block_2()\\n+    net.add(tf.keras.layers.BatchNormalization())\\n+    net.add(tf.keras.layers.ReLU())\\n+    net.add(tf.keras.layers.GlobalAvgPool2D())\\n+    net.add(tf.keras.layers.Flatten())\\n+    net.add(tf.keras.layers.Dense(10))\\n+    return net\\n+```\\n+\\n+## 培训\\n+\\n+由于我们在这里使用的是更深的网络，因此在本节中，我们将把输入高度和宽度从224减少到96，以简化计算。\\n+\\n+```{.python .input}\\n+#@tab all\\n+lr, num_epochs, batch_size = 0.1, 10, 256\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\\n+d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+## 摘要\\n+\\n+* 在跨层连接方面，与输入和输出相加的ResNet不同，DenseNet在通道维度上连接输入和输出。\\n+* 构成DenseNet的主要组件是密集块和过渡层。\\n+* 在组成网络时，我们需要通过添加过渡层来再次缩小通道数量，从而使维度保持在可控范围内。\\n+\\n+## 练习\\n+\\n+1. 为什么我们在过渡层中使用平均池而不是最大池？\\n+1. DenseNet论文中提到的优点之一是它的模型参数比ResNet的模型参数小。为甚麽会这样呢？\\n+1. DenseNet受到批评的一个问题是它的高内存消耗。\\n+    1. 真的是这样吗？尝试将输入形状更改为$224\\\\times 224$，以查看实际的图形处理器内存消耗。\\n+    1. 您能想出一种降低内存消耗的替代方法吗？您需要如何更改框架？\\n+1. 实施DenseNet论文:cite:`Huang.Liu.Van-Der-Maaten.ea.2017`的表1中列出的各种DenseNet版本。\\n+1. 应用DenseNet的思想设计了一个基于MLP的模型。将其应用于:numref:`sec_kaggle_house`年度房价预测任务。\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/87)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/88)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/331)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-modern/googlenet_baidu.md b/chapter_convolutional-modern/googlenet_baidu.md\\nnew file mode 100644\\nindex 00000000..abc9278a\\n--- /dev/null\\n+++ b/chapter_convolutional-modern/googlenet_baidu.md\\n@@ -0,0 +1,343 @@\\n+# 具有并行连接的网络（GoogLeNet）\\n+:label:`sec_googlenet`\\n+\\n+2014年，*GoogLeNet*赢得了ImageNet挑战赛，提出了一种结合NiN优势和重复块:cite:`Szegedy.Liu.Jia.ea.2015`范例的结构。这篇论文的一个重点是解决什么样的卷积核是最好的问题。毕竟，以前流行的网络使用小到$1 \\\\times 1$和大到$11 \\\\times 11$的选择。本文的一个观点是，有时使用不同大小的内核的组合是有利的。在本节中，我们将介绍GoogLeNet，它提供了原始模型的一个稍微简化的版本：我们省略了一些为稳定训练而添加的特殊特性，但是现在有了更好的训练算法，这些特性是不必要的。\\n+\\n+## 初始块\\n+\\n+在GoogLeNet中，基本的卷积块被称为“*Inception block*”，很可能是因为电影《盗梦空间》（Inception*）（“我们需要更深入一些”）中的一句话命名的，这部电影启动了一个病毒性模因。\\n+\\n+![Structure of the Inception block.](../img/inception.svg)\\n+:label:`fig_inception`\\n+\\n+如:numref:`fig_inception`所示，起始块由四条并行路径组成。前三条路径使用窗口大小为$1\\\\times 1$、$3\\\\times 3$和$5\\\\times 5$的卷积层从不同的空间大小提取信息。中间的两条路径对输入执行$1\\\\times 1$卷积，以减少信道数量，降低模型的复杂性。第四条路径使用$3\\\\times 3$最大池化层，然后是$1\\\\times 1$卷积层来更改信道数。这四条路径都使用适当的填充，使输入和输出具有相同的高度和宽度。最后，沿着每个路径的输出沿着信道维度串联起来，并构成块的输出。初始块的通常调整的超参数是每层输出通道的数量。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+class Inception(nn.Block):\\n+    # `c1`--`c4` are the number of output channels for each path\\n+    def __init__(self, c1, c2, c3, c4, **kwargs):\\n+        super(Inception, self).__init__(**kwargs)\\n+        # Path 1 is a single 1 x 1 convolutional layer\\n+        self.p1_1 = nn.Conv2D(c1, kernel_size=1, activation=\\'relu\\')\\n+        # Path 2 is a 1 x 1 convolutional layer followed by a 3 x 3\\n+        # convolutional layer\\n+        self.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation=\\'relu\\')\\n+        self.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1,\\n+                              activation=\\'relu\\')\\n+        # Path 3 is a 1 x 1 convolutional layer followed by a 5 x 5\\n+        # convolutional layer\\n+        self.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation=\\'relu\\')\\n+        self.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2,\\n+                              activation=\\'relu\\')\\n+        # Path 4 is a 3 x 3 maximum pooling layer followed by a 1 x 1\\n+        # convolutional layer\\n+        self.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)\\n+        self.p4_2 = nn.Conv2D(c4, kernel_size=1, activation=\\'relu\\')\\n+\\n+    def forward(self, x):\\n+        p1 = self.p1_1(x)\\n+        p2 = self.p2_2(self.p2_1(x))\\n+        p3 = self.p3_2(self.p3_1(x))\\n+        p4 = self.p4_2(self.p4_1(x))\\n+        # Concatenate the outputs on the channel dimension\\n+        return np.concatenate((p1, p2, p3, p4), axis=1)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+from torch.nn import functional as F\\n+\\n+class Inception(nn.Module):\\n+    # `c1`--`c4` are the number of output channels for each path\\n+    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\\n+        super(Inception, self).__init__(**kwargs)\\n+        # Path 1 is a single 1 x 1 convolutional layer\\n+        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\\n+        # Path 2 is a 1 x 1 convolutional layer followed by a 3 x 3\\n+        # convolutional layer\\n+        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\\n+        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\\n+        # Path 3 is a 1 x 1 convolutional layer followed by a 5 x 5\\n+        # convolutional layer\\n+        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\\n+        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\\n+        # Path 4 is a 3 x 3 maximum pooling layer followed by a 1 x 1\\n+        # convolutional layer\\n+        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\\n+        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\\n+\\n+    def forward(self, x):\\n+        p1 = F.relu(self.p1_1(x))\\n+        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\\n+        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\\n+        p4 = F.relu(self.p4_2(self.p4_1(x)))\\n+        # Concatenate the outputs on the channel dimension\\n+        return torch.cat((p1, p2, p3, p4), dim=1)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+class Inception(tf.keras.Model):\\n+    # `c1`--`c4` are the number of output channels for each path\\n+    def __init__(self, c1, c2, c3, c4):\\n+        super().__init__()\\n+        # Path 1 is a single 1 x 1 convolutional layer\\n+        self.p1_1 = tf.keras.layers.Conv2D(c1, 1, activation=\\'relu\\')\\n+        # Path 2 is a 1 x 1 convolutional layer followed by a 3 x 3\\n+        # convolutional layer\\n+        self.p2_1 = tf.keras.layers.Conv2D(c2[0], 1, activation=\\'relu\\')\\n+        self.p2_2 = tf.keras.layers.Conv2D(c2[1], 3, padding=\\'same\\',\\n+                                           activation=\\'relu\\')\\n+        # Path 3 is a 1 x 1 convolutional layer followed by a 5 x 5\\n+        # convolutional layer\\n+        self.p3_1 = tf.keras.layers.Conv2D(c3[0], 1, activation=\\'relu\\')\\n+        self.p3_2 = tf.keras.layers.Conv2D(c3[1], 5, padding=\\'same\\',\\n+                                           activation=\\'relu\\')\\n+        # Path 4 is a 3 x 3 maximum pooling layer followed by a 1 x 1\\n+        # convolutional layer\\n+        self.p4_1 = tf.keras.layers.MaxPool2D(3, 1, padding=\\'same\\')\\n+        self.p4_2 = tf.keras.layers.Conv2D(c4, 1, activation=\\'relu\\')\\n+\\n+\\n+    def call(self, x):\\n+        p1 = self.p1_1(x)\\n+        p2 = self.p2_2(self.p2_1(x))\\n+        p3 = self.p3_2(self.p3_1(x))\\n+        p4 = self.p4_2(self.p4_1(x))\\n+        # Concatenate the outputs on the channel dimension\\n+        return tf.keras.layers.Concatenate()([p1, p2, p3, p4])\\n+```\\n+\\n+为了理解这个网络为什么工作得这么好，考虑一下过滤器的组合。他们在不同的范围内探索图像。这意味着不同程度的细节可以被不同的过滤器有效地识别。同时，我们可以为不同的范围分配不同数量的参数（例如，更多的参数用于短期，但不能完全忽略长范围）。\\n+\\n+## 谷歌模型\\n+\\n+如:numref:`fig_inception_full`所示，GoogLeNet使用总共9个初始块和全球平均池的堆栈来生成其估计值。初始块之间的最大池化降低了维度。第一个模块类似于AlexNet和LeNet。块的堆栈是从VGG继承的，全局平均池避免了在末尾出现一堆完全连接的层。\\n+\\n+![The GoogLeNet architecture.](../img/inception-full.svg)\\n+:label:`fig_inception_full`\\n+\\n+我们现在可以一块一块地实现GoogLeNet。第一模块使用64信道$7\\\\times 7$卷积层。\\n+\\n+```{.python .input}\\n+b1 = nn.Sequential()\\n+b1.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3, activation=\\'relu\\'),\\n+       nn.MaxPool2D(pool_size=3, strides=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\\n+                   nn.ReLU(),\\n+                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def b1():\\n+    return tf.keras.models.Sequential([\\n+        tf.keras.layers.Conv2D(64, 7, strides=2, padding=\\'same\\',\\n+                               activation=\\'relu\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\\'same\\')])\\n+```\\n+\\n+第二个模块使用两个卷积层：首先，64信道$1\\\\times 1$卷积层，然后是$3\\\\times 3$卷积层，它将信道数增加三倍。这对应于初始块中的第二条路径。\\n+\\n+```{.python .input}\\n+b2 = nn.Sequential()\\n+b2.add(nn.Conv2D(64, kernel_size=1, activation=\\'relu\\'),\\n+       nn.Conv2D(192, kernel_size=3, padding=1, activation=\\'relu\\'),\\n+       nn.MaxPool2D(pool_size=3, strides=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\\n+                   nn.ReLU(),\\n+                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\\n+                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def b2():\\n+    return tf.keras.Sequential([\\n+        tf.keras.layers.Conv2D(64, 1, activation=\\'relu\\'),\\n+        tf.keras.layers.Conv2D(192, 3, padding=\\'same\\', activation=\\'relu\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\\'same\\')])\\n+```\\n+\\n+第三个模块串联两个完整的初始模块。第一起始块的输出信道数为$64+128+32+32=256$，四条路径中的输出信道比为$64:128:32:32=2:4:1:1$。第二和第三路径首先将输入信道的数目分别减少到$96/192=1/2$和$16/192=1/12$，然后连接第二卷积层。第二起始块的输出信道的数目增加到$128+192+96+64=480$，并且四个路径中的输出信道比的数目为$128:192:96:64 = 4:6:3:2$。第二和第三路径首先将输入信道的数目分别减少到$128/256=1/2$和$32/256=1/8$。\\n+\\n+```{.python .input}\\n+b3 = nn.Sequential()\\n+b3.add(Inception(64, (96, 128), (16, 32), 32),\\n+       Inception(128, (128, 192), (32, 96), 64),\\n+       nn.MaxPool2D(pool_size=3, strides=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),\\n+                   Inception(256, 128, (128, 192), (32, 96), 64),\\n+                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def b3():\\n+    return tf.keras.models.Sequential([\\n+        Inception(64, (96, 128), (16, 32), 32),\\n+        Inception(128, (128, 192), (32, 96), 64),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\\'same\\')])\\n+```\\n+\\n+第四个模块更复杂。它串联五个起始块，它们分别有$192+208+48+64=512$、$160+224+64+64=512$、$128+256+64+64=512$、$112+288+64+64=528$和$256+320+128+128=832$个输出通道。分配给这些路径的信道数与第三模块中的信道数相似：具有$3\\\\times 3$卷积层的第二条路径输出最大数量的信道，第二条路径仅具有$1\\\\times 1$卷积层，第三条路径具有$5\\\\times 5$卷积层，第四条路径有$3\\\\times 3$个最大池层。第二和第三路径将首先根据比率减少信道的数量。这些比率在不同的初始阶段略有不同。\\n+\\n+```{.python .input}\\n+b4 = nn.Sequential()\\n+b4.add(Inception(192, (96, 208), (16, 48), 64),\\n+       Inception(160, (112, 224), (24, 64), 64),\\n+       Inception(128, (128, 256), (24, 64), 64),\\n+       Inception(112, (144, 288), (32, 64), 64),\\n+       Inception(256, (160, 320), (32, 128), 128),\\n+       nn.MaxPool2D(pool_size=3, strides=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\\n+                   Inception(512, 160, (112, 224), (24, 64), 64),\\n+                   Inception(512, 128, (128, 256), (24, 64), 64),\\n+                   Inception(512, 112, (144, 288), (32, 64), 64),\\n+                   Inception(528, 256, (160, 320), (32, 128), 128),\\n+                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def b4():\\n+    return tf.keras.Sequential([\\n+        Inception(192, (96, 208), (16, 48), 64),\\n+        Inception(160, (112, 224), (24, 64), 64),\\n+        Inception(128, (128, 256), (24, 64), 64),\\n+        Inception(112, (144, 288), (32, 64), 64),\\n+        Inception(256, (160, 320), (32, 128), 128),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\\'same\\')])\\n+```\\n+\\n+第五个模块有两个起始块，分别有$256+320+128+128=832$和$384+384+128+128=1024$个输出通道。分配给每个路径的信道数与第三和第四模块中的相同，但具体值不同。应该注意的是，第五块后面是输出层。将NiN的平均高度更改为该块中每个层的平均宽度。最后，我们将输出转换成一个二维数组，后面是一个完全连接的层，其输出数量是标签类的数量。\\n+\\n+```{.python .input}\\n+b5 = nn.Sequential()\\n+b5.add(Inception(256, (160, 320), (32, 128), 128),\\n+       Inception(384, (192, 384), (48, 128), 128),\\n+       nn.GlobalAvgPool2D())\\n+\\n+net = nn.Sequential()\\n+net.add(b1, b2, b3, b4, b5, nn.Dense(10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\\n+                   Inception(832, 384, (192, 384), (48, 128), 128),\\n+                   nn.AdaptiveMaxPool2d((1,1)),\\n+                   nn.Flatten())\\n+\\n+net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def b5():\\n+    return tf.keras.Sequential([\\n+        Inception(256, (160, 320), (32, 128), 128),\\n+        Inception(384, (192, 384), (48, 128), 128),\\n+        tf.keras.layers.GlobalAvgPool2D(),\\n+        tf.keras.layers.Flatten()\\n+    ])\\n+# Recall that this has to be a function that will be passed to\\n+# `d2l.train_ch6()` so that model building/compiling need to be within\\n+# `strategy.scope()` in order to utilize the CPU/GPU devices that we have\\n+def net():\\n+    return tf.keras.Sequential([b1(), b2(), b3(), b4(), b5(),\\n+                                tf.keras.layers.Dense(10)])\\n+```\\n+\\n+GoogLeNet模型计算复杂，因此不像VGG那样容易修改信道数量。为了有一个合理的训练时间，我们把输入的高度和宽度从224减少到96。这简化了计算。各个模块之间输出形状的变化如下所示。\\n+\\n+```{.python .input}\\n+X = np.random.uniform(size=(1, 1, 96, 96))\\n+net.initialize()\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.name, \\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+X = torch.rand(size=(1, 1, 96, 96))\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__,\\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.random.uniform(shape=(1, 96, 96, 1))\\n+for layer in net().layers:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__, \\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+## 培训\\n+\\n+和以前一样，我们使用时尚MNIST数据集训练我们的模型。在调用训练过程之前，我们将其转换为$96 \\\\times 96$像素分辨率。\\n+\\n+```{.python .input}\\n+#@tab all\\n+lr, num_epochs, batch_size = 0.1, 10, 128\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\\n+d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+## 摘要\\n+\\n+* 初始块相当于具有四条路径的子网。它通过不同窗口形状的卷积层和最大池层并行提取信息。$1 \\\\times 1$卷积降低了每像素级别上的信道维数。最大池会降低分辨率。\\n+* GoogLeNet将多个设计良好的初始块与系列中的其他层连接起来。通过对ImageNet数据集的大量实验，得到了初始块中分配的信道数的比值。\\n+* GoogLeNet，以及它的后续版本，是ImageNet上最有效的模型之一，它以较低的计算复杂度提供了类似的测试精度。\\n+\\n+## 练习\\n+\\n+1. GoogLeNet有几个迭代。尝试实现并运行它们。其中包括以下内容：\\n+    * 添加批处理规范化层:cite:`Ioffe.Szegedy.2015`，如后面:numref:`sec_batch_norm`中所述。\\n+    * 对起始区块:cite:`Szegedy.Vanhoucke.Ioffe.ea.2016`进行调整。\\n+    * 使用标签平滑进行模型正则化:cite:`Szegedy.Vanhoucke.Ioffe.ea.2016`。\\n+    * 如:numref:`sec_resnet`后面所述，将其包括在剩余连接:cite:`Szegedy.Ioffe.Vanhoucke.ea.2017`中。\\n+1. Glento Gooet的最小尺寸是多少？\\n+1. 比较AlexNet、VGG和NiN与GoogLeNet的模型参数大小。后两种网络结构如何显著减小模型参数的大小？\\n+1. 为什么我们一开始需要长程卷积？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/81)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/82)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/316)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-modern/googlenet_tencent.md b/chapter_convolutional-modern/googlenet_tencent.md\\nnew file mode 100644\\nindex 00000000..4e259c6a\\n--- /dev/null\\n+++ b/chapter_convolutional-modern/googlenet_tencent.md\\n@@ -0,0 +1,343 @@\\n+# 具有并行级联的网络(GoogLeNet)\\n+:label:`sec_googlenet`\\n+\\n+2014年，*GoogleNet*赢得了ImageNet挑战赛，提出了一个结合了NIN的优势和重复区块:cite:`Szegedy.Liu.Jia.ea.2015`的范例的结构。本文的一个重点是解决哪种大小的卷积核是最好的问题。毕竟，以前流行的电视网使用的选择小到$1 \\\\times 1$，大到$11 \\\\times 11$。本文中的一个见解是，有时使用不同大小的内核组合可能是有利的。在本节中，我们将介绍GoogLeNet，并提供原始模型的一个稍微简化的版本：我们省略了一些特别的功能，这些功能是为了稳定训练而添加的，但随着更好的训练算法的提供，这些功能现在是不必要的。\\n+\\n+## 起始块\\n+\\n+谷歌乐网中基本的卷积挡路被称为“盗梦空间挡路”，可能是因为电影“盗梦空间”中的一句话(“我们需要走得更深”)而得名，电影“盗梦空间”引发了病毒式的表情包。\\n+\\n+![Structure of the Inception block.](../img/inception.svg)\\n+:label:`fig_inception`\\n+\\n+如:numref:`fig_inception`所示，初始挡路由四条平行路径组成。前三条路径使用窗口大小为$1\\\\times 1$、$3\\\\times 3$和$5\\\\times 5$的卷积层从不同的空间大小提取信息。中间两条路径对输入执行$1\\\\times 1$卷积，以减少通道数量，从而降低模型的复杂性。第四条路径使用$3\\\\times 3$的最大池层，然后使用$1\\\\times 1$的卷积层来改变信道的数量。这四条路径都使用适当的填充来赋予输入和输出相同的高度和宽度。最后，沿每条路径的输出沿通道维度串联，构成挡路的输出。盗梦空间挡路通常调优的超参数是每层的输出通道数。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+class Inception(nn.Block):\\n+    # `c1`--`c4` are the number of output channels for each path\\n+    def __init__(self, c1, c2, c3, c4, **kwargs):\\n+        super(Inception, self).__init__(**kwargs)\\n+        # Path 1 is a single 1 x 1 convolutional layer\\n+        self.p1_1 = nn.Conv2D(c1, kernel_size=1, activation=\\'relu\\')\\n+        # Path 2 is a 1 x 1 convolutional layer followed by a 3 x 3\\n+        # convolutional layer\\n+        self.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation=\\'relu\\')\\n+        self.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1,\\n+                              activation=\\'relu\\')\\n+        # Path 3 is a 1 x 1 convolutional layer followed by a 5 x 5\\n+        # convolutional layer\\n+        self.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation=\\'relu\\')\\n+        self.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2,\\n+                              activation=\\'relu\\')\\n+        # Path 4 is a 3 x 3 maximum pooling layer followed by a 1 x 1\\n+        # convolutional layer\\n+        self.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)\\n+        self.p4_2 = nn.Conv2D(c4, kernel_size=1, activation=\\'relu\\')\\n+\\n+    def forward(self, x):\\n+        p1 = self.p1_1(x)\\n+        p2 = self.p2_2(self.p2_1(x))\\n+        p3 = self.p3_2(self.p3_1(x))\\n+        p4 = self.p4_2(self.p4_1(x))\\n+        # Concatenate the outputs on the channel dimension\\n+        return np.concatenate((p1, p2, p3, p4), axis=1)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+from torch.nn import functional as F\\n+\\n+class Inception(nn.Module):\\n+    # `c1`--`c4` are the number of output channels for each path\\n+    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\\n+        super(Inception, self).__init__(**kwargs)\\n+        # Path 1 is a single 1 x 1 convolutional layer\\n+        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\\n+        # Path 2 is a 1 x 1 convolutional layer followed by a 3 x 3\\n+        # convolutional layer\\n+        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\\n+        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\\n+        # Path 3 is a 1 x 1 convolutional layer followed by a 5 x 5\\n+        # convolutional layer\\n+        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\\n+        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\\n+        # Path 4 is a 3 x 3 maximum pooling layer followed by a 1 x 1\\n+        # convolutional layer\\n+        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\\n+        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\\n+\\n+    def forward(self, x):\\n+        p1 = F.relu(self.p1_1(x))\\n+        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\\n+        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\\n+        p4 = F.relu(self.p4_2(self.p4_1(x)))\\n+        # Concatenate the outputs on the channel dimension\\n+        return torch.cat((p1, p2, p3, p4), dim=1)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+class Inception(tf.keras.Model):\\n+    # `c1`--`c4` are the number of output channels for each path\\n+    def __init__(self, c1, c2, c3, c4):\\n+        super().__init__()\\n+        # Path 1 is a single 1 x 1 convolutional layer\\n+        self.p1_1 = tf.keras.layers.Conv2D(c1, 1, activation=\\'relu\\')\\n+        # Path 2 is a 1 x 1 convolutional layer followed by a 3 x 3\\n+        # convolutional layer\\n+        self.p2_1 = tf.keras.layers.Conv2D(c2[0], 1, activation=\\'relu\\')\\n+        self.p2_2 = tf.keras.layers.Conv2D(c2[1], 3, padding=\\'same\\',\\n+                                           activation=\\'relu\\')\\n+        # Path 3 is a 1 x 1 convolutional layer followed by a 5 x 5\\n+        # convolutional layer\\n+        self.p3_1 = tf.keras.layers.Conv2D(c3[0], 1, activation=\\'relu\\')\\n+        self.p3_2 = tf.keras.layers.Conv2D(c3[1], 5, padding=\\'same\\',\\n+                                           activation=\\'relu\\')\\n+        # Path 4 is a 3 x 3 maximum pooling layer followed by a 1 x 1\\n+        # convolutional layer\\n+        self.p4_1 = tf.keras.layers.MaxPool2D(3, 1, padding=\\'same\\')\\n+        self.p4_2 = tf.keras.layers.Conv2D(c4, 1, activation=\\'relu\\')\\n+\\n+\\n+    def call(self, x):\\n+        p1 = self.p1_1(x)\\n+        p2 = self.p2_2(self.p2_1(x))\\n+        p3 = self.p3_2(self.p3_1(x))\\n+        p4 = self.p4_2(self.p4_1(x))\\n+        # Concatenate the outputs on the channel dimension\\n+        return tf.keras.layers.Concatenate()([p1, p2, p3, p4])\\n+```\\n+\\n+要直观了解此网络运行良好的原因，请考虑过滤器的组合。他们在不同的范围内探索图像。这意味着不同程度的细节可以被不同的过滤器有效地识别。同时，我们可以为不同的范围分配不同数量的参数(例如，更多的参数用于短范围，但不能完全忽略长范围)。\\n+\\n+## GoogLeNet模式\\n+\\n+如:numref:`fig_inception_full`所示，GoogleNet使用总共9个初始块的堆栈和全球平均汇集来生成其估计。起始块之间的最大池化降低了维数。第一个模块类似于AlexNet和LeNet。数据块堆栈继承自VGG，全局平均池避免了末端的完全连接层堆栈。\\n+\\n+![The GoogLeNet architecture.](../img/inception-full.svg)\\n+:label:`fig_inception_full`\\n+\\n+我们现在可以一块一块地实现GoogLeNet。第一模块使用64通道$7\\\\times 7$卷积层。\\n+\\n+```{.python .input}\\n+b1 = nn.Sequential()\\n+b1.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3, activation=\\'relu\\'),\\n+       nn.MaxPool2D(pool_size=3, strides=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\\n+                   nn.ReLU(),\\n+                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def b1():\\n+    return tf.keras.models.Sequential([\\n+        tf.keras.layers.Conv2D(64, 7, strides=2, padding=\\'same\\',\\n+                               activation=\\'relu\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\\'same\\')])\\n+```\\n+\\n+第二个模块使用两个卷积层：首先是64通道$1\\\\times 1$卷积层，然后是将通道数增加三倍的$3\\\\times 3$卷积层。这对应于“盗梦空间”挡路中的第二条路径。\\n+\\n+```{.python .input}\\n+b2 = nn.Sequential()\\n+b2.add(nn.Conv2D(64, kernel_size=1, activation=\\'relu\\'),\\n+       nn.Conv2D(192, kernel_size=3, padding=1, activation=\\'relu\\'),\\n+       nn.MaxPool2D(pool_size=3, strides=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\\n+                   nn.ReLU(),\\n+                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\\n+                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def b2():\\n+    return tf.keras.Sequential([\\n+        tf.keras.layers.Conv2D(64, 1, activation=\\'relu\\'),\\n+        tf.keras.layers.Conv2D(192, 3, padding=\\'same\\', activation=\\'relu\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\\'same\\')])\\n+```\\n+\\n+第三个模块串联两个完整的“盗梦空间”模块。第一条盗梦空间挡路的输出通道数为$64+128+32+32=256$，四路输出通道数比为$64:128:32:32=2:4:1:1$。第二和第三路径首先将输入信道的数量分别减少到$96/192=1/2$和$16/192=1/12$，然后连接第二卷积层。第二条盗梦空间挡路的输出通道数增加到$128+192+96+64=480$个，四路输出通道数比为$128:192:96:64 = 4:6:3:2$。第二和第三路径首先将输入通道的数量分别减少到$128/256=1/2$和$32/256=1/8$。\\n+\\n+```{.python .input}\\n+b3 = nn.Sequential()\\n+b3.add(Inception(64, (96, 128), (16, 32), 32),\\n+       Inception(128, (128, 192), (32, 96), 64),\\n+       nn.MaxPool2D(pool_size=3, strides=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),\\n+                   Inception(256, 128, (128, 192), (32, 96), 64),\\n+                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def b3():\\n+    return tf.keras.models.Sequential([\\n+        Inception(64, (96, 128), (16, 32), 32),\\n+        Inception(128, (128, 192), (32, 96), 64),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\\'same\\')])\\n+```\\n+\\n+第四个模块比较复杂。它串联了五个先发模块，它们分别有$192+208+48+64=512$、$160+224+64+64=512$、$128+256+64+64=512$、$112+288+64+64=528$和$256+320+128+128=832$个输出通道。分配给这些路径的信道数量与第三模块中的类似：具有$3\\\\times 3$卷积层的第二路径输出的信道数量最多，其次是仅具有$1\\\\times 1$卷积层的第一路径，具有$5\\\\times 5$卷积层的第三路径，以及具有$3\\\\times 3$最大池层的第四路径。第二条路径和第三条路径将首先根据该比例减少信道数量。这些比率在不同的“盗梦空间”区块中略有不同。\\n+\\n+```{.python .input}\\n+b4 = nn.Sequential()\\n+b4.add(Inception(192, (96, 208), (16, 48), 64),\\n+       Inception(160, (112, 224), (24, 64), 64),\\n+       Inception(128, (128, 256), (24, 64), 64),\\n+       Inception(112, (144, 288), (32, 64), 64),\\n+       Inception(256, (160, 320), (32, 128), 128),\\n+       nn.MaxPool2D(pool_size=3, strides=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\\n+                   Inception(512, 160, (112, 224), (24, 64), 64),\\n+                   Inception(512, 128, (128, 256), (24, 64), 64),\\n+                   Inception(512, 112, (144, 288), (32, 64), 64),\\n+                   Inception(528, 256, (160, 320), (32, 128), 128),\\n+                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def b4():\\n+    return tf.keras.Sequential([\\n+        Inception(192, (96, 208), (16, 48), 64),\\n+        Inception(160, (112, 224), (24, 64), 64),\\n+        Inception(128, (128, 256), (24, 64), 64),\\n+        Inception(112, (144, 288), (32, 64), 64),\\n+        Inception(256, (160, 320), (32, 128), 128),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\\'same\\')])\\n+```\\n+\\n+第五个模块有两个初始模块，分别有$256+320+128+128=832$和$384+384+128+128=1024$个输出通道。分配给每条路径的通道数与第三和第四个模块中的相同，但在特定值上有所不同。需要注意的是，第五个挡路之后是输出层。此挡路使用全局平均池层将每个通道的高度和宽度更改为1，就像在nin中一样。最后，我们将输出转换为一个二维数组，后面跟着一个完全连接的层，其输出的数量就是标签分类的数量。\\n+\\n+```{.python .input}\\n+b5 = nn.Sequential()\\n+b5.add(Inception(256, (160, 320), (32, 128), 128),\\n+       Inception(384, (192, 384), (48, 128), 128),\\n+       nn.GlobalAvgPool2D())\\n+\\n+net = nn.Sequential()\\n+net.add(b1, b2, b3, b4, b5, nn.Dense(10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\\n+                   Inception(832, 384, (192, 384), (48, 128), 128),\\n+                   nn.AdaptiveMaxPool2d((1,1)),\\n+                   nn.Flatten())\\n+\\n+net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def b5():\\n+    return tf.keras.Sequential([\\n+        Inception(256, (160, 320), (32, 128), 128),\\n+        Inception(384, (192, 384), (48, 128), 128),\\n+        tf.keras.layers.GlobalAvgPool2D(),\\n+        tf.keras.layers.Flatten()\\n+    ])\\n+# Recall that this has to be a function that will be passed to\\n+# `d2l.train_ch6()` so that model building/compiling need to be within\\n+# `strategy.scope()` in order to utilize the CPU/GPU devices that we have\\n+def net():\\n+    return tf.keras.Sequential([b1(), b2(), b3(), b4(), b5(),\\n+                                tf.keras.layers.Dense(10)])\\n+```\\n+\\n+GoogLeNet模型计算复杂，因此不像在VGG中那样容易修改通道数。为了在Fashion-MNIST上有一个合理的训练时间，我们将输入的高度和宽度从224降低到96。这简化了计算。不同模块之间输出形状的变化如下所示。\\n+\\n+```{.python .input}\\n+X = np.random.uniform(size=(1, 1, 96, 96))\\n+net.initialize()\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.name, \\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+X = torch.rand(size=(1, 1, 96, 96))\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__,\\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.random.uniform(shape=(1, 96, 96, 1))\\n+for layer in net().layers:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__, \\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+## 培训\\n+\\n+与前面一样，我们使用Fashion-MNIST数据集训练我们的模型。在调用训练过程之前，我们将其转换为$96 \\\\times 96$像素分辨率。\\n+\\n+```{.python .input}\\n+#@tab all\\n+lr, num_epochs, batch_size = 0.1, 10, 128\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\\n+d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+## 摘要\\n+\\n+* 先发挡路相当于一个有四条路径的子网。它通过不同窗口形状的卷积层和最大汇聚层并行提取信息。$1 \\\\times 1$卷积降低了每像素级的通道维数。最大池容量会降低分辨率。\\n+* GoogLeNet将多个设计良好的Inception块与其他层串联起来。《盗梦空间挡路》中分配的通道数比例是通过在ImageNet数据集上进行大量实验得到的。\\n+* GoogLeNet及其后续版本是ImageNet上最有效的模型之一，提供了类似的测试精度和较低的计算复杂度。\\n+\\n+## 练习\\n+\\n+1. GoogLeNet有几次迭代。尝试实现和运行它们。其中一些包括以下内容：\\n+    * 添加批归一化层:cite:`Ioffe.Szegedy.2015`，如稍后在:numref:`sec_batch_norm`中所述。\\n+    * 对盗梦空间挡路:cite:`Szegedy.Vanhoucke.Ioffe.ea.2016`进行调整。\\n+    * 对模型正则化:cite:`Szegedy.Vanhoucke.Ioffe.ea.2016`使用标签平滑。\\n+    * 包括在剩余连接:cite:`Szegedy.Ioffe.Vanhoucke.ea.2017`中，如稍后在:numref:`sec_resnet`中描述的。\\n+1. GoogLeNet工作的最小图像大小是多少？\\n+1. 将AlexNet、VGG和nin的模型参数大小与GoogLeNet进行比较。后两种网络架构如何显著降低模型参数大小？\\n+1. 为什么我们一开始需要长距离卷积呢？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/81)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/82)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/316)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-modern/index_baidu.md b/chapter_convolutional-modern/index_baidu.md\\nnew file mode 100644\\nindex 00000000..94d0b959\\n--- /dev/null\\n+++ b/chapter_convolutional-modern/index_baidu.md\\n@@ -0,0 +1,20 @@\\n+# 现代卷积神经网络\\n+:label:`chap_modern_cnn`\\n+\\n+既然我们了解了连接CNN的基本原理，我们将带您参观一下现代CNN架构。CNN的许多研究项目（或者说每一章都是建立在这一章的基础上的）这些项目都是建立在这一章的基础上的。这些网络中的每一个都是一个简单的主导体系结构，许多都是ImageNet竞赛的优胜者或亚军，该竞赛自2010年以来一直是计算机视觉监督学习进展的晴雨表。\\n+\\n+这些模型包括AlexNet，第一个在大规模视觉挑战中击败传统计算机视觉方法的大型网络；VGG网络，它利用许多重复的元素块；网络中的网络（NiN），它将整个神经网络逐块卷积在输入上；GoogLeNet，它使用具有并行连接的网络；剩余网络（ResNet），它仍然是计算机视觉中最流行的现成体系结构；以及密集连接网络（DenseNet），计算成本很高，但已经设置了一些最新的基准。\\n+\\n+虽然“深度”神经网络的概念非常简单（将一堆层堆叠在一起），但是性能可以在不同的体系结构和超参数选择中发生很大变化。本章所描述的神经网络是直觉、一些数学见解和大量尝试和错误的产物。我们按时间顺序展示这些模型，部分是为了传达一种历史感，这样您就可以对该领域的发展方向形成自己的直觉，或许还可以开发自己的架构。例如，本章描述的批处理规范化和剩余连接为深层模型的训练和设计提供了两种流行的思想。\\n+\\n+```toc\\n+:maxdepth: 2\\n+\\n+alexnet\\n+vgg\\n+nin\\n+googlenet\\n+batch-norm\\n+resnet\\n+densenet\\n+```\\ndiff --git a/chapter_convolutional-modern/index_tencent.md b/chapter_convolutional-modern/index_tencent.md\\nnew file mode 100644\\nindex 00000000..e91df011\\n--- /dev/null\\n+++ b/chapter_convolutional-modern/index_tencent.md\\n@@ -0,0 +1,20 @@\\n+# 现代卷积神经网络\\n+:label:`chap_modern_cnn`\\n+\\n+既然我们了解了将CNN连接在一起的基础知识，我们将带您参观一下现代CNN架构。在本章中，每一节都对应于一个重要的CNN架构，该架构在某种程度上(或当前)是构建许多研究项目和部署系统的基础模型。这些网络中的每一个都曾一度占据主导地位，其中许多都是ImageNet竞赛的获胜者或亚军，自2010年以来，ImageNet竞赛一直是计算机视觉领域监督学习进展的晴雨表。\\n+\\n+这些模型包括AlexNet，它是第一个在大规模视觉挑战中击败传统计算机视觉方法的大规模网络；VGG网络，它利用了许多重复的元素块；网络中的网络(NIN)，它将整个神经网络分块卷积到输入上；GoogLeNet，它使用具有并行级联的网络；残差网络(ResNet)，它仍然是计算机视觉中最流行的现成体系结构；以及密集连接网络(DenseNet)，它使用具有并行级联的网络\\n+\\n+虽然“深度”神经网络的概念非常简单(将一堆层堆叠在一起)，但不同的体系结构和超参数选择的性能可能会有很大差异。本章描述的神经网络是直觉、一些数学见解和大量试验和错误的产物。我们按时间顺序介绍这些模型，部分原因是为了传达一种历史感，以便您可以形成自己对该领域发展方向的直觉，或许还可以开发您自己的体系结构。例如，本章中描述的批量规范化和剩余连接为训练和设计深层模型提供了两种流行的想法。\\n+\\n+```toc\\n+:maxdepth: 2\\n+\\n+alexnet\\n+vgg\\n+nin\\n+googlenet\\n+batch-norm\\n+resnet\\n+densenet\\n+```\\ndiff --git a/chapter_convolutional-modern/nin_baidu.md b/chapter_convolutional-modern/nin_baidu.md\\nnew file mode 100644\\nindex 00000000..c8108e84\\n--- /dev/null\\n+++ b/chapter_convolutional-modern/nin_baidu.md\\n@@ -0,0 +1,191 @@\\n+# 网络中的网络（NiN）\\n+:label:`sec_nin`\\n+\\n+LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积和池化层来提取利用空间结构的特征，然后通过完全连接的层对表示进行后处理。AlexNet和VGG对LeNet的改进主要在于后来的网络如何扩展和深化这两个模块。或者，可以想象在这个过程的早期使用完全连接的层。然而，如果不小心使用密集层，可能会完全放弃表现的空间结构，\\n+*network-in-network*（*NiN*）块提供了另一种选择。\\n+它们是基于一个非常简单的见解提出的：在每个像素的通道上分别使用MLP :cite:`Lin.Chen.Yan.2013`。\\n+\\n+## NiN块\\n+\\n+回想一下，卷积层的输入和输出由四维张量组成，其轴与示例、通道、高度和宽度相对应。还记得，完全连接层的输入和输出通常是与示例和特征相对应的二维张量。NiN背后的想法是在每个像素位置（对于每个高度和宽度）应用一个完全连接的层。如果我们在每个空间位置绑定权重，我们可以将其视为$1\\\\times 1$卷积层（如:numref:`sec_channels`中所述）或独立于每个像素位置的完全连接层。另一种查看方法是将空间维度中的每个元素（高度和宽度）视为与示例等效，将通道视为等效于要素。\\n+\\n+:numref:`fig_nin`说明了VGG和NiN及其区块之间的主要结构差异。NiN块由一个卷积层和两个$1\\\\times 1$卷积层组成，它们作为具有ReLU激活的每像素完全连接层。第一层的卷积窗口形状通常由用户设置。随后的窗造型固定为$1 \\\\times 1$。\\n+\\n+![Comparing architectures of VGG and NiN, and their blocks.](../img/nin.svg)\\n+:width:`600px`\\n+:label:`fig_nin`\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+def nin_block(num_channels, kernel_size, strides, padding):\\n+    blk = nn.Sequential()\\n+    blk.add(nn.Conv2D(num_channels, kernel_size, strides, padding,\\n+                      activation=\\'relu\\'),\\n+            nn.Conv2D(num_channels, kernel_size=1, activation=\\'relu\\'),\\n+            nn.Conv2D(num_channels, kernel_size=1, activation=\\'relu\\'))\\n+    return blk\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+\\n+def nin_block(in_channels, out_channels, kernel_size, strides, padding):\\n+    return nn.Sequential(\\n+        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),\\n+        nn.ReLU(),\\n+        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),\\n+        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+def nin_block(num_channels, kernel_size, strides, padding):\\n+    return tf.keras.models.Sequential([\\n+        tf.keras.layers.Conv2D(num_channels, kernel_size, strides=strides,\\n+                               padding=padding, activation=\\'relu\\'),\\n+        tf.keras.layers.Conv2D(num_channels, kernel_size=1,\\n+                               activation=\\'relu\\'),\\n+        tf.keras.layers.Conv2D(num_channels, kernel_size=1,\\n+                               activation=\\'relu\\')])\\n+```\\n+\\n+## 没有模型\\n+\\n+最初的NiN网络是在AlexNet之后不久被提出的，显然从中得到了一些启示。NiN使用窗口形状为$11\\\\times 11$、$5\\\\times 5$和$3\\\\times 3$的卷积层，相应的输出信道数与AlexNet中相同。每个NiN块后面是一个最大的池层，步长为2，窗口形状为$3\\\\times 3$。\\n+\\n+NiN和AlexNet的一个显著区别是NiN完全避免了完全连接的层。相反，NiN使用一个NiN块，其输出通道数等于label类的数量，后跟一个*global*average pooling layer，生成一个logits向量。NiN设计的一个优点是它显著减少了所需模型参数的数量。然而，在实践中，这种设计有时需要增加模型训练时间。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nin_block(96, kernel_size=11, strides=4, padding=0),\\n+        nn.MaxPool2D(pool_size=3, strides=2),\\n+        nin_block(256, kernel_size=5, strides=1, padding=2),\\n+        nn.MaxPool2D(pool_size=3, strides=2),\\n+        nin_block(384, kernel_size=3, strides=1, padding=1),\\n+        nn.MaxPool2D(pool_size=3, strides=2),\\n+        nn.Dropout(0.5),\\n+        # There are 10 label classes\\n+        nin_block(10, kernel_size=3, strides=1, padding=1),\\n+        # The global average pooling layer automatically sets the window shape\\n+        # to the height and width of the input\\n+        nn.GlobalAvgPool2D(),\\n+        # Transform the four-dimensional output into two-dimensional output\\n+        # with a shape of (batch size, 10)\\n+        nn.Flatten())\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(\\n+    nin_block(1, 96, kernel_size=11, strides=4, padding=0),\\n+    nn.MaxPool2d(3, stride=2),\\n+    nin_block(96, 256, kernel_size=5, strides=1, padding=2),\\n+    nn.MaxPool2d(3, stride=2),\\n+    nin_block(256, 384, kernel_size=3, strides=1, padding=1),\\n+    nn.MaxPool2d(3, stride=2),\\n+    nn.Dropout(0.5),\\n+    # There are 10 label classes\\n+    nin_block(384, 10, kernel_size=3, strides=1, padding=1),\\n+    nn.AdaptiveAvgPool2d((1, 1)),\\n+    # Transform the four-dimensional output into two-dimensional output with a\\n+    # shape of (batch size, 10)\\n+    nn.Flatten())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def net():\\n+    return tf.keras.models.Sequential([\\n+        nin_block(96, kernel_size=11, strides=4, padding=\\'valid\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\\n+        nin_block(256, kernel_size=5, strides=1, padding=\\'same\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\\n+        nin_block(384, kernel_size=3, strides=1, padding=\\'same\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\\n+        tf.keras.layers.Dropout(0.5),\\n+        # There are 10 label classes\\n+        nin_block(10, kernel_size=3, strides=1, padding=\\'same\\'),\\n+        tf.keras.layers.GlobalAveragePooling2D(),\\n+        tf.keras.layers.Reshape((1, 1, 10)),\\n+        # Transform the four-dimensional output into two-dimensional output\\n+        # with a shape of (batch size, 10)\\n+        tf.keras.layers.Flatten(),\\n+        ])\\n+```\\n+\\n+我们创建一个数据示例来查看每个块的输出形状。\\n+\\n+```{.python .input}\\n+X = np.random.uniform(size=(1, 1, 224, 224))\\n+net.initialize()\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.name, \\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+X = torch.rand(size=(1, 1, 224, 224))\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__,\\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.random.uniform((1, 224, 224, 1))\\n+for layer in net().layers:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__,\\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+## 培训\\n+\\n+像以前一样，我们用时装设计师来训练模特。宁的训练和亚历克内特和VGG的训练相似。\\n+\\n+```{.python .input}\\n+#@tab all\\n+lr, num_epochs, batch_size = 0.1, 10, 128\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\\n+d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+## 摘要\\n+\\n+* NiN使用由一个卷积层和多个$1\\\\times 1$个卷积层组成的块。这可以在卷积堆栈中使用，以允许更多的每像素非线性。\\n+* NiN删除了完全连接的层，并在将通道数量减少到所需的输出数量（例如，时装MNIST为10）后，用全局平均池（即对所有位置求和）替换它们。\\n+* 移除完全连接的层可减少过度装配。NiN的参数少得多。\\n+* NiN的设计影响了许多后来的CNN设计。\\n+\\n+## 练习\\n+\\n+1. 调整超参数以提高分类精度。\\n+1. 为什么在NiN块中有两个$1\\\\times 1$个卷积层？去掉其中一个，观察分析实验现象。\\n+1. 计算NiN的资源使用情况。\\n+    1. 参数的数量是多少？\\n+    1. 计算量是多少？\\n+    1. 训练期间需要多少记忆？\\n+    1. 预测期间需要多少内存？\\n+1. 一步将$384 \\\\times 5 \\\\times 5$表示法缩减为$10 \\\\times 5 \\\\times 5$表示法可能存在哪些问题？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/79)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/80)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/332)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-modern/nin_tencent.md b/chapter_convolutional-modern/nin_tencent.md\\nnew file mode 100644\\nindex 00000000..42e74fe7\\n--- /dev/null\\n+++ b/chapter_convolutional-modern/nin_tencent.md\\n@@ -0,0 +1,191 @@\\n+# 网络中的网络(Nin)\\n+:label:`sec_nin`\\n+\\n+LENet、AlexNet和VGG都有一个共同的设计模式：通过一系列卷积和汇聚层来提取利用*空间*结构的特征，然后通过完全连通的层对表示进行后处理。AlexNet和VGG对LeNet的改进主要在于这些后来的网络如何拓宽和深化这两个模块。或者，您可以想象在该过程的早期使用完全连接层。然而，不小心使用致密层可能会完全放弃表示的空间结构，\\n+*网络中的网络*(*nin*)块提供了另一种选择。\\n+它们是基于一个非常简单的见解提出的：在通道上对每个像素分别使用mlp :cite:`Lin.Chen.Yan.2013`。\\n+\\n+## NIN块\\n+\\n+回想一下，卷积层的输入和输出由四维张量组成，这些张量的轴对应于示例、通道、高度和宽度。还要回想一下，完全连接层的输入和输出通常是对应于示例和特征的二维张量。nin背后的想法是在每个像素位置(针对每个高度和宽度)应用一个完全连接的层。如果我们将权重连接到每个空间位置，我们可以将其视为$1\\\\times 1$卷积层(如:numref:`sec_channels`中所述)或独立作用于每个像素位置的完全连接层。看待这一点的另一种方法是将空间维度(高度和宽度)中的每个元素视为等同于示例，将通道视为等同于功能。\\n+\\n+:numref:`fig_nin`说明了VGG和NIN之间的主要结构差异，以及它们的块。NIN挡路由一个卷积层和两个$1\\\\times 1$卷积层组成，这两个卷积层充当具有REU激活的每像素全连接层。第一层的卷积窗口形状通常由用户设置。后续窗造型固定为$1 \\\\times 1$。\\n+\\n+![Comparing architectures of VGG and NiN, and their blocks.](../img/nin.svg)\\n+:width:`600px`\\n+:label:`fig_nin`\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+def nin_block(num_channels, kernel_size, strides, padding):\\n+    blk = nn.Sequential()\\n+    blk.add(nn.Conv2D(num_channels, kernel_size, strides, padding,\\n+                      activation=\\'relu\\'),\\n+            nn.Conv2D(num_channels, kernel_size=1, activation=\\'relu\\'),\\n+            nn.Conv2D(num_channels, kernel_size=1, activation=\\'relu\\'))\\n+    return blk\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+\\n+def nin_block(in_channels, out_channels, kernel_size, strides, padding):\\n+    return nn.Sequential(\\n+        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),\\n+        nn.ReLU(),\\n+        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),\\n+        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+def nin_block(num_channels, kernel_size, strides, padding):\\n+    return tf.keras.models.Sequential([\\n+        tf.keras.layers.Conv2D(num_channels, kernel_size, strides=strides,\\n+                               padding=padding, activation=\\'relu\\'),\\n+        tf.keras.layers.Conv2D(num_channels, kernel_size=1,\\n+                               activation=\\'relu\\'),\\n+        tf.keras.layers.Conv2D(num_channels, kernel_size=1,\\n+                               activation=\\'relu\\')])\\n+```\\n+\\n+## NIN模型\\n+\\n+最初的NIN网络是在AlexNet之后不久提出的，显然得到了一些启示。NIN使用窗口形状为$11\\\\times 11$、$5\\\\times 5$和$3\\\\times 3$的卷积层，对应的输出通道数与Alexnet中相同。每个nin挡路后面都有一个最大池层，跨度为2，窗口形状为$3\\\\times 3$。\\n+\\n+nin和AlexNet之间的一个重要区别是，nin完全避免了完全连接层。取而代之的是，nin使用nin挡路，其输出通道的数量等于标签类别的数量，后跟一个*全局*平均池层，从而产生一个logit向量。nin设计的一个优点是它大大减少了所需模型参数的数量。然而，在实践中，这种设计有时需要增加模型训练时间。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nin_block(96, kernel_size=11, strides=4, padding=0),\\n+        nn.MaxPool2D(pool_size=3, strides=2),\\n+        nin_block(256, kernel_size=5, strides=1, padding=2),\\n+        nn.MaxPool2D(pool_size=3, strides=2),\\n+        nin_block(384, kernel_size=3, strides=1, padding=1),\\n+        nn.MaxPool2D(pool_size=3, strides=2),\\n+        nn.Dropout(0.5),\\n+        # There are 10 label classes\\n+        nin_block(10, kernel_size=3, strides=1, padding=1),\\n+        # The global average pooling layer automatically sets the window shape\\n+        # to the height and width of the input\\n+        nn.GlobalAvgPool2D(),\\n+        # Transform the four-dimensional output into two-dimensional output\\n+        # with a shape of (batch size, 10)\\n+        nn.Flatten())\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(\\n+    nin_block(1, 96, kernel_size=11, strides=4, padding=0),\\n+    nn.MaxPool2d(3, stride=2),\\n+    nin_block(96, 256, kernel_size=5, strides=1, padding=2),\\n+    nn.MaxPool2d(3, stride=2),\\n+    nin_block(256, 384, kernel_size=3, strides=1, padding=1),\\n+    nn.MaxPool2d(3, stride=2),\\n+    nn.Dropout(0.5),\\n+    # There are 10 label classes\\n+    nin_block(384, 10, kernel_size=3, strides=1, padding=1),\\n+    nn.AdaptiveAvgPool2d((1, 1)),\\n+    # Transform the four-dimensional output into two-dimensional output with a\\n+    # shape of (batch size, 10)\\n+    nn.Flatten())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def net():\\n+    return tf.keras.models.Sequential([\\n+        nin_block(96, kernel_size=11, strides=4, padding=\\'valid\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\\n+        nin_block(256, kernel_size=5, strides=1, padding=\\'same\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\\n+        nin_block(384, kernel_size=3, strides=1, padding=\\'same\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\\n+        tf.keras.layers.Dropout(0.5),\\n+        # There are 10 label classes\\n+        nin_block(10, kernel_size=3, strides=1, padding=\\'same\\'),\\n+        tf.keras.layers.GlobalAveragePooling2D(),\\n+        tf.keras.layers.Reshape((1, 1, 10)),\\n+        # Transform the four-dimensional output into two-dimensional output\\n+        # with a shape of (batch size, 10)\\n+        tf.keras.layers.Flatten(),\\n+        ])\\n+```\\n+\\n+我们创建一个数据示例来查看每个挡路的输出形状。\\n+\\n+```{.python .input}\\n+X = np.random.uniform(size=(1, 1, 224, 224))\\n+net.initialize()\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.name, \\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+X = torch.rand(size=(1, 1, 224, 224))\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__,\\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.random.uniform((1, 224, 224, 1))\\n+for layer in net().layers:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__,\\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+## 培训\\n+\\n+和以前一样，我们使用Fashion-MNIST来训练模型。Nin的训练类似于AlexNet和VGG的训练。\\n+\\n+```{.python .input}\\n+#@tab all\\n+lr, num_epochs, batch_size = 0.1, 10, 128\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\\n+d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+## 摘要\\n+\\n+* NIN使用由卷积层和多个$1\\\\times 1$卷积层组成的块。这可以在卷积堆栈内使用，以允许更多的每像素非线性。\\n+* NIN在将通道数减少到所需的输出数(例如，对于Fashion-MNIST为10)后，移除完全连接的层，并将其替换为全局平均池(即，对所有位置求和)。\\n+* 删除完全连接的图层可减少过度拟合。NIN的参数要少得多。\\n+* NIN的设计影响了CNN后来的许多设计。\\n+\\n+## 练习\\n+\\n+1. 调整超参数以提高分类精度。\\n+1. 为什么在挡路里会有两个$1\\\\times 1$卷积层？去掉其中一个，然后观察和分析实验现象。\\n+1. 计算nin的资源使用率。\\n+    1. 参数的数量是多少？\\n+    1. 计算量是多少？\\n+    1. 培训期间需要多少内存？\\n+    1. 预测期间需要多少内存？\\n+1. 一步将$384 \\\\times 5 \\\\times 5$表示减少到$10 \\\\times 5 \\\\times 5$表示可能会出现什么问题？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/79)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/80)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/332)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-modern/resnet_baidu.md b/chapter_convolutional-modern/resnet_baidu.md\\nnew file mode 100644\\nindex 00000000..a4367fbc\\n--- /dev/null\\n+++ b/chapter_convolutional-modern/resnet_baidu.md\\n@@ -0,0 +1,374 @@\\n+# 剩余网络（ResNet）\\n+:label:`sec_resnet`\\n+\\n+随着我们设计越来越深的网络，了解添加层如何增加网络的复杂性和表现力变得越来越重要。更重要的是设计网络的能力，在这种网络中，添加层会使网络更具表现力，而不仅仅是与众不同。为了取得一些进展，我们需要一些数学知识。\\n+\\n+## 函数类\\n+\\n+考虑$\\\\mathcal{F}$，一种特定网络架构（连同学习速率和其他超参数设置）可以达到的功能类别。也就是说，对于所有$f \\\\in \\\\mathcal{F}$，存在一些参数集（例如权重和偏差），这些参数可以通过在合适的数据集上进行训练而获得。让我们假设$f^*$是我们真正想要找到的“真相”函数。如果是$\\\\mathcal{F}$，我们的状态很好，但通常我们不会那么幸运。相反，我们将尝试找到一些$f^*_\\\\mathcal{F}$，这是我们在$\\\\mathcal{F}$中的最佳选择。例如，给定一个具有$\\\\mathbf{X}$特性和$\\\\mathbf{y}$标签的数据集，我们可以尝试通过解决以下优化问题来找到它：\\n+\\n+$$f^*_\\\\mathcal{F} := \\\\mathop{\\\\mathrm{argmin}}_f L(\\\\mathbf{X}, \\\\mathbf{y}, f) \\\\text{ subject to } f \\\\in \\\\mathcal{F}.$$\\n+\\n+只有合理的假设是，如果我们设计一个不同的、更强大的体系结构$\\\\mathcal{F}\\'$，我们将获得更好的结果。换句话说，我们预计$f^*_{\\\\mathcal{F}\\'}$比$f^*_{\\\\mathcal{F}}$“更好”。然而，如果$\\\\mathcal{F} \\\\not\\\\subseteq \\\\mathcal{F}\\'$，则无法保证这种情况会发生。事实上，$f^*_{\\\\mathcal{F}\\'}$可能更糟。如:numref:`fig_functionclasses`所示，对于非嵌套函数类，较大的函数类并不总是向“真”函数$f^*$靠拢。例如，在:numref:`fig_functionclasses`的左边，虽然$\\\\mathcal{F}_3$比$f^*$更接近$f^*$，但$\\\\mathcal{F}_6$却离开了，并且不能保证进一步增加复杂性可以减少与$f^*$的距离。对于嵌套函数类:numref:`fig_functionclasses`右侧的$\\\\mathcal{F}_1 \\\\subseteq \\\\ldots \\\\subseteq \\\\mathcal{F}_6$，我们可以从非嵌套函数类中避免上述问题。\\n+\\n+![For non-nested function classes, a larger (indicated by area) function class does not guarantee to get closer to the \"truth\" function ($f^*$). This does not happen in nested function classes.](../img/functionclasses.svg)\\n+:label:`fig_functionclasses`\\n+\\n+因此，只有当较大的函数类包含较小的函数类时，我们才能保证增加它们严格地增加网络的表达能力。对于深度神经网络，如果我们能将新增加的层训练成一个识别函数$f(\\\\mathbf{x}) = \\\\mathbf{x}$，新模型将与原模型一样有效。由于新模型可能会得到更好的解决方案来适应训练数据集，因此增加的层可能会更容易减少训练误差。\\n+\\n+这是他等人提出的问题。当工作在非常深的计算机视觉模型:cite:`He.Zhang.Ren.ea.2016`。他们提出的“剩余网络”（*ResNet*）的核心思想是，每个附加层都应该更容易地包含身份函数作为其元素之一。这些考虑是相当深刻的，但他们导致了一个惊人的简单的解决方案，一个*剩余块*。凭借它，ResNet赢得了2015年ImageNet大规模视觉识别挑战赛。这个设计对如何建立深层神经网络产生了深远的影响。\\n+\\n+## 残余块体\\n+\\n+让我们关注神经网络的局部部分，如:numref:`fig_residual_block`所示。用$\\\\mathbf{x}$表示输入。我们假设我们希望通过学习获得的所需底层映射是$f(\\\\mathbf{x})$，用作顶部激活函数的输入。在:numref:`fig_residual_block`的左边，虚线框内的部分必须直接学习映射$f(\\\\mathbf{x})$。在右边，虚线框内的部分需要学习*残差映射*$f(\\\\mathbf{x}) - \\\\mathbf{x}$，这就是残差块如何获得其名称的。如果身份映射$f(\\\\mathbf{x}) = \\\\mathbf{x}$是所需的底层映射，则残差映射更容易学习：我们只需将虚线框内上部权重层（例如，完全连接层和卷积层）的权重和偏差推到零。:numref:`fig_residual_block`中的右图说明了ResNet的*剩余块*，其中携带层输入$\\\\mathbf{x}$到加法运算符的实线称为*剩余连接*（或*快捷连接*）。使用剩余块，输入可以通过层间的剩余连接更快地向前传播。\\n+\\n+![A regular block (left) and a residual block (right).](../img/residual-block.svg)\\n+:label:`fig_residual_block`\\n+\\n+ResNet遵循VGG的完整$3\\\\times 3$卷积层设计。剩余块具有两个$3\\\\times 3$个卷积层，具有相同数量的输出信道。每个卷积层后面是一个批处理规范化层和一个ReLU激活函数。然后，我们跳过这两个卷积运算，直接在最终的ReLU激活函数之前添加输入。这种设计要求两个卷积层的输出必须与输入具有相同的形状，以便将它们相加。如果我们想要改变通道的数量，我们需要引入一个额外的$1\\\\times 1$卷积层来将输入转换成加法运算所需的形状。让我们看看下面的代码。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+class Residual(nn.Block):  #@save\\n+    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):\\n+        super().__init__(**kwargs)\\n+        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,\\n+                               strides=strides)\\n+        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)\\n+        if use_1x1conv:\\n+            self.conv3 = nn.Conv2D(num_channels, kernel_size=1,\\n+                                   strides=strides)\\n+        else:\\n+            self.conv3 = None\\n+        self.bn1 = nn.BatchNorm()\\n+        self.bn2 = nn.BatchNorm()\\n+\\n+    def forward(self, X):\\n+        Y = npx.relu(self.bn1(self.conv1(X)))\\n+        Y = self.bn2(self.conv2(Y))\\n+        if self.conv3:\\n+            X = self.conv3(X)\\n+        return npx.relu(Y + X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+from torch.nn import functional as F\\n+\\n+class Residual(nn.Module):  #@save\\n+    def __init__(self, input_channels, num_channels,\\n+                 use_1x1conv=False, strides=1):\\n+        super().__init__()\\n+        self.conv1 = nn.Conv2d(input_channels, num_channels,\\n+                               kernel_size=3, padding=1, stride=strides)\\n+        self.conv2 = nn.Conv2d(num_channels, num_channels,\\n+                               kernel_size=3, padding=1)\\n+        if use_1x1conv:\\n+            self.conv3 = nn.Conv2d(input_channels, num_channels,\\n+                                   kernel_size=1, stride=strides)\\n+        else:\\n+            self.conv3 = None\\n+        self.bn1 = nn.BatchNorm2d(num_channels)\\n+        self.bn2 = nn.BatchNorm2d(num_channels)\\n+        self.relu = nn.ReLU(inplace=True)\\n+\\n+    def forward(self, X):\\n+        Y = F.relu(self.bn1(self.conv1(X)))\\n+        Y = self.bn2(self.conv2(Y))\\n+        if self.conv3:\\n+            X = self.conv3(X)\\n+        Y += X\\n+        return F.relu(Y)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+class Residual(tf.keras.Model):  #@save\\n+    def __init__(self, num_channels, use_1x1conv=False, strides=1):\\n+        super().__init__()\\n+        self.conv1 = tf.keras.layers.Conv2D(\\n+            num_channels, padding=\\'same\\', kernel_size=3, strides=strides)\\n+        self.conv2 = tf.keras.layers.Conv2D(\\n+            num_channels, kernel_size=3, padding=\\'same\\')\\n+        self.conv3 = None\\n+        if use_1x1conv:\\n+            self.conv3 = tf.keras.layers.Conv2D(\\n+                num_channels, kernel_size=1, strides=strides)\\n+        self.bn1 = tf.keras.layers.BatchNormalization()\\n+        self.bn2 = tf.keras.layers.BatchNormalization()\\n+\\n+    def call(self, X):\\n+        Y = tf.keras.activations.relu(self.bn1(self.conv1(X)))\\n+        Y = self.bn2(self.conv2(Y))\\n+        if self.conv3 is not None:\\n+            X = self.conv3(X)\\n+        Y += X\\n+        return tf.keras.activations.relu(Y)\\n+```\\n+\\n+此代码生成两种类型的网络：一种是在`use_1x1conv=False`应用ReLU非线性之前将输入添加到输出，另一种是在添加之前通过$1 \\\\times 1$卷积调整信道和分辨率。:numref:`fig_resnet_block`说明了这一点：\\n+\\n+![ResNet block with and without $1 \\\\times 1$ convolution.](../img/resnet-block.svg)\\n+:label:`fig_resnet_block`\\n+\\n+现在让我们看看输入和输出是相同形状的情况。\\n+\\n+```{.python .input}\\n+blk = Residual(3)\\n+blk.initialize()\\n+X = np.random.uniform(size=(4, 3, 6, 6))\\n+blk(X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+blk = Residual(3,3)\\n+X = torch.rand(4, 3, 6, 6)\\n+Y = blk(X)\\n+Y.shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+blk = Residual(3)\\n+X = tf.random.uniform((4, 6, 6, 3))\\n+Y = blk(X)\\n+Y.shape\\n+```\\n+\\n+我们还可以选择将输出高度和宽度减半，同时增加输出通道的数量。\\n+\\n+```{.python .input}\\n+blk = Residual(6, use_1x1conv=True, strides=2)\\n+blk.initialize()\\n+blk(X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+blk = Residual(3,6, use_1x1conv=True, strides=2)\\n+blk(X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+blk = Residual(6, use_1x1conv=True, strides=2)\\n+blk(X).shape\\n+```\\n+\\n+## ResNet模型\\n+\\n+ResNet的前两层与我们前面描述的GoogLeNet是相同的：$7\\\\times 7$卷积层有64个输出通道，步长为2，后面是$3\\\\times 3$最大池层，步长为2。区别是在ResNet中每个卷积层之后添加的批处理规范化层。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\\n+        nn.BatchNorm(), nn.Activation(\\'relu\\'),\\n+        nn.MaxPool2D(pool_size=3, strides=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\\n+                   nn.BatchNorm2d(64), nn.ReLU(),\\n+                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+b1 = tf.keras.models.Sequential([\\n+    tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding=\\'same\\'),\\n+    tf.keras.layers.BatchNormalization(),\\n+    tf.keras.layers.Activation(\\'relu\\'),\\n+    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\\'same\\')])\\n+```\\n+\\n+GoogLeNet使用了由Inception块组成的四个模块。然而，ResNet使用由剩余块组成的四个模块，每个模块使用具有相同数量输出信道的几个剩余块。第一个模块中的通道数与输入通道数相同。由于已经使用了跨距为2的最大池层，因此不必减小高度和宽度。在每个后续模块的第一剩余块中，与前一模块相比，信道数量增加了一倍，并且高度和宽度减半。\\n+\\n+现在，我们实现这个模块。注意，对第一个模块执行了特殊处理。\\n+\\n+```{.python .input}\\n+def resnet_block(num_channels, num_residuals, first_block=False):\\n+    blk = nn.Sequential()\\n+    for i in range(num_residuals):\\n+        if i == 0 and not first_block:\\n+            blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\\n+        else:\\n+            blk.add(Residual(num_channels))\\n+    return blk\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def resnet_block(input_channels, num_channels, num_residuals,\\n+                 first_block=False):\\n+    blk = []\\n+    for i in range(num_residuals):\\n+        if i == 0 and not first_block:\\n+            blk.append(Residual(input_channels, num_channels,\\n+                                use_1x1conv=True, strides=2))\\n+        else:\\n+            blk.append(Residual(num_channels, num_channels))\\n+    return blk\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class ResnetBlock(tf.keras.layers.Layer):\\n+    def __init__(self, num_channels, num_residuals, first_block=False,\\n+                 **kwargs):\\n+        super(ResnetBlock, self).__init__(**kwargs)\\n+        self.residual_layers = []\\n+        for i in range(num_residuals):\\n+            if i == 0 and not first_block:\\n+                self.residual_layers.append(\\n+                    Residual(num_channels, use_1x1conv=True, strides=2))\\n+            else:\\n+                self.residual_layers.append(Residual(num_channels))\\n+\\n+    def call(self, X):\\n+        for layer in self.residual_layers.layers:\\n+            X = layer(X)\\n+        return X\\n+```\\n+\\n+然后，我们将所有模块添加到ResNet中。这里，每个模块使用两个剩余块。\\n+\\n+```{.python .input}\\n+net.add(resnet_block(64, 2, first_block=True),\\n+        resnet_block(128, 2),\\n+        resnet_block(256, 2),\\n+        resnet_block(512, 2))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\\n+b3 = nn.Sequential(*resnet_block(64, 128, 2))\\n+b4 = nn.Sequential(*resnet_block(128, 256, 2))\\n+b5 = nn.Sequential(*resnet_block(256, 512, 2))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+b2 = ResnetBlock(64, 2, first_block=True)\\n+b3 = ResnetBlock(128, 2)\\n+b4 = ResnetBlock(256, 2)\\n+b5 = ResnetBlock(512, 2)\\n+```\\n+\\n+最后，就像GoogLeNet一样，我们添加了一个全局平均池层，然后是完全连接的层输出。\\n+\\n+```{.python .input}\\n+net.add(nn.GlobalAvgPool2D(), nn.Dense(10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(b1, b2, b3, b4, b5,\\n+                    nn.AdaptiveAvgPool2d((1,1)),\\n+                    nn.Flatten(), nn.Linear(512, 10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+# Recall that we define this as a function so we can reuse later and run it\\n+# within `tf.distribute.MirroredStrategy`\\'s scope to utilize various\\n+# computational resources, e.g. GPUs. Also note that even though we have\\n+# created b1, b2, b3, b4, b5 but we will recreate them inside this function\\'s\\n+# scope instead\\n+def net():\\n+    return tf.keras.Sequential([\\n+        # The following layers are the same as b1 that we created earlier\\n+        tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding=\\'same\\'),\\n+        tf.keras.layers.BatchNormalization(),\\n+        tf.keras.layers.Activation(\\'relu\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\\'same\\'),\\n+        # The following layers are the same as b2, b3, b4, and b5 that we\\n+        # created earlier\\n+        ResnetBlock(64, 2, first_block=True),\\n+        ResnetBlock(128, 2),\\n+        ResnetBlock(256, 2),\\n+        ResnetBlock(512, 2),\\n+        tf.keras.layers.GlobalAvgPool2D(),\\n+        tf.keras.layers.Dense(units=10)])\\n+```\\n+\\n+每个模块有4个卷积层（不包括$1\\\\times 1$卷积层）。加上第一个$7\\\\times 7$卷积层和最后一个全连通层，共有18个层。因此，这种模型通常被称为ResNet-18。通过在模块中配置不同数量的信道和剩余块，我们可以创建不同的ResNet模型，例如更深的152层ResNet-152。虽然ResNet的主要架构与GoogLeNet相似，但是ResNet的结构更简单、更容易修改。所有这些因素导致ResNet的迅速和广泛的使用。:numref:`fig_resnet18`描述了完整的ResNet-18。\\n+\\n+![The ResNet-18 architecture.](../img/resnet18.svg)\\n+:label:`fig_resnet18`\\n+\\n+在训练ResNet之前，让我们观察一下ResNet中不同模块的输入形状是如何变化的。在所有以前的架构中，分辨率降低，而通道数量增加，直到全局平均池层聚集所有特性为止。\\n+\\n+```{.python .input}\\n+X = np.random.uniform(size=(1, 1, 224, 224))\\n+net.initialize()\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.name, \\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+X = torch.rand(size=(1, 1, 224, 224))\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__,\\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.random.uniform(shape=(1, 224, 224, 1))\\n+for layer in net().layers:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__,\\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+## 培训\\n+\\n+就像以前一样，我们在时尚MNIST数据集上训练ResNet。\\n+\\n+```{.python .input}\\n+#@tab all\\n+lr, num_epochs, batch_size = 0.05, 10, 256\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\\n+d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+## 摘要\\n+\\n+* 嵌套函数类是理想的。在深层神经网络中学习另一层作为身份函数（尽管这是一个极端情况）应该很容易。\\n+* 残差映射可以更容易地学习同一函数，例如将权重层中的参数推到零。\\n+* 利用残差块可以训练出一个有效的深层神经网络。输入可以通过层间的残余连接更快地向前传播。\\n+* ResNet对随后的深层神经网络的设计产生了重大影响，无论是卷积的还是序列的。\\n+\\n+## 练习\\n+\\n+1. :numref:`fig_inception`中的起始块与剩余块之间的主要区别是什么？在删除了Inception块中的一些路径之后，它们是如何相互关联的？\\n+1. 参考ResNet论文:cite:`He.Zhang.Ren.ea.2016`中的表1，以实现不同的变体。\\n+1. 对于更深层次的网络，ResNet引入了“瓶颈”架构来降低模型的复杂性。试着去实现它。\\n+1. 在ResNet的后续版本中，作者将“卷积、批处理规范化和激活”结构更改为“批处理规范化、激活和卷积”结构。你自己做这个改进。详见:cite:`He.Zhang.Ren.ea.2016*1`中的图1。\\n+1. 为什么即使函数类是嵌套的，我们为什么不能不加限制地增加函数的复杂性呢？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/85)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/86)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/333)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-modern/resnet_tencent.md b/chapter_convolutional-modern/resnet_tencent.md\\nnew file mode 100644\\nindex 00000000..b33ce9da\\n--- /dev/null\\n+++ b/chapter_convolutional-modern/resnet_tencent.md\\n@@ -0,0 +1,374 @@\\n+# 剩余网络(ResNet)\\n+:label:`sec_resnet`\\n+\\n+随着我们设计越来越深入的网络，必须了解添加层如何增加网络的复杂性和表现力。更重要的是设计网络的能力，在这种情况下，添加层严格地使网络更具表现力，而不仅仅是不同。要取得一些进步，我们需要一点数学知识。\\n+\\n+## 函数类\\n+\\n+考虑$\\\\mathcal{F}$，特定网络架构(连同学习率和其他超参数设置)可以达到的功能类别。也就是说，对于所有的$f \\\\in \\\\mathcal{F}$，存在可以通过对合适的数据集进行训练而获得的某一组参数(例如，权重和偏差)。让我们假设$f^*$是我们真正想要找到的“真”函数。如果是在$\\\\mathcal{F}$，我们的状态很好，但通常我们不会那么幸运。取而代之的是，我们将努力找到$f^*_\\\\mathcal{F}$左右，这是我们最好的$\\\\mathcal{F}$以内的赌注。例如，给定具有特征$\\\\mathbf{X}$和标签$\\\\mathbf{y}$的数据集，我们可以尝试通过解决以下优化问题来找到它：\\n+\\n+$$f^*_\\\\mathcal{F} := \\\\mathop{\\\\mathrm{argmin}}_f L(\\\\mathbf{X}, \\\\mathbf{y}, f) \\\\text{ subject to } f \\\\in \\\\mathcal{F}.$$\\n+\\n+如果我们设计一个不同的、功能更强大的体系结构$\\\\mathcal{F}\\'$，我们应该会得到更好的结果，这是唯一合理的假设。换句话说，我们预计$f^*_{\\\\mathcal{F}\\'}$比$f^*_{\\\\mathcal{F}}$“更好”。然而，如果$\\\\mathcal{F} \\\\not\\\\subseteq \\\\mathcal{F}\\'$，甚至不能保证这种情况会发生。事实上，$f^*_{\\\\mathcal{F}\\'}$很可能更糟。如:numref:`fig_functionclasses`所示，对于非嵌套的函数类，较大的函数类并不总是向“真”函数$f^*$移动。例如，在:numref:`fig_functionclasses`的左侧，虽然$\\\\mathcal{F}_3$比$\\\\mathcal{F}_1$更接近$f^*$，但$\\\\mathcal{F}_6$会远离，并且不能保证进一步增加复杂性可以减少与$f^*$的距离。对于嵌套函数类(其中$\\\\mathcal{F}_1 \\\\subseteq \\\\ldots \\\\subseteq \\\\mathcal{F}_6$在:numref:`fig_functionclasses`的右侧)，我们可以从非嵌套函数类中避免上述问题。\\n+\\n+![For non-nested function classes, a larger (indicated by area) function class does not guarantee to get closer to the \"truth\" function ($f^*$). This does not happen in nested function classes.](../img/functionclasses.svg)\\n+:label:`fig_functionclasses`\\n+\\n+因此，只有当较大的函数类包含较小的函数类时，我们才能保证严格增加它们会增加网络的表达能力。对于深度神经网络，如果我们能够将新增的层训练成一个同一性函数$f(\\\\mathbf{x}) = \\\\mathbf{x}$，那么新的模型将和原来的模型一样有效。由于新模型可能会得到更好的解决方案来拟合训练数据集，所以增加的层可能会使其更容易减少训练错误。\\n+\\n+这是他等人提出的问题。在非常深入的计算机视觉模型:cite:`He.Zhang.Ren.ea.2016`上工作时考虑。他们提出的“剩余网络*”(*ResNet*)的核心思想是，每个附加层都应该更容易地包含身份函数作为其元素之一。这些考虑是相当深刻的，但它们导致了一个令人惊讶的简单的解决方案，即*残余挡路*。凭借它，ResNet在2015年的ImageNet大规模视觉识别挑战赛中夺冠。这一设计对如何构建深度神经网络产生了深远的影响。\\n+\\n+## 剩余区块\\n+\\n+让我们关注神经网络的局部部分，如:numref:`fig_residual_block`所示。用$\\\\mathbf{x}$表示输入。我们假设希望通过学习获得的底层映射是$f(\\\\mathbf{x})$，用作顶部激活函数的输入。在:numref:`fig_residual_block`的左侧，虚线框内的部分必须直接学习映射$f(\\\\mathbf{x})$。右边虚线框内的部分需要学习*残差映射*$f(\\\\mathbf{x}) - \\\\mathbf{x}$，残差挡路就是这样得名的。如果身份映射$f(\\\\mathbf{x}) = \\\\mathbf{x}$是期望的底层映射，则残差映射更容易学习：我们只需要将虚线框内的上权重层(例如，全连通层和卷积层)的权重和偏差推至零。:numref:`fig_residual_block`中的右图说明了Resnet的“剩余挡路”，其中将层输入$\\\\mathbf{x}$携带到加法运算符的实线称为“剩余连接”(或“快捷连接”)。有了残留块，输入可以通过层间的残馀连接更快地向前传播。\\n+\\n+![A regular block (left) and a residual block (right).](../img/residual-block.svg)\\n+:label:`fig_residual_block`\\n+\\n+RESNet遵循VGG的全$3\\\\times 3$卷积层设计。剩余挡路具有两个$3\\\\times 3$卷积层，具有相同的输出通道数。每个卷积层之后是批归一化层和RELU激活函数。然后，我们跳过这两个卷积操作，将输入直接添加到最终的REU激活函数之前。这种设计要求两个卷积层的输出必须与输入具有相同的形状，以便它们可以相加在一起。如果我们想要改变通道的数量，我们需要引入额外的$1\\\\times 1$卷积层来将输入转换成加法运算所需的形状。让我们看看下面的代码。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+class Residual(nn.Block):  #@save\\n+    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):\\n+        super().__init__(**kwargs)\\n+        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,\\n+                               strides=strides)\\n+        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)\\n+        if use_1x1conv:\\n+            self.conv3 = nn.Conv2D(num_channels, kernel_size=1,\\n+                                   strides=strides)\\n+        else:\\n+            self.conv3 = None\\n+        self.bn1 = nn.BatchNorm()\\n+        self.bn2 = nn.BatchNorm()\\n+\\n+    def forward(self, X):\\n+        Y = npx.relu(self.bn1(self.conv1(X)))\\n+        Y = self.bn2(self.conv2(Y))\\n+        if self.conv3:\\n+            X = self.conv3(X)\\n+        return npx.relu(Y + X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+from torch.nn import functional as F\\n+\\n+class Residual(nn.Module):  #@save\\n+    def __init__(self, input_channels, num_channels,\\n+                 use_1x1conv=False, strides=1):\\n+        super().__init__()\\n+        self.conv1 = nn.Conv2d(input_channels, num_channels,\\n+                               kernel_size=3, padding=1, stride=strides)\\n+        self.conv2 = nn.Conv2d(num_channels, num_channels,\\n+                               kernel_size=3, padding=1)\\n+        if use_1x1conv:\\n+            self.conv3 = nn.Conv2d(input_channels, num_channels,\\n+                                   kernel_size=1, stride=strides)\\n+        else:\\n+            self.conv3 = None\\n+        self.bn1 = nn.BatchNorm2d(num_channels)\\n+        self.bn2 = nn.BatchNorm2d(num_channels)\\n+        self.relu = nn.ReLU(inplace=True)\\n+\\n+    def forward(self, X):\\n+        Y = F.relu(self.bn1(self.conv1(X)))\\n+        Y = self.bn2(self.conv2(Y))\\n+        if self.conv3:\\n+            X = self.conv3(X)\\n+        Y += X\\n+        return F.relu(Y)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+class Residual(tf.keras.Model):  #@save\\n+    def __init__(self, num_channels, use_1x1conv=False, strides=1):\\n+        super().__init__()\\n+        self.conv1 = tf.keras.layers.Conv2D(\\n+            num_channels, padding=\\'same\\', kernel_size=3, strides=strides)\\n+        self.conv2 = tf.keras.layers.Conv2D(\\n+            num_channels, kernel_size=3, padding=\\'same\\')\\n+        self.conv3 = None\\n+        if use_1x1conv:\\n+            self.conv3 = tf.keras.layers.Conv2D(\\n+                num_channels, kernel_size=1, strides=strides)\\n+        self.bn1 = tf.keras.layers.BatchNormalization()\\n+        self.bn2 = tf.keras.layers.BatchNormalization()\\n+\\n+    def call(self, X):\\n+        Y = tf.keras.activations.relu(self.bn1(self.conv1(X)))\\n+        Y = self.bn2(self.conv2(Y))\\n+        if self.conv3 is not None:\\n+            X = self.conv3(X)\\n+        Y += X\\n+        return tf.keras.activations.relu(Y)\\n+```\\n+\\n+此代码生成两种类型的网络：一种是在每次应用RELU非线性之前将输入添加到输出，无论何时`use_1x1conv=False`；另一种是在添加之前通过$1 \\\\times 1$卷积调整通道和分辨率。:numref:`fig_resnet_block`说明了这一点：\\n+\\n+![ResNet block with and without $1 \\\\times 1$ convolution.](../img/resnet-block.svg)\\n+:label:`fig_resnet_block`\\n+\\n+现在让我们看一下输入和输出形状相同的情况。\\n+\\n+```{.python .input}\\n+blk = Residual(3)\\n+blk.initialize()\\n+X = np.random.uniform(size=(4, 3, 6, 6))\\n+blk(X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+blk = Residual(3,3)\\n+X = torch.rand(4, 3, 6, 6)\\n+Y = blk(X)\\n+Y.shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+blk = Residual(3)\\n+X = tf.random.uniform((4, 6, 6, 3))\\n+Y = blk(X)\\n+Y.shape\\n+```\\n+\\n+我们还可以选择将输出高度和宽度减半，同时增加输出通道的数量。\\n+\\n+```{.python .input}\\n+blk = Residual(6, use_1x1conv=True, strides=2)\\n+blk.initialize()\\n+blk(X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+blk = Residual(3,6, use_1x1conv=True, strides=2)\\n+blk(X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+blk = Residual(6, use_1x1conv=True, strides=2)\\n+blk(X).shape\\n+```\\n+\\n+## RESNET模型\\n+\\n+ResNet的前两层与我们之前描述的GoogLeNet相同：$7\\\\times 7$卷积层，64个输出通道，步长为2，紧随其后的是$3\\\\times 3$最大汇聚层，步长为2，不同的是在Resnet中的每个卷积层之后增加了批归一化层。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\\n+        nn.BatchNorm(), nn.Activation(\\'relu\\'),\\n+        nn.MaxPool2D(pool_size=3, strides=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\\n+                   nn.BatchNorm2d(64), nn.ReLU(),\\n+                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+b1 = tf.keras.models.Sequential([\\n+    tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding=\\'same\\'),\\n+    tf.keras.layers.BatchNormalization(),\\n+    tf.keras.layers.Activation(\\'relu\\'),\\n+    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\\'same\\')])\\n+```\\n+\\n+GoogLeNet使用由Inception块组成的四个模块。然而，ResNet使用由残差块组成的四个模块，每个模块使用具有相同输出通道数的几个残差块。第一模块中的通道数与输入通道数相同。由于已经使用了跨度为2的最大汇聚层，因此没有必要减小高度和宽度。在后续每个模块的第一个剩余挡路中，通道数比前一个模块翻了一番，高度和宽度都减半。\\n+\\n+现在，我们实现此模块。请注意，已经对第一个模块执行了特殊处理。\\n+\\n+```{.python .input}\\n+def resnet_block(num_channels, num_residuals, first_block=False):\\n+    blk = nn.Sequential()\\n+    for i in range(num_residuals):\\n+        if i == 0 and not first_block:\\n+            blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\\n+        else:\\n+            blk.add(Residual(num_channels))\\n+    return blk\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def resnet_block(input_channels, num_channels, num_residuals,\\n+                 first_block=False):\\n+    blk = []\\n+    for i in range(num_residuals):\\n+        if i == 0 and not first_block:\\n+            blk.append(Residual(input_channels, num_channels,\\n+                                use_1x1conv=True, strides=2))\\n+        else:\\n+            blk.append(Residual(num_channels, num_channels))\\n+    return blk\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class ResnetBlock(tf.keras.layers.Layer):\\n+    def __init__(self, num_channels, num_residuals, first_block=False,\\n+                 **kwargs):\\n+        super(ResnetBlock, self).__init__(**kwargs)\\n+        self.residual_layers = []\\n+        for i in range(num_residuals):\\n+            if i == 0 and not first_block:\\n+                self.residual_layers.append(\\n+                    Residual(num_channels, use_1x1conv=True, strides=2))\\n+            else:\\n+                self.residual_layers.append(Residual(num_channels))\\n+\\n+    def call(self, X):\\n+        for layer in self.residual_layers.layers:\\n+            X = layer(X)\\n+        return X\\n+```\\n+\\n+然后，我们将所有模块添加到ResNet中。这里，每个模块使用两个剩余块。\\n+\\n+```{.python .input}\\n+net.add(resnet_block(64, 2, first_block=True),\\n+        resnet_block(128, 2),\\n+        resnet_block(256, 2),\\n+        resnet_block(512, 2))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\\n+b3 = nn.Sequential(*resnet_block(64, 128, 2))\\n+b4 = nn.Sequential(*resnet_block(128, 256, 2))\\n+b5 = nn.Sequential(*resnet_block(256, 512, 2))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+b2 = ResnetBlock(64, 2, first_block=True)\\n+b3 = ResnetBlock(128, 2)\\n+b4 = ResnetBlock(256, 2)\\n+b5 = ResnetBlock(512, 2)\\n+```\\n+\\n+最后，就像GoogLeNet一样，我们添加一个全局平均池层，然后是完全连接层输出。\\n+\\n+```{.python .input}\\n+net.add(nn.GlobalAvgPool2D(), nn.Dense(10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(b1, b2, b3, b4, b5,\\n+                    nn.AdaptiveAvgPool2d((1,1)),\\n+                    nn.Flatten(), nn.Linear(512, 10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+# Recall that we define this as a function so we can reuse later and run it\\n+# within `tf.distribute.MirroredStrategy`\\'s scope to utilize various\\n+# computational resources, e.g. GPUs. Also note that even though we have\\n+# created b1, b2, b3, b4, b5 but we will recreate them inside this function\\'s\\n+# scope instead\\n+def net():\\n+    return tf.keras.Sequential([\\n+        # The following layers are the same as b1 that we created earlier\\n+        tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding=\\'same\\'),\\n+        tf.keras.layers.BatchNormalization(),\\n+        tf.keras.layers.Activation(\\'relu\\'),\\n+        tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\\'same\\'),\\n+        # The following layers are the same as b2, b3, b4, and b5 that we\\n+        # created earlier\\n+        ResnetBlock(64, 2, first_block=True),\\n+        ResnetBlock(128, 2),\\n+        ResnetBlock(256, 2),\\n+        ResnetBlock(512, 2),\\n+        tf.keras.layers.GlobalAvgPool2D(),\\n+        tf.keras.layers.Dense(units=10)])\\n+```\\n+\\n+每个模块中有4个卷积层(不包括$1\\\\times 1$个卷积层)。加上前$7\\\\times 7$卷积层和最后的全连通层，总共有18层。因此，此型号通常称为ResNet-18。通过在模块中配置不同数量的通道和剩余块，我们可以创建不同的ResNet模型，例如更深层的152层ResNet-152。虽然ResNet的主要架构与GoogLeNet相似，但ResNet的结构更简单，更容易修改。所有这些因素都导致了ResNet的迅速和广泛使用。:numref:`fig_resnet18`描述了完整的Resnet-18。\\n+\\n+![The ResNet-18 architecture.](../img/resnet18.svg)\\n+:label:`fig_resnet18`\\n+\\n+在训练ResNet之前，让我们观察一下ResNet中不同模块之间的输入形状是如何变化的。与所有以前的体系结构一样，在全局平均池层聚合所有功能之前，分辨率会降低，而通道数量会增加。\\n+\\n+```{.python .input}\\n+X = np.random.uniform(size=(1, 1, 224, 224))\\n+net.initialize()\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.name, \\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+X = torch.rand(size=(1, 1, 224, 224))\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__,\\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.random.uniform(shape=(1, 224, 224, 1))\\n+for layer in net().layers:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__,\\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+## 培训\\n+\\n+我们像以前一样，在Fashion-MNIST数据集上训练ResNet。\\n+\\n+```{.python .input}\\n+#@tab all\\n+lr, num_epochs, batch_size = 0.05, 10, 256\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\\n+d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+## 摘要\\n+\\n+* 需要嵌套函数类。在深层神经网络中学习额外的一层作为身份函数(尽管这是一个极端情况)应该很容易。\\n+* 残差映射可以更容易地学习恒等式函数，例如将权层中的参数推到零。\\n+* 我们可以通过残差块来训练一个有效的深度神经网络。输入可以通过层间的剩余连接向前传播得更快。\\n+* RESNET对随后的深层神经网络的设计产生了重大影响，无论是卷积性质还是顺序性质。\\n+\\n+## 练习\\n+\\n+1. :numref:`fig_inception`的盗梦空间挡路和剩馀的挡路有哪些主要区别？在移除了《盗梦空间》挡路中的一些路径之后，它们之间是如何关联的呢？\\n+1. 请参阅Resnet论文:cite:`He.Zhang.Ren.ea.2016`中的表1以实现不同的变体。\\n+1. 对于更深层的网络，ResNet引入了“瓶颈”架构来降低模型复杂性。试着去实现它。\\n+1. 在ResNet的后续版本中，作者将“卷积、批处理标准化和激活”结构更改为“批处理标准化、激活和卷积”结构。你自己做这个改进吧。有关详细信息，请参见:cite:`He.Zhang.Ren.ea.2016*1`中的图1。\\n+1. 为什么即使函数类是嵌套的，我们为什么不能不加限制地增加函数的复杂性呢？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/85)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/86)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/333)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-modern/vgg_baidu.md b/chapter_convolutional-modern/vgg_baidu.md\\nnew file mode 100644\\nindex 00000000..77ffd8fe\\n--- /dev/null\\n+++ b/chapter_convolutional-modern/vgg_baidu.md\\n@@ -0,0 +1,215 @@\\n+# 使用块的网络（VGG）\\n+:label:`sec_vgg`\\n+\\n+虽然AlexNet提供了经验证据证明deepcnns可以获得良好的结果，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。在下面的章节中，我们将介绍几个常用于设计深层网络的启发式概念。\\n+\\n+这一领域的进展反映了芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程。类似地，神经网络结构的设计也逐渐变得更加抽象，研究人员从单个神经元的角度思考问题，发展到整个层次，现在又转向模块，重复各层的模式。\\n+\\n+使用块的想法最早出现在[visualgeometry Group](http://www.robots.ox.ac.uk/~vgg/)（VGG）在牛津大学，在他们的同名网络。通过使用循环和子例程，可以很容易地在任何现代深度学习框架的代码中实现这些重复的结构。\\n+\\n+## VGG区块\\n+\\n+经典cnn的基本组成部分是一系列的：（i）具有填充以保持分辨率的卷积层，（ii）诸如ReLU的非线性，（iii）诸如max pooling层的池层。一个VGG块由一系列卷积层组成，然后是用于空间下采样的最大池层。在最初的VGG论文:cite:`Simonyan.Zisserman.2014`中，作者使用了填充为1（保持高度和宽度）的$3\\\\times3$核的卷积和步长为2（每个块后的分辨率减半）的$2 \\\\times 2$最大池。在下面的代码中，我们定义了一个名为`vgg_block`的函数来实现一个VGG块。该函数接受两个参数，分别对应于卷积层`num_convs`的数量和输出信道的数量`num_channels`。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+def vgg_block(num_convs, num_channels):\\n+    blk = nn.Sequential()\\n+    for _ in range(num_convs):\\n+        blk.add(nn.Conv2D(num_channels, kernel_size=3,\\n+                          padding=1, activation=\\'relu\\'))\\n+    blk.add(nn.MaxPool2D(pool_size=2, strides=2))\\n+    return blk\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+\\n+def vgg_block(num_convs, in_channels, out_channels):\\n+    layers=[]\\n+    for _ in range(num_convs):\\n+        layers.append(nn.Conv2d(in_channels, out_channels,\\n+                                kernel_size=3, padding=1))\\n+        layers.append(nn.ReLU())\\n+        in_channels = out_channels\\n+    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\\n+    return nn.Sequential(*layers)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+def vgg_block(num_convs, num_channels):\\n+    blk = tf.keras.models.Sequential()\\n+    for _ in range(num_convs):\\n+        blk.add(tf.keras.layers.Conv2D(num_channels,kernel_size=3,\\n+                                    padding=\\'same\\',activation=\\'relu\\'))\\n+    blk.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\\n+    return blk\\n+```\\n+\\n+## VGG网络\\n+\\n+与AlexNet和LeNet一样，VGG网络可以分为两部分：第一部分主要由卷积层和池层组成，第二部分由完全连接的层组成。这在:numref:`fig_vgg`中有描述。\\n+\\n+![From AlexNet to VGG that is designed from building blocks.](../img/vgg.svg)\\n+:width:`400px`\\n+:label:`fig_vgg`\\n+\\n+网络的卷积部分连续连接:numref:`fig_vgg`（也在`vgg_block`函数中定义）的几个VGG块。下面的变量`conv_arch`包含一个元组列表（每个块一个），其中每个元组包含两个值：卷积层的数量和输出通道的数量，这正是调用`vgg_block`函数所需的参数。VGG网络的完全连接部分与AlexNet中涵盖的部分相同。\\n+\\n+原VGG网络有5个卷积块，前两个有一个卷积层，后三个包含两个卷积层。第一个块有64个输出信道，每个随后的块将输出信道的数目加倍，直到这个数目达到512个。由于该网络使用8个卷积层和3个完全连接的层，因此通常称为VGG-11。\\n+\\n+```{.python .input}\\n+#@tab all\\n+conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\\n+```\\n+\\n+以下代码实现VGG-11。这是在`conv_arch`上执行for循环的简单问题。\\n+\\n+```{.python .input}\\n+def vgg(conv_arch):\\n+    net = nn.Sequential()\\n+    # The convolutional part\\n+    for (num_convs, num_channels) in conv_arch:\\n+        net.add(vgg_block(num_convs, num_channels))\\n+    # The fully-connected part\\n+    net.add(nn.Dense(4096, activation=\\'relu\\'), nn.Dropout(0.5),\\n+            nn.Dense(4096, activation=\\'relu\\'), nn.Dropout(0.5),\\n+            nn.Dense(10))\\n+    return net\\n+\\n+net = vgg(conv_arch)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def vgg(conv_arch):\\n+    # The convolutional part\\n+    conv_blks=[]\\n+    in_channels=1\\n+    for (num_convs, out_channels) in conv_arch:\\n+        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\\n+        in_channels = out_channels\\n+\\n+    return nn.Sequential(\\n+        *conv_blks, nn.Flatten(),\\n+        # The fully-connected part\\n+        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\\n+        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\\n+        nn.Linear(4096, 10))\\n+\\n+net = vgg(conv_arch)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def vgg(conv_arch):\\n+    net = tf.keras.models.Sequential()\\n+    # The convulational part\\n+    for (num_convs, num_channels) in conv_arch:\\n+        net.add(vgg_block(num_convs, num_channels))\\n+    # The fully-connected part\\n+    net.add(tf.keras.models.Sequential([\\n+        tf.keras.layers.Flatten(),\\n+        tf.keras.layers.Dense(4096, activation=\\'relu\\'),\\n+        tf.keras.layers.Dropout(0.5),\\n+        tf.keras.layers.Dense(4096, activation=\\'relu\\'),\\n+        tf.keras.layers.Dropout(0.5),\\n+        tf.keras.layers.Dense(10)]))\\n+    return net\\n+\\n+net = vgg(conv_arch)\\n+```\\n+\\n+接下来，我们将构造一个高度和宽度为224的单通道数据示例来观察每个层的输出形状。\\n+\\n+```{.python .input}\\n+net.initialize()\\n+X = np.random.uniform(size=(1, 1, 224, 224))\\n+for blk in net:\\n+    X = blk(X)\\n+    print(blk.name, \\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+X = torch.randn(size=(1, 1, 224, 224))\\n+for blk in net:\\n+    X = blk(X)\\n+    print(blk.__class__.__name__,\\'output shape:\\\\t\\',X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.random.uniform((1, 224, 224, 1))\\n+for blk in net.layers:\\n+    X = blk(X)\\n+    print(blk.__class__.__name__,\\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+如您所见，我们将每个块的高度和宽度减半，最终达到7的高度和宽度，然后展开表示以供网络的完全连接部分处理。\\n+\\n+## 培训\\n+\\n+由于VGG-11比AlexNet计算量更大，因此我们构建了一个信道数较少的网络。这对时装设计师的培训已经足够了。\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+ratio = 4\\n+small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\\n+net = vgg(small_conv_arch)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+ratio = 4\\n+small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\\n+# Recall that this has to be a function that will be passed to\\n+# `d2l.train_ch6()` so that model building/compiling need to be within\\n+# `strategy.scope()` in order to utilize the CPU/GPU devices that we have\\n+net = lambda: vgg(small_conv_arch)\\n+```\\n+\\n+除了使用稍大的学习率外，模型训练过程类似于AlexNet在:numref:`sec_alexnet`中的训练过程。\\n+\\n+```{.python .input}\\n+#@tab all\\n+lr, num_epochs, batch_size = 0.05, 10, 128\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\\n+d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+## 摘要\\n+\\n+* VGG-11使用可重用卷积块构造网络。不同的VGG模型可以通过每个块中卷积层和输出通道的数量的差异来定义。\\n+* 块的使用导致网络定义的非常紧凑的表示。它可以有效地设计复杂的网络。\\n+* 在他们的VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是，他们发现几层深的和窄的卷积（即$3 \\\\times 3$）比较少的更宽的卷积更有效。\\n+\\n+## 练习\\n+\\n+1. 当打印出图层的尺寸时，我们只看到了8个结果，而不是11个。剩下的3层信息去了哪里？\\n+1. 与AlexNet相比，VGG的计算速度要慢得多，而且需要更多的GPU内存。分析原因。\\n+1. 试着将时尚MNIST中图片的高度和宽度从224更改为96。这对实验有什么影响？\\n+1. 参考VGG论文:cite:`Simonyan.Zisserman.2014`中的表1构建其他常见模型，如VGG-16或VGG-19。\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/77)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/78)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/277)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-modern/vgg_tencent.md b/chapter_convolutional-modern/vgg_tencent.md\\nnew file mode 100644\\nindex 00000000..404c822f\\n--- /dev/null\\n+++ b/chapter_convolutional-modern/vgg_tencent.md\\n@@ -0,0 +1,215 @@\\n+# 使用块的网络(VGG)\\n+:label:`sec_vgg`\\n+\\n+虽然AlexNet提供了经验证据，证明深度CNN可以取得良好的结果，但它并没有提供一个通用的模板来指导后续的研究人员设计新的网络。在接下来的几节中，我们将介绍几个常用于设计深层网络的启发式概念。\\n+\\n+这一领域的进步反映了芯片设计的进步，在芯片设计中，工程师们从放置晶体管到逻辑元件再到逻辑块。同样，神经网络结构的设计也逐渐变得更加抽象，研究人员从从单个神经元到整个层，再到现在的块，重复层的模式。\\n+\\n+使用块的想法最早出现在牛津大学的[视觉几何小组](http://www.robots.ox.ac.uk/~vgg/))，在他们同名的*VGG*网络中。通过使用循环和子例程，可以使用任何现代深度学习框架在代码中轻松实现这些重复结构。\\n+\\n+## VGG块\\n+\\n+经典CNN的基本构建挡路是以下序列：(I)具有填充以保持分辨率的卷积层，(Ii)诸如RELU的非线性，(Iii)诸如最大池层的池层。一个VGG挡路由一系列卷积层组成，紧随其后的是用于空间下采样的最大汇聚层。在最初的VGG论文:cite:`Simonyan.Zisserman.2014`中，作者使用了$3\\\\times3$个核的卷积，填充为1(保持高度和宽度不变)和$2 \\\\times 2$最大合并，步长为2(每次挡路之后分辨率减半)。在下面的代码中，我们定义了一个名为`vgg_block`的函数来实现一个vgg挡路。该函数接受对应于卷积层`num_convs`的数目和输出通道`num_channels`的数目的两个自变量。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+def vgg_block(num_convs, num_channels):\\n+    blk = nn.Sequential()\\n+    for _ in range(num_convs):\\n+        blk.add(nn.Conv2D(num_channels, kernel_size=3,\\n+                          padding=1, activation=\\'relu\\'))\\n+    blk.add(nn.MaxPool2D(pool_size=2, strides=2))\\n+    return blk\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+\\n+def vgg_block(num_convs, in_channels, out_channels):\\n+    layers=[]\\n+    for _ in range(num_convs):\\n+        layers.append(nn.Conv2d(in_channels, out_channels,\\n+                                kernel_size=3, padding=1))\\n+        layers.append(nn.ReLU())\\n+        in_channels = out_channels\\n+    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\\n+    return nn.Sequential(*layers)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+def vgg_block(num_convs, num_channels):\\n+    blk = tf.keras.models.Sequential()\\n+    for _ in range(num_convs):\\n+        blk.add(tf.keras.layers.Conv2D(num_channels,kernel_size=3,\\n+                                    padding=\\'same\\',activation=\\'relu\\'))\\n+    blk.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\\n+    return blk\\n+```\\n+\\n+## VGG网络\\n+\\n+与AlexNet和LeNet一样，VGG网络可以分为两部分：第一部分主要由卷积和池层组成，第二部分由全连通层组成。:numref:`fig_vgg`中描述了这一点。\\n+\\n+![From AlexNet to VGG that is designed from building blocks.](../img/vgg.svg)\\n+:width:`400px`\\n+:label:`fig_vgg`\\n+\\n+网络的卷积部分连续连接来自:numref:`fig_vgg`(也在`vgg_block`函数中定义)的几个VGG块。以下变量`conv_arch`由元组列表(每个挡路一个)组成，其中每个元组包含两个值：卷积层数和输出通道数，它们恰恰是调用`vgg_block`函数所需的参数。VGG网络的完全连接部分与AlexNet中涵盖的部分完全相同。\\n+\\n+原始的VGG网络有5个卷积块，其中前两个各有一个卷积层，后三个各有两个卷积层。第一个挡路有64个输出通道，每个后续的挡路都会使输出通道的数量翻一番，直到这个数字达到512。由于该网络使用8个卷积层和3个全连接层，因此通常称为VGG-11。\\n+\\n+```{.python .input}\\n+#@tab all\\n+conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\\n+```\\n+\\n+以下代码实现VGG-11。这是一个简单的问题，只需在`conv_arch`上执行一个for循环即可。\\n+\\n+```{.python .input}\\n+def vgg(conv_arch):\\n+    net = nn.Sequential()\\n+    # The convolutional part\\n+    for (num_convs, num_channels) in conv_arch:\\n+        net.add(vgg_block(num_convs, num_channels))\\n+    # The fully-connected part\\n+    net.add(nn.Dense(4096, activation=\\'relu\\'), nn.Dropout(0.5),\\n+            nn.Dense(4096, activation=\\'relu\\'), nn.Dropout(0.5),\\n+            nn.Dense(10))\\n+    return net\\n+\\n+net = vgg(conv_arch)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def vgg(conv_arch):\\n+    # The convolutional part\\n+    conv_blks=[]\\n+    in_channels=1\\n+    for (num_convs, out_channels) in conv_arch:\\n+        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\\n+        in_channels = out_channels\\n+\\n+    return nn.Sequential(\\n+        *conv_blks, nn.Flatten(),\\n+        # The fully-connected part\\n+        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\\n+        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\\n+        nn.Linear(4096, 10))\\n+\\n+net = vgg(conv_arch)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def vgg(conv_arch):\\n+    net = tf.keras.models.Sequential()\\n+    # The convulational part\\n+    for (num_convs, num_channels) in conv_arch:\\n+        net.add(vgg_block(num_convs, num_channels))\\n+    # The fully-connected part\\n+    net.add(tf.keras.models.Sequential([\\n+        tf.keras.layers.Flatten(),\\n+        tf.keras.layers.Dense(4096, activation=\\'relu\\'),\\n+        tf.keras.layers.Dropout(0.5),\\n+        tf.keras.layers.Dense(4096, activation=\\'relu\\'),\\n+        tf.keras.layers.Dropout(0.5),\\n+        tf.keras.layers.Dense(10)]))\\n+    return net\\n+\\n+net = vgg(conv_arch)\\n+```\\n+\\n+接下来，我们将构建一个高度为224、宽度为224的单通道数据示例，以观察每一层的输出形状。\\n+\\n+```{.python .input}\\n+net.initialize()\\n+X = np.random.uniform(size=(1, 1, 224, 224))\\n+for blk in net:\\n+    X = blk(X)\\n+    print(blk.name, \\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+X = torch.randn(size=(1, 1, 224, 224))\\n+for blk in net:\\n+    X = blk(X)\\n+    print(blk.__class__.__name__,\\'output shape:\\\\t\\',X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.random.uniform((1, 224, 224, 1))\\n+for blk in net.layers:\\n+    X = blk(X)\\n+    print(blk.__class__.__name__,\\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+如您所见，我们将每个挡路的高度和宽度减半，最终达到高度和宽度7，然后展平表示以供网络的完全连接部分处理。\\n+\\n+## 培训\\n+\\n+由于VGG-11比AlexNet计算量大，所以我们构造了一个信道数较少的网络。这对时尚-MNIST的培训来说绰绰有余。\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+ratio = 4\\n+small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\\n+net = vgg(small_conv_arch)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+ratio = 4\\n+small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\\n+# Recall that this has to be a function that will be passed to\\n+# `d2l.train_ch6()` so that model building/compiling need to be within\\n+# `strategy.scope()` in order to utilize the CPU/GPU devices that we have\\n+net = lambda: vgg(small_conv_arch)\\n+```\\n+\\n+除了使用稍大的学习率外，该模型的训练过程与ALEXNET在:numref:`sec_alexnet`中的过程类似。\\n+\\n+```{.python .input}\\n+#@tab all\\n+lr, num_epochs, batch_size = 0.05, 10, 128\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\\n+d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+## 摘要\\n+\\n+* VGG-11使用可重复使用的卷积块构建网络。根据每个挡路中卷积层数和输出通道的不同，可以定义不同的VGG模型。\\n+* 块的使用使得网络定义的表示非常紧凑。它允许高效地设计复杂网络。\\n+* 在他们的VGG论文中，Simonyan和Ziserman试验了各种架构。特别是，他们发现，几层深而窄的卷积(即$3 \\\\times 3$)比较少层的较宽卷积更有效。\\n+\\n+## 练习\\n+\\n+1. 当打印出图层的尺寸时，我们只看到了8个结果，而不是11个。剩下的3个图层信息到哪里去了？\\n+1. 与AlexNet相比，VGG的计算速度要慢得多，而且需要更多的GPU内存。分析造成这种情况的原因。\\n+1. 尝试将Fashion-MNIST中图像的高度和宽度从224更改为96。这对实验有什么影响？\\n+1. 参考VGG论文:cite:`Simonyan.Zisserman.2014`中的表1构造其他常见型号，如VGG-16或VGG-19。\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/77)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/78)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/277)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-neural-networks/channels_baidu.md b/chapter_convolutional-neural-networks/channels_baidu.md\\nnew file mode 100644\\nindex 00000000..f2cb432c\\n--- /dev/null\\n+++ b/chapter_convolutional-neural-networks/channels_baidu.md\\n@@ -0,0 +1,172 @@\\n+# 多输入多输出通道\\n+:label:`sec_channels`\\n+\\n+虽然我们已经在:numref:`subsec_why-conv-channels`中描述了构成每个图像的多个信道（例如，彩色图像具有标准的RGB信道来指示红、绿和蓝的数量）和多个信道的卷积层，但是到目前为止，我们通过仅使用单个输入和单个输出信道来简化我们的所有数值示例。这使得我们可以将输入、卷积核和输出看作二维张量。\\n+\\n+当我们在混合中添加通道时，我们的输入和隐藏的表示都变成了三维张量。例如，每个RGB输入图像具有$3\\\\times h\\\\times w$的形状。我们将这个尺寸为3的轴称为*通道*维度。在本节中，我们将更深入地研究具有多个输入和多个输出通道的卷积核。\\n+\\n+## 多输入通道\\n+\\n+当输入数据包含多个通道时，需要构造一个与输入数据具有相同数目输入通道的卷积核，以便与输入数据进行互相关。假设输入数据的信道数为$c_i$，卷积核的输入信道数也需要为$c_i$。如果我们的卷积核的窗口形状是$k_h\\\\times k_w$，那么当$c_i=1$时，我们可以把卷积核看作$k_h\\\\times k_w$形状的二维张量。\\n+\\n+然而，当$c_i>1$时，我们需要一个包含形状为$k_h\\\\times k_w$的张量的内核，用于*每个*输入通道。将这些$c_i$张量连接在一起可以得到形状为$c_i\\\\times k_h\\\\times k_w$的卷积核。由于输入和卷积核都有$c_i$个通道，我们可以对每个通道的输入二维张量和卷积核的二维张量进行互相关运算，将$c_i$的结果相加（对通道求和）得到二维张量。这是多通道输入和多输入通道卷积核之间二维互相关的结果。\\n+\\n+在:numref:`fig_conv_multi_in`中，我们演示了一个具有两个输入信道的二维互相关的示例。阴影部分是第一个输出元素以及用于输出计算的输入和核张量元素：$(1\\\\times1+2\\\\times2+4\\\\times3+5\\\\times4)+(0\\\\times0+1\\\\times1+3\\\\times2+4\\\\times3)=56$。\\n+\\n+![Cross-correlation computation with 2 input channels.](../img/conv-multi-in.svg)\\n+:label:`fig_conv_multi_in`\\n+\\n+为了确保我们真正理解这里的情况，我们可以自己用多个输入通道实现互相关操作。请注意，我们所做的就是对每个通道执行一个互相关操作，然后将结果相加。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import np, npx\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+```\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+def corr2d_multi_in(X, K):\\n+    # First, iterate through the 0th dimension (channel dimension) of `X` and\\n+    # `K`. Then, add them together\\n+    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+def corr2d_multi_in(X, K):\\n+    # First, iterate through the 0th dimension (channel dimension) of `X` and\\n+    # `K`. Then, add them together\\n+    return tf.reduce_sum([d2l.corr2d(x, k) for x, k in zip(X, K)], axis=0)\\n+```\\n+\\n+我们可以构造与:numref:`fig_conv_multi_in`中的值相对应的输入张量`X`和核张量`K`，以验证互相关运算的输出。\\n+\\n+```{.python .input}\\n+#@tab all\\n+X = d2l.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\\n+               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\\n+K = d2l.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\\n+\\n+corr2d_multi_in(X, K)\\n+```\\n+\\n+## 多输出通道\\n+\\n+不管输入通道的数量如何，到目前为止，我们总是以一个输出通道结束。然而，正如我们在:numref:`subsec_why-conv-channels`中所讨论的，在每一层都有多个信道是至关重要的。在最流行的神经网络架构中，当我们在神经网络中往上走的时候，我们实际上会增加信道的维数，通常是通过下采样来交换空间分辨率以获得更大的信道深度*。直观地说，您可以将每个频道看作是对一些不同功能集的响应。现实比对这种直觉的最天真的解释要复杂一些，因为表象不是独立学习的，而是为了共同使用而优化的。因此，可能不是单通道学习边缘检测器，而是通道空间中的某个方向对应于检测边缘。\\n+\\n+用$c_i$和$c_o$分别表示输入和输出通道的数目，并让$k_h$和$k_w$为内核的高度和宽度。为了获得多个通道的输出，我们可以为*每个*输出通道创建一个形状为$c_i\\\\times k_h\\\\times k_w$的内核张量。我们将它们连接到输出通道维上，这样卷积核的形状是$c_o\\\\times c_i\\\\times k_h\\\\times k_w$。在互相关运算中，每个输出通道上的结果都是从对应于该输出通道的卷积核计算出来的，并从输入张量中的所有通道中获取输入。\\n+\\n+我们实现了一个互相关函数来计算多个信道的输出，如下所示。\\n+\\n+```{.python .input}\\n+#@tab all\\n+def corr2d_multi_in_out(X, K):\\n+    # Iterate through the 0th dimension of `K`, and each time, perform\\n+    # cross-correlation operations with input `X`. All of the results are\\n+    # stacked together\\n+    return d2l.stack([corr2d_multi_in(X, k) for k in K], 0)\\n+```\\n+\\n+通过将核张量`K`与`K+1`（`K`中每个元素加一个）和`K+2`连接起来，构造了一个具有3个输出通道的卷积核。\\n+\\n+```{.python .input}\\n+#@tab all\\n+K = d2l.stack((K, K + 1, K + 2), 0)\\n+K.shape\\n+```\\n+\\n+下面，我们对输入张量`X`与内核张量`K`执行互相关操作。现在输出包含3个通道。第一通道的结果与先前输入张量`X`和多输入单输出通道核的结果一致。\\n+\\n+```{.python .input}\\n+#@tab all\\n+corr2d_multi_in_out(X, K)\\n+```\\n+\\n+## $1\\\\times 1$卷积层\\n+\\n+一开始，$1 \\\\times 1$卷积，即$k_h = k_w = 1$，似乎没有多大意义。毕竟，卷积与相邻像素相关。$1 \\\\times 1$卷积显然不是。尽管如此，它们仍然是流行的操作，有时也包含在复杂的深层网络的设计中。让我们更详细地了解一下它的实际作用。\\n+\\n+因为使用了最小窗口，$1\\\\times 1$卷积失去了更大的卷积层识别由高度和宽度维度上相邻元素之间相互作用组成的模式的能力。$1\\\\times 1$卷积的唯一计算发生在信道尺寸上。\\n+\\n+:numref:`fig_conv_1x1`显示了使用$1\\\\times 1$卷积核与3个输入通道和2个输出通道的互相关计算。请注意，输入和输出具有相同的高度和宽度。输出中的每个元素都是从输入图像中同一位置*的元素*的线性组合派生的。可以将$1\\\\times 1$卷积层看作是在每个像素位置应用的全连接层，用于将$c_i$对应的输入值转换为$c_o$的输出值。因为这仍然是一个卷积层，所以权重是跨像素位置绑定的。因此，$1\\\\times 1$卷积层需要$c_o\\\\times c_i$权重（加上偏差）。\\n+\\n+![The cross-correlation computation uses the $1\\\\times 1$ convolution kernel with 3 input channels and 2 output channels. The input and output have the same height and width.](../img/conv-1x1.svg)\\n+:label:`fig_conv_1x1`\\n+\\n+让我们检查一下这在实践中是否有效：我们使用完全连接层实现$1 \\\\times 1$卷积。唯一的问题是我们需要在矩阵乘法之前和之后对数据形状进行一些调整。\\n+\\n+```{.python .input}\\n+#@tab all\\n+def corr2d_multi_in_out_1x1(X, K):\\n+    c_i, h, w = X.shape\\n+    c_o = K.shape[0]\\n+    X = d2l.reshape(X, (c_i, h * w))\\n+    K = d2l.reshape(K, (c_o, c_i))\\n+    Y = d2l.matmul(K, X)  # Matrix multiplication in the fully-connected layer\\n+    return d2l.reshape(Y, (c_o, h, w))\\n+```\\n+\\n+当执行$1\\\\times 1$卷积时，上述函数相当于先前实现的互相关函数`corr2d_multi_in_out`。让我们用一些样本数据来验证这一点。\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+X = d2l.normal(0, 1, (3, 3, 3))\\n+K = d2l.normal(0, 1, (2, 3, 1, 1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = d2l.normal((3, 3, 3), 0, 1)\\n+K = d2l.normal((2, 3, 1, 1), 0, 1)\\n+```\\n+\\n+```{.python .input}\\n+#@tab all\\n+Y1 = corr2d_multi_in_out_1x1(X, K)\\n+Y2 = corr2d_multi_in_out(X, K)\\n+assert float(d2l.reduce_sum(d2l.abs(Y1 - Y2))) < 1e-6\\n+```\\n+\\n+## 摘要\\n+\\n+* 多通道可以用来扩展卷积层的模型参数。\\n+* 当以每像素为基础应用时，$1\\\\times 1$卷积层相当于全连接层。\\n+* $1\\\\times 1$卷积层通常用于调整网络层之间的信道数量和控制模型复杂性。\\n+\\n+## 练习\\n+\\n+1. 假设我们有两个卷积核，大小分别为$k_1$和$k_2$（中间没有非线性）。\\n+    1. 证明了运算结果可以用一个卷积表示。\\n+    1. 等效单卷积的维数是多少？\\n+    1. 反之亦然吗？\\n+1. 假设输入为$c_i\\\\times h\\\\times w$，卷积核为$c_o\\\\times c_i\\\\times k_h\\\\times k_w$，填充为$(p_h, p_w)$，步长为$(s_h, s_w)$。\\n+    1. 前向传播的计算成本（乘法和加法）是多少？\\n+    1. 内存占用是多少？\\n+    1. 向后计算的内存占用是多少？\\n+    1. 反向传播的计算成本是多少？\\n+1. 如果我们将输入通道$c_i$和输出通道$c_o$的数量加倍，计算数量会增加多少？如果我们把填充物翻一番会怎么样？\\n+1. 如果卷积核的高度和宽度是$k_h=k_w=1$，前向传播的计算复杂度是多少？\\n+1. 本节最后一个示例中的变量`Y1`和`Y2`是否完全相同？为什么？\\n+1. 当卷积窗口不是$1\\\\times 1$时，如何使用矩阵乘法实现卷积？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/69)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/70)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/273)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-neural-networks/channels_tencent.md b/chapter_convolutional-neural-networks/channels_tencent.md\\nnew file mode 100644\\nindex 00000000..70732e56\\n--- /dev/null\\n+++ b/chapter_convolutional-neural-networks/channels_tencent.md\\n@@ -0,0 +1,172 @@\\n+# 多输入多输出通道\\n+:label:`sec_channels`\\n+\\n+虽然我们已经在:numref:`subsec_why-conv-channels`中描述了组成每个图像的多个通道(例如，彩色图像具有表示红、绿和蓝的量的标准rgb通道)和多个通道的卷积层，但到目前为止，我们通过仅使用单个输入和单个输出通道简化了所有数值示例。这使得我们可以把我们的输入、卷积核和输出都看作是二维张量。\\n+\\n+当我们将通道添加到混合中时，我们的输入和隐藏表示都变成了三维张量。例如，每个rgb输入图像具有形状$3\\\\times h\\\\times w$。我们将这个大小为3的轴称为*channel*维。在本节中，我们将更深入地研究具有多个输入和多个输出通道的卷积内核。\\n+\\n+## 多个输入通道\\n+\\n+当输入数据包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相关。假设输入数据的通道数为$c_i$，则卷积核的输入通道数也需要为$c_i$。如果我们的卷积核的窗口形状是$k_h\\\\times k_w$，那么当为$c_i=1$时，我们可以认为我们的卷积核只是形状$k_h\\\\times k_w$的二维张量。\\n+\\n+但是，当值为$c_i>1$时，我们需要一个核，该核包含*每个*输入通道的形状为$k_h\\\\times k_w$的张量。将这$c_i$个张量连接在一起产生形状为$c_i\\\\times k_h\\\\times k_w$的卷积核。由于输入和卷积核各自具有$c_i$个通道，因此我们可以对每个通道的输入的二维张量和卷积核的二维张量执行互相关操作，将$c_i$个结果相加(对通道求和)以产生二维张量。这是多通道输入和多输入通道卷积核之间二维互相关的结果。\\n+\\n+在:numref:`fig_conv_multi_in`中，我们演示了一个具有两个输入通道的二维互相关的例子。阴影部分是第一个输出元素以及用于输出计算的输入和核张量元素：$(1\\\\times1+2\\\\times2+4\\\\times3+5\\\\times4)+(0\\\\times0+1\\\\times1+3\\\\times2+4\\\\times3)=56$。\\n+\\n+![Cross-correlation computation with 2 input channels.](../img/conv-multi-in.svg)\\n+:label:`fig_conv_multi_in`\\n+\\n+为了确保我们真正理解这里发生的事情，我们可以自己实现具有多个输入通道的互相关操作。请注意，我们所做的全部工作就是对每个通道执行一次互相关操作，然后将结果相加。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import np, npx\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+```\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+def corr2d_multi_in(X, K):\\n+    # First, iterate through the 0th dimension (channel dimension) of `X` and\\n+    # `K`. Then, add them together\\n+    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+def corr2d_multi_in(X, K):\\n+    # First, iterate through the 0th dimension (channel dimension) of `X` and\\n+    # `K`. Then, add them together\\n+    return tf.reduce_sum([d2l.corr2d(x, k) for x, k in zip(X, K)], axis=0)\\n+```\\n+\\n+我们可以构造输入张量`X`和核张量`K`，它们对应于:numref:`fig_conv_multi_in`中的值，以验证互相关运算的输出。\\n+\\n+```{.python .input}\\n+#@tab all\\n+X = d2l.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\\n+               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\\n+K = d2l.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\\n+\\n+corr2d_multi_in(X, K)\\n+```\\n+\\n+## 多个输出通道\\n+\\n+不管有多少个输入通道，到目前为止，我们总是得到一个输出通道。但是，正如我们在:numref:`subsec_why-conv-channels`中讨论的那样，每层都必须有多个通道。在最流行的神经网络体系结构中，我们实际上会随着神经网络中更高的位置而增加通道维度，通常是向下采样以牺牲空间分辨率来换取更大的*通道深度*。直观地说，您可以将每个通道视为对某些不同功能集的响应。现实比对这种直觉的最天真的解释要复杂一些，因为表征不是独立学习的，而是经过优化才能共同发挥作用的。因此，可能不是单个通道学习边缘检测器，而是通道空间中的某个方向对应于检测边缘。\\n+\\n+分别用$c_i$和$c_o$表示输入和输出通道的数量，并让$k_h$和$k_w$表示内核的高度和宽度。要获得具有多个通道的输出，我们可以为*每个*输出通道创建一个形状为$c_i\\\\times k_h\\\\times k_w$的核张量。我们将它们串联在输出通道维度上，因此卷积核的形状为$c_o\\\\times c_i\\\\times k_h\\\\times k_w$。在互相关运算中，每个输出通道的结果从对应于该输出通道的卷积核中计算，并从输入张量中的所有通道获取输入。\\n+\\n+我们实现了一个互相关函数来计算多个通道的输出，如下所示。\\n+\\n+```{.python .input}\\n+#@tab all\\n+def corr2d_multi_in_out(X, K):\\n+    # Iterate through the 0th dimension of `K`, and each time, perform\\n+    # cross-correlation operations with input `X`. All of the results are\\n+    # stacked together\\n+    return d2l.stack([corr2d_multi_in(X, k) for k in K], 0)\\n+```\\n+\\n+我们通过将核张量`K`与`K+1`(对于`K`中的每个元素加上一个)和`K+2`级联来构造具有3个输出通道的卷积核。\\n+\\n+```{.python .input}\\n+#@tab all\\n+K = d2l.stack((K, K + 1, K + 2), 0)\\n+K.shape\\n+```\\n+\\n+下面，我们对输入张量`X`和核张量`K`执行互相关操作。现在输出包含3个通道。第一通道的结果与先前输入张量`X`和多输入通道、单输出通道核的结果一致。\\n+\\n+```{.python .input}\\n+#@tab all\\n+corr2d_multi_in_out(X, K)\\n+```\\n+\\n+## $1\\\\times 1$卷积层\\n+\\n+起初，$1 \\\\times 1$的卷积，即$k_h = k_w = 1$，似乎没有多大意义。毕竟，卷积使相邻像素相互关联。$1 \\\\times 1$的卷积显然不会。尽管如此，它们仍然是常见的操作，有时会包含在复杂深层网络的设计中。让我们更详细地看看它的实际作用。\\n+\\n+因为使用了最小窗口，所以$1\\\\times 1$卷积失去了较大卷积层识别由高度和宽度维度中相邻元素之间的相互作用组成的图案的能力。仅在通道维度上进行$1\\\\times 1$卷积的计算。\\n+\\n+:numref:`fig_conv_1x1`示出了使用具有3个输入通道和2个输出通道的$1\\\\times 1$卷积核进行的互相关计算。请注意，输入和输出具有相同的高度和宽度。输出中的每个元素都是从输入图像中*相同位置*的元素的线性组合中导出的。您可以将$1\\\\times 1$卷积层视为构成一个完全连接的层，该层应用于每个单个像素位置，以将$c_i$个相应的输入值转换为$c_o$个输出值。因为这仍然是一个卷积层，所以权重是跨像素位置绑定的。因此，$1\\\\times 1$卷积层需要$c_o\\\\times c_i$个权重(加上偏差)。\\n+\\n+![The cross-correlation computation uses the $1\\\\times 1$ convolution kernel with 3 input channels and 2 output channels. The input and output have the same height and width.](../img/conv-1x1.svg)\\n+:label:`fig_conv_1x1`\\n+\\n+让我们检查一下这在实践中是否有效：我们使用完全连接层实现$1 \\\\times 1$的卷积。唯一的问题是，我们需要在矩阵乘法前后对数据形状进行一些调整。\\n+\\n+```{.python .input}\\n+#@tab all\\n+def corr2d_multi_in_out_1x1(X, K):\\n+    c_i, h, w = X.shape\\n+    c_o = K.shape[0]\\n+    X = d2l.reshape(X, (c_i, h * w))\\n+    K = d2l.reshape(K, (c_o, c_i))\\n+    Y = d2l.matmul(K, X)  # Matrix multiplication in the fully-connected layer\\n+    return d2l.reshape(Y, (c_o, h, w))\\n+```\\n+\\n+当执行$1\\\\times 1$卷积时，上述函数等同于先前实现的互相关函数`corr2d_multi_in_out`。让我们用一些样本数据来验证这一点。\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+X = d2l.normal(0, 1, (3, 3, 3))\\n+K = d2l.normal(0, 1, (2, 3, 1, 1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = d2l.normal((3, 3, 3), 0, 1)\\n+K = d2l.normal((2, 3, 1, 1), 0, 1)\\n+```\\n+\\n+```{.python .input}\\n+#@tab all\\n+Y1 = corr2d_multi_in_out_1x1(X, K)\\n+Y2 = corr2d_multi_in_out(X, K)\\n+assert float(d2l.reduce_sum(d2l.abs(Y1 - Y2))) < 1e-6\\n+```\\n+\\n+## 摘要\\n+\\n+* 可以使用多个通道来扩展卷积层的模型参数。\\n+* 当以每像素为基础应用时，$1\\\\times 1$卷积层等同于完全连接层。\\n+* $1\\\\times 1$卷积层通常用于调整网络层之间的信道数量并控制模型复杂性。\\n+\\n+## 练习\\n+\\n+1. 假设我们有两个大小分别为$k_1$和$k_2$的卷积核(其间没有非线性)。\\n+    1. 证明了运算结果可以用一次卷积表示。\\n+    1. 等效单卷积的维数是多少？\\n+    1. 反之亦然吗？\\n+1. 假设形状$c_i\\\\times h\\\\times w$的输入和形状$c_o\\\\times c_i\\\\times k_h\\\\times k_w$的卷积核、填充$(p_h, p_w)$和跨度$(s_h, s_w)$。\\n+    1. 前向传播的计算开销(乘法和加法)是多少？\\n+    1. 内存占用量是多少？\\n+    1. 向后计算的内存占用有多大？\\n+    1. 反向传播的计算成本是多少？\\n+1. 如果我们将输入通道$c_i$的数量和输出通道$c_o$的数量增加一倍，计算的数量会增加多少倍？如果我们把填充物翻一番会怎么样？\\n+1. 如果卷积核的高度和宽度为$k_h=k_w=1$，则前向传播的计算复杂度是多少？\\n+1. 本节最后一个示例中的变量`Y1`和`Y2`是否完全相同？为什么？\\n+1. 当卷积窗口不是$1\\\\times 1$时，如何使用矩阵乘法实现卷积？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/69)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/70)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/273)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-neural-networks/conv-layer_baidu.md b/chapter_convolutional-neural-networks/conv-layer_baidu.md\\nnew file mode 100644\\nindex 00000000..0b93bafc\\n--- /dev/null\\n+++ b/chapter_convolutional-neural-networks/conv-layer_baidu.md\\n@@ -0,0 +1,313 @@\\n+# 图像卷积\\n+:label:`sec_conv_layer`\\n+\\n+既然我们了解了卷积层在理论上是如何工作的，那么我们就可以看看它们在实际中是如何工作的。基于卷积神经网络作为探索图像数据结构的有效架构的动机，我们坚持以图像为例。\\n+\\n+## 互相关运算\\n+\\n+回想一下，严格地说，卷积层是用词不当的，因为它们所表达的运算被更准确地描述为互相关。基于我们对:numref:`sec_why-conv`中卷积层的描述，在这样一个层中，输入张量和核张量通过互相关运算产生输出张量。\\n+\\n+让我们暂时忽略通道，看看它是如何处理二维数据和隐藏表示的。在:numref:`fig_correlation`中，输入是高度为3、宽度为3的二维张量。我们将张量的形状标记为$3 \\\\times 3$或（$3$，$3$）。内核的高度和宽度都是2。内核窗口（或卷积窗口）的形状由内核的高度和宽度决定（这里是$2 \\\\times 2$）。\\n+\\n+![Two-dimensional cross-correlation operation. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\\\times0+1\\\\times1+3\\\\times2+4\\\\times3=19$.](../img/correlation.svg)\\n+:label:`fig_correlation`\\n+\\n+在二维互相关运算中，我们从位于输入张量左上角的卷积窗口开始，并将其从左到右和从上到下滑动到输入张量。当卷积窗口滑动到某个位置时，包含在该窗口中的输入子传感器与核张量进行元素相乘，得到的张量相加得到一个单一的标量值。这个结果给出了相应位置的输出张量值。这里，输出张量的高度为2，宽度为2，四个元素由二维互相关运算导出：\\n+\\n+$$\\n+0\\\\times0+1\\\\times1+3\\\\times2+4\\\\times3=19,\\\\\\\\\\n+1\\\\times0+2\\\\times1+4\\\\times2+5\\\\times3=25,\\\\\\\\\\n+3\\\\times0+4\\\\times1+6\\\\times2+7\\\\times3=37,\\\\\\\\\\n+4\\\\times0+5\\\\times1+7\\\\times2+8\\\\times3=43.\\n+$$\\n+\\n+请注意，沿每个轴，输出大小略小于输入大小。由于核的宽度和高度大于1，我们只能正确地计算核在图像中完全适合的位置的互相关，输出大小由输入大小$n_h \\\\times n_w$减去卷积核$k_h \\\\times k_w$的大小通过\\n+\\n+$$(n_h-k_h+1) \\\\times (n_w-k_w+1).$$\\n+\\n+这是因为我们需要足够的空间在图像上“移动”卷积核。稍后，我们将看到如何通过在图像边界周围填充零来保持大小不变，以便有足够的空间来移动内核。接下来，我们在`corr2d`函数中实现这个过程，该函数接受输入张量`X`和内核张量`K`，并返回输出张量`Y`。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import autograd, np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+```\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+def corr2d(X, K):  #@save\\n+    \"\"\"Compute 2D cross-correlation.\"\"\"\\n+    h, w = K.shape\\n+    Y = d2l.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\\n+    for i in range(Y.shape[0]):\\n+        for j in range(Y.shape[1]):\\n+            Y[i, j] = d2l.reduce_sum((X[i: i + h, j: j + w] * K))\\n+    return Y\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+def corr2d(X, K):  #@save\\n+    \"\"\"Compute 2D cross-correlation.\"\"\"\\n+    h, w = K.shape\\n+    Y = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)))\\n+    for i in range(Y.shape[0]):\\n+        for j in range(Y.shape[1]):\\n+            Y[i, j].assign(tf.reduce_sum(\\n+                X[i: i + h, j: j + w] * K))\\n+    return Y\\n+```\\n+\\n+我们可以从:numref:`fig_correlation`构造输入张量`X`和核张量`K`来验证上述二维互相关运算实现的输出。\\n+\\n+```{.python .input}\\n+#@tab all\\n+X = d2l.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\\n+K = d2l.tensor([[0.0, 1.0], [2.0, 3.0]])\\n+corr2d(X, K)\\n+```\\n+\\n+## 卷积层\\n+\\n+卷积层将输入和内核交叉相关，并添加标量偏差以产生输出。卷积层的两个参数是核和标量偏差。当训练基于卷积层的模型时，我们通常会随机初始化内核，就像我们使用完全连接的层一样。\\n+\\n+我们现在准备实现一个基于上面定义的`corr2d`函数的二维卷积层。在`__init__`构造函数中，我们声明`weight`和`bias`作为两个模型参数。前向传播函数调用`corr2d`函数并添加偏差。\\n+\\n+```{.python .input}\\n+class Conv2D(nn.Block):\\n+    def __init__(self, kernel_size, **kwargs):\\n+        super().__init__(**kwargs)\\n+        self.weight = self.params.get(\\'weight\\', shape=kernel_size)\\n+        self.bias = self.params.get(\\'bias\\', shape=(1,))\\n+\\n+    def forward(self, x):\\n+        return corr2d(x, self.weight.data()) + self.bias.data()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class Conv2D(nn.Module):\\n+    def __init__(self, kernel_size):\\n+        super().__init__()\\n+        self.weight = nn.Parameter(torch.rand(kernel_size))\\n+        self.bias = nn.Parameter(torch.zeros(1))\\n+\\n+    def forward(self, x):\\n+        return corr2d(x, self.weight) + self.bias\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class Conv2D(tf.keras.layers.Layer):\\n+    def __init__(self):\\n+        super().__init__()\\n+\\n+    def build(self, kernel_size):\\n+        initializer = tf.random_normal_initializer()\\n+        self.weight = self.add_weight(name=\\'w\\', shape=kernel_size,\\n+                                      initializer=initializer)\\n+        self.bias = self.add_weight(name=\\'b\\', shape=(1, ),\\n+                                    initializer=initializer)\\n+\\n+    def call(self, inputs):\\n+        return corr2d(inputs, self.weight) + self.bias\\n+```\\n+\\n+在$h \\\\times w$卷积或$h \\\\times w$卷积核中，卷积核的高度和宽度分别为$h$和$w$。我们还将具有$h \\\\times w$卷积核的卷积层称为$h \\\\times w$卷积层。\\n+\\n+## 图像中目标的边缘检测\\n+\\n+让我们花点时间来解析卷积层的一个简单应用：通过找到像素变化的位置来检测图像中对象的边缘。首先，我们构造一个$6\\\\times 8$像素的“图像”。中间四列为黑色（0），其余为白色（1）。\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+X = d2l.ones((6, 8))\\n+X[:, 2:6] = 0\\n+X\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.Variable(tf.ones((6, 8)))\\n+X[:, 2:6].assign(tf.zeros(X[:, 2:6].shape))\\n+X\\n+```\\n+\\n+接下来，我们构造一个高度为1、宽度为2的内核`K`。当我们对输入进行互相关运算时，如果水平相邻元素相同，则输出为0。否则，输出为非零。\\n+\\n+```{.python .input}\\n+#@tab all\\n+K = d2l.tensor([[1.0, -1.0]])\\n+```\\n+\\n+我们已经准备好使用参数`X`（我们的输入）和`K`（我们的内核）执行互相关操作。如您所见，我们检测1代表从白色到黑色的边缘，以及-1代表从黑色到白色的边缘。所有其他输出值为0。\\n+\\n+```{.python .input}\\n+#@tab all\\n+Y = corr2d(X, K)\\n+Y\\n+```\\n+\\n+我们现在可以将内核应用于转置图像。正如所料，它消失了。内核`K`只检测垂直边缘。\\n+\\n+```{.python .input}\\n+#@tab all\\n+corr2d(d2l.transpose(X), K)\\n+```\\n+\\n+## 学习内核\\n+\\n+如果我们知道这正是我们正在寻找的，那么用有限差分`[1, -1]`设计边缘检测器是一件很好的事情。然而，当我们观察更大的内核，并考虑连续的卷积层时，可能不可能精确地指定每个过滤器应该手动执行的操作。\\n+\\n+现在让我们看看是否可以通过只查看输入-输出对来学习从`X`生成`Y`的内核。我们首先构造一个卷积层，并将其核初始化为随机张量。接下来，在每次迭代中，我们将使用平方误差来比较`Y`与卷积层的输出。然后我们可以计算梯度来更新内核。为了简单起见，下面我们对二维卷积层使用内置类，并忽略偏差。\\n+\\n+```{.python .input}\\n+# Construct a two-dimensional convolutional layer with 1 output channel and a\\n+# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\\n+conv2d = nn.Conv2D(1, kernel_size=(1, 2), use_bias=False)\\n+conv2d.initialize()\\n+\\n+# The two-dimensional convolutional layer uses four-dimensional input and\\n+# output in the format of (example, channel, height, width), where the batch\\n+# size (number of examples in the batch) and the number of channels are both 1\\n+X = X.reshape(1, 1, 6, 8)\\n+Y = Y.reshape(1, 1, 6, 7)\\n+\\n+for i in range(10):\\n+    with autograd.record():\\n+        Y_hat = conv2d(X)\\n+        l = (Y_hat - Y) ** 2\\n+    l.backward()\\n+    # Update the kernel\\n+    conv2d.weight.data()[:] -= 3e-2 * conv2d.weight.grad()\\n+    if (i + 1) % 2 == 0:\\n+        print(f\\'batch {i+1}, loss {float(l.sum()):.3f}\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+# Construct a two-dimensional convolutional layer with 1 output channel and a\\n+# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\\n+conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)\\n+\\n+# The two-dimensional convolutional layer uses four-dimensional input and\\n+# output in the format of (example channel, height, width), where the batch\\n+# size (number of examples in the batch) and the number of channels are both 1\\n+X = X.reshape((1, 1, 6, 8))\\n+Y = Y.reshape((1, 1, 6, 7))\\n+\\n+for i in range(10):\\n+    Y_hat = conv2d(X)\\n+    l = (Y_hat - Y) ** 2\\n+    conv2d.zero_grad()\\n+    l.sum().backward()\\n+    # Update the kernel\\n+    conv2d.weight.data[:] -= 3e-2 * conv2d.weight.grad\\n+    if (i + 1) % 2 == 0:\\n+        print(f\\'batch {i+1}, loss {l.sum():.3f}\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+# Construct a two-dimensional convolutional layer with 1 output channel and a\\n+# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\\n+conv2d = tf.keras.layers.Conv2D(1, (1, 2), use_bias=False)\\n+\\n+# The two-dimensional convolutional layer uses four-dimensional input and\\n+# output in the format of (example channel, height, width), where the batch\\n+# size (number of examples in the batch) and the number of channels are both 1\\n+X = tf.reshape(X, (1, 6, 8, 1))\\n+Y = tf.reshape(Y, (1, 6, 7, 1))\\n+\\n+Y_hat = conv2d(X)\\n+for i in range(10):\\n+    with tf.GradientTape(watch_accessed_variables=False) as g:\\n+        g.watch(conv2d.weights[0])\\n+        Y_hat = conv2d(X)\\n+        l = (abs(Y_hat - Y)) ** 2\\n+        # Update the kernel\\n+        update = tf.multiply(3e-2, g.gradient(l, conv2d.weights[0]))\\n+        weights = conv2d.get_weights()\\n+        weights[0] = conv2d.weights[0] - update\\n+        conv2d.set_weights(weights)\\n+        if (i + 1) % 2 == 0:\\n+            print(f\\'batch {i+1}, loss {tf.reduce_sum(l):.3f}\\')\\n+```\\n+\\n+请注意，在10次迭代之后，错误已经降到一个小值。现在我们来看看我们所学的核张量。\\n+\\n+```{.python .input}\\n+d2l.reshape(conv2d.weight.data(), (1, 2))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+d2l.reshape(conv2d.weight.data, (1, 2))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+d2l.reshape(conv2d.get_weights()[0], (1, 2))\\n+```\\n+\\n+实际上，学习到的核张量非常接近我们之前定义的核张量`K`。\\n+\\n+## 互相关和卷积\\n+\\n+回想一下我们在:numref:`sec_why-conv`中观察到的互相关和卷积运算之间的对应关系。这里让我们继续考虑二维卷积层。如果这样的层执行:eqref:`eq_2d-conv-discrete`中定义的严格卷积运算，而不是互相关怎么办？为了得到严格*卷积*运算的输出，我们只需要水平和垂直翻转二维核张量，然后用输入张量执行*互相关*运算。\\n+\\n+值得注意的是，由于核是在深度学习中从数据中学习的，因此无论这些层执行严格的卷积运算还是互相关运算，卷积层的输出都不会受到影响。\\n+\\n+为了说明这一点，假设卷积层执行*互相关*并学习:numref:`fig_correlation`中的内核，该内核在这里表示为矩阵$\\\\mathbf{K}$。假设其他条件不变，当这个层执行严格的*卷积*时，学习的内核$\\\\mathbf{K}\\'$在水平和垂直翻转$\\\\mathbf{K}\\'$之后将与$\\\\mathbf{K}$相同。也就是说，当卷积层对:numref:`fig_correlation`和$\\\\mathbf{K}\\'$中的输入执行严格*卷积*时，将获得:numref:`fig_correlation`中相同的输出（输入与$\\\\mathbf{K}$的互相关）。\\n+\\n+为了与深度学习文献中的标准术语保持一致，我们将继续将互相关操作称为卷积，尽管严格地说，它略有不同。此外，我们使用术语*元素*来表示表示层表示或卷积核的任何张量的入口（或组件）。\\n+\\n+## 特征图和感受野\\n+\\n+如在:numref:`subsec_why-conv-channels`中所述，:numref:`fig_correlation`中输出的卷积层有时被称为*特征映射*，因为它可以被视为到下一层的空间维度（例如，宽度和高度）中的学习表示（特征）。在CNNs中，对于某一层的任何元素$x$，其*接收场*是指在前向传播期间可能影响$x$计算的所有元素（来自所有先前层）。注意，接收野可能大于输入的实际大小。\\n+\\n+让我们继续用:numref:`fig_correlation`来解释感受野。给定$2 \\\\times 2$卷积核，阴影输出元素（值$19$）的接收域是输入阴影部分的四个元素。现在让我们将$2 \\\\times 2$的输出表示为$\\\\mathbf{Y}$，并考虑一个更深的CNN，该CNN具有附加的$\\\\mathbf{Y}$卷积层，该卷积层以$\\\\mathbf{Y}$为输入，输出单个元素$z$。在这种情况下，$\\\\mathbf{Y}$上的$z$的接收字段包括$\\\\mathbf{Y}$的所有四个元素，而输入上的接收字段包括所有九个输入元素。因此，当一个特征图中的任何元素需要一个更大的接受野来检测更广区域的输入特征时，我们可以构建一个更深的网络。\\n+\\n+## 摘要\\n+\\n+* 二维卷积层的核心计算是二维互相关运算。最简单的形式是，对二维输入数据和内核执行互相关操作，然后添加一个偏差。\\n+* 我们可以设计一个核来检测图像的边缘。\\n+* 我们可以从数据中学习内核的参数。\\n+* 通过从数据中学习核，卷积层的输出不受影响，而不管这些层执行了什么操作（严格卷积或互相关）。\\n+* 当特征图中的任何元素需要一个更大的接受域来检测输入的更广泛的特征时，可以考虑更深的网络。\\n+\\n+## 练习\\n+\\n+1. 构建一个具有对角线边缘的图像`X`。\\n+    1. 如果将本节中的内核`K`应用于它，会发生什么情况？\\n+    1. 如果你转置`X`怎么办？\\n+    1. 如果你转置`K`怎么办？\\n+1. 当您尝试自动找到我们创建的`Conv2D`类的渐变时，您看到了什么样的错误消息？\\n+1. 如何通过改变输入张量和核张量来将互相关运算表示为矩阵乘法？\\n+1. 手工设计一些内核。\\n+    1. 二阶导数的核形式是什么？\\n+    1. 积分的核心是什么？\\n+    1. 得到$d$次导数的最小核大小是多少？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/65)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/66)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/271)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-neural-networks/conv-layer_tencent.md b/chapter_convolutional-neural-networks/conv-layer_tencent.md\\nnew file mode 100644\\nindex 00000000..54e0223b\\n--- /dev/null\\n+++ b/chapter_convolutional-neural-networks/conv-layer_tencent.md\\n@@ -0,0 +1,313 @@\\n+# 图像的卷积\\n+:label:`sec_conv_layer`\\n+\\n+现在我们了解了卷积分层在理论上是如何工作的，现在我们准备看看它们在实践中是如何工作的。基于我们将卷积神经网络作为探索图像数据结构的有效架构的动机，我们坚持以图像为运行示例。\\n+\\n+## 互相关运算\\n+\\n+回想一下，严格地说，卷积层是一个用词不当的词，因为它们所表达的运算更准确地被描述为互相关。基于我们在:numref:`sec_why-conv`中对卷积层的描述，在这样的层中，输入张量和核张量通过互相关运算来组合以产生输出张量。\\n+\\n+让我们暂时忽略通道，看看这是如何处理二维数据和隐藏表示的。在:numref:`fig_correlation`中，输入是高度为3、宽度为3的二维张量。我们将张量的形状标记为$3 \\\\times 3$或($3$,$3$)。内核的高度和宽度都是2。*内核窗口*(或*卷积窗口*)的形状由内核的高度和宽度(这里是$2 \\\\times 2$)决定。\\n+\\n+![Two-dimensional cross-correlation operation. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\\\times0+1\\\\times1+3\\\\times2+4\\\\times3=19$.](../img/correlation.svg)\\n+:label:`fig_correlation`\\n+\\n+在二维互相关运算中，我们从位于输入张量左上角的卷积窗口开始，然后从左到右和从上到下跨输入张量滑动。当卷积窗口滑动到某个位置时，该窗口中包含的输入子张量与核张量按元素相乘，并将得到的张量相加得到单个标量值。该结果给出了相应位置的输出张量值。这里，输出张量的高度为2，宽度为2，四个元素从二维互相关运算中导出：\\n+\\n+$$\\n+0\\\\times0+1\\\\times1+3\\\\times2+4\\\\times3=19,\\\\\\\\\\n+1\\\\times0+2\\\\times1+4\\\\times2+5\\\\times3=25,\\\\\\\\\\n+3\\\\times0+4\\\\times1+6\\\\times2+7\\\\times3=37,\\\\\\\\\\n+4\\\\times0+5\\\\times1+7\\\\times2+8\\\\times3=43.\\n+$$\\n+\\n+请注意，沿每个轴，输出大小略小于输入大小。因为核的宽度和高度大于1，所以我们只能正确地计算核完全适合于图像内的位置的互相关，输出大小由输入大小$n_h \\\\times n_w$减去卷积核$k_h \\\\times k_w$的大小通过\\n+\\n+$$(n_h-k_h+1) \\\\times (n_w-k_w+1).$$\\n+\\n+情况就是这样，因为我们需要足够的空间来在图像中“移动”卷积内核。稍后，我们将了解如何通过在图像边界周围填充零来保持大小不变，以便有足够的空间来移动内核。接下来，我们在`corr2d`函数中实现该过程，该函数接受输入张量`X`和核张量`K`，并返回输出张量`Y`。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import autograd, np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+```\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+def corr2d(X, K):  #@save\\n+    \"\"\"Compute 2D cross-correlation.\"\"\"\\n+    h, w = K.shape\\n+    Y = d2l.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\\n+    for i in range(Y.shape[0]):\\n+        for j in range(Y.shape[1]):\\n+            Y[i, j] = d2l.reduce_sum((X[i: i + h, j: j + w] * K))\\n+    return Y\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+def corr2d(X, K):  #@save\\n+    \"\"\"Compute 2D cross-correlation.\"\"\"\\n+    h, w = K.shape\\n+    Y = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)))\\n+    for i in range(Y.shape[0]):\\n+        for j in range(Y.shape[1]):\\n+            Y[i, j].assign(tf.reduce_sum(\\n+                X[i: i + h, j: j + w] * K))\\n+    return Y\\n+```\\n+\\n+我们可以构造输入张量`X`和核张量`K`从:numref:`fig_correlation`来验证二维互相关运算的上述实现的输出。\\n+\\n+```{.python .input}\\n+#@tab all\\n+X = d2l.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\\n+K = d2l.tensor([[0.0, 1.0], [2.0, 3.0]])\\n+corr2d(X, K)\\n+```\\n+\\n+## 卷积层\\n+\\n+卷积层使输入和核互相关，并添加标量偏置以产生输出。卷积层的两个参数是核和标量偏置。当训练基于卷积层的模型时，我们通常会随机初始化内核，就像我们对完全连接层所做的那样。\\n+\\n+现在我们准备基于上面定义的`corr2d`函数实现二维卷积层。在`__init__`构造函数中，我们将`weight`和`bias`声明为两个模型参数。正向传播函数调用`corr2d`函数并添加偏置。\\n+\\n+```{.python .input}\\n+class Conv2D(nn.Block):\\n+    def __init__(self, kernel_size, **kwargs):\\n+        super().__init__(**kwargs)\\n+        self.weight = self.params.get(\\'weight\\', shape=kernel_size)\\n+        self.bias = self.params.get(\\'bias\\', shape=(1,))\\n+\\n+    def forward(self, x):\\n+        return corr2d(x, self.weight.data()) + self.bias.data()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class Conv2D(nn.Module):\\n+    def __init__(self, kernel_size):\\n+        super().__init__()\\n+        self.weight = nn.Parameter(torch.rand(kernel_size))\\n+        self.bias = nn.Parameter(torch.zeros(1))\\n+\\n+    def forward(self, x):\\n+        return corr2d(x, self.weight) + self.bias\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class Conv2D(tf.keras.layers.Layer):\\n+    def __init__(self):\\n+        super().__init__()\\n+\\n+    def build(self, kernel_size):\\n+        initializer = tf.random_normal_initializer()\\n+        self.weight = self.add_weight(name=\\'w\\', shape=kernel_size,\\n+                                      initializer=initializer)\\n+        self.bias = self.add_weight(name=\\'b\\', shape=(1, ),\\n+                                    initializer=initializer)\\n+\\n+    def call(self, inputs):\\n+        return corr2d(inputs, self.weight) + self.bias\\n+```\\n+\\n+在$h \\\\times w$卷积核或$h \\\\times w$卷积核中，卷积核的高度和宽度分别为$h$和$w$。我们也将具有$h \\\\times w$卷积核的卷积层简称为$h \\\\times w$卷积层。\\n+\\n+## 图像中的目标边缘检测\\n+\\n+让我们花一点时间来分析卷积层的一个简单应用：通过找到像素变化的位置来检测图像中对象的边缘。首先，我们构建一个$6\\\\times 8$像素的“图像”。中间四列为黑色(0)，睡觉为白色(1)。\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+X = d2l.ones((6, 8))\\n+X[:, 2:6] = 0\\n+X\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.Variable(tf.ones((6, 8)))\\n+X[:, 2:6].assign(tf.zeros(X[:, 2:6].shape))\\n+X\\n+```\\n+\\n+接下来，我们构造一个高度为1，宽度为2的核`K`，当我们与输入执行互相关运算时，如果水平相邻的元素相同，则输出为0。否则，输出为非零。\\n+\\n+```{.python .input}\\n+#@tab all\\n+K = d2l.tensor([[1.0, -1.0]])\\n+```\\n+\\n+我们已经准备好使用参数`X`(我们的输入)和`K`(我们的内核)执行互相关操作。如您所见，我们检测到从白色到黑色的边缘为1，从黑色到白色的边缘为-1。所有其他输出均采用值0。\\n+\\n+```{.python .input}\\n+#@tab all\\n+Y = corr2d(X, K)\\n+Y\\n+```\\n+\\n+现在，我们可以将内核应用于转置后的图像。不出所料，它消失了。内核`K`仅检测垂直边缘。\\n+\\n+```{.python .input}\\n+#@tab all\\n+corr2d(d2l.transpose(X), K)\\n+```\\n+\\n+## 学习内核\\n+\\n+如果我们知道这正是我们正在寻找的，那么用有限差分`[1, -1]`设计一个边缘检测器是非常巧妙的。然而，当我们查看较大的内核并考虑连续的卷积层时，可能不可能精确地手动指定每个过滤应该做什么。\\n+\\n+现在，让我们看看是否可以通过只查看输入-输出对来了解从`Y`生成`X`的内核。我们首先构造卷积层，并将其核初始化为随机张量。接下来，在每次迭代中，我们将使用平方误差将`Y`与卷积层的输出进行比较。然后我们可以计算梯度来更新内核。为简单起见，在下面我们使用二维卷积层的内置类，而忽略偏差。\\n+\\n+```{.python .input}\\n+# Construct a two-dimensional convolutional layer with 1 output channel and a\\n+# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\\n+conv2d = nn.Conv2D(1, kernel_size=(1, 2), use_bias=False)\\n+conv2d.initialize()\\n+\\n+# The two-dimensional convolutional layer uses four-dimensional input and\\n+# output in the format of (example, channel, height, width), where the batch\\n+# size (number of examples in the batch) and the number of channels are both 1\\n+X = X.reshape(1, 1, 6, 8)\\n+Y = Y.reshape(1, 1, 6, 7)\\n+\\n+for i in range(10):\\n+    with autograd.record():\\n+        Y_hat = conv2d(X)\\n+        l = (Y_hat - Y) ** 2\\n+    l.backward()\\n+    # Update the kernel\\n+    conv2d.weight.data()[:] -= 3e-2 * conv2d.weight.grad()\\n+    if (i + 1) % 2 == 0:\\n+        print(f\\'batch {i+1}, loss {float(l.sum()):.3f}\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+# Construct a two-dimensional convolutional layer with 1 output channel and a\\n+# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\\n+conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)\\n+\\n+# The two-dimensional convolutional layer uses four-dimensional input and\\n+# output in the format of (example channel, height, width), where the batch\\n+# size (number of examples in the batch) and the number of channels are both 1\\n+X = X.reshape((1, 1, 6, 8))\\n+Y = Y.reshape((1, 1, 6, 7))\\n+\\n+for i in range(10):\\n+    Y_hat = conv2d(X)\\n+    l = (Y_hat - Y) ** 2\\n+    conv2d.zero_grad()\\n+    l.sum().backward()\\n+    # Update the kernel\\n+    conv2d.weight.data[:] -= 3e-2 * conv2d.weight.grad\\n+    if (i + 1) % 2 == 0:\\n+        print(f\\'batch {i+1}, loss {l.sum():.3f}\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+# Construct a two-dimensional convolutional layer with 1 output channel and a\\n+# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\\n+conv2d = tf.keras.layers.Conv2D(1, (1, 2), use_bias=False)\\n+\\n+# The two-dimensional convolutional layer uses four-dimensional input and\\n+# output in the format of (example channel, height, width), where the batch\\n+# size (number of examples in the batch) and the number of channels are both 1\\n+X = tf.reshape(X, (1, 6, 8, 1))\\n+Y = tf.reshape(Y, (1, 6, 7, 1))\\n+\\n+Y_hat = conv2d(X)\\n+for i in range(10):\\n+    with tf.GradientTape(watch_accessed_variables=False) as g:\\n+        g.watch(conv2d.weights[0])\\n+        Y_hat = conv2d(X)\\n+        l = (abs(Y_hat - Y)) ** 2\\n+        # Update the kernel\\n+        update = tf.multiply(3e-2, g.gradient(l, conv2d.weights[0]))\\n+        weights = conv2d.get_weights()\\n+        weights[0] = conv2d.weights[0] - update\\n+        conv2d.set_weights(weights)\\n+        if (i + 1) % 2 == 0:\\n+            print(f\\'batch {i+1}, loss {tf.reduce_sum(l):.3f}\\')\\n+```\\n+\\n+请注意，在10次迭代之后，误差已下降到一个较小的值。现在我们来看看我们学过的核张量。\\n+\\n+```{.python .input}\\n+d2l.reshape(conv2d.weight.data(), (1, 2))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+d2l.reshape(conv2d.weight.data, (1, 2))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+d2l.reshape(conv2d.get_weights()[0], (1, 2))\\n+```\\n+\\n+事实上，学习到的核张量非常接近我们早先定义的核张量`K`。\\n+\\n+## 互相关和卷积\\n+\\n+回想一下我们从互相关和卷积运算之间:numref:`sec_why-conv`的对应关系中观察到的情况。这里，让我们继续考虑二维卷积层。如果这些层执行:eqref:`eq_2d-conv-discrete`中定义的严格卷积运算，而不是交叉相关，该怎么办？为了得到严格的*卷积*运算的输出，我们只需要水平和垂直翻转二维核张量，然后与输入张量进行*互相关*运算。\\n+\\n+值得注意的是，由于核在深度学习中是从数据中学习的，所以无论卷积层执行严格的卷积运算还是互相关运算，卷积层的输出都保持不受影响。\\n+\\n+为了说明这一点，假设卷积层执行*互相关‘并且在:numref:`fig_correlation`中学习核，其在这里被表示为矩阵$\\\\mathbf{K}$。假设其他条件保持不变，当该层改为执行严格的*卷积*时，学习到的内核$\\\\mathbf{K}\\'$将在$\\\\mathbf{K}$被水平和垂直翻转之后与$\\\\mathbf{K}\\'$相同。也就是说，当卷积层在:numref:`fig_correlation`和$\\\\mathbf{K}\\'$中对输入执行严格的“卷积”时，将在:numref:`fig_correlation`中获得相同的输出(输入和$\\\\mathbf{K}$的互相关)。\\n+\\n+为了与深度学习文献中的标准术语保持一致，我们将继续将互相关运算称为卷积，尽管严格地说，它略有不同。此外，我们使用术语*元素*来指代表示层表示或卷积核的任何张量的条目(或组件)。\\n+\\n+## 要素地图和感受场\\n+\\n+如在:numref:`subsec_why-conv-channels`中所描述的，在:numref:`fig_correlation`中输出的卷积层有时被称为*特征图*，因为它可以被视为到后续层的空间维度(例如，宽度和高度)中的学习表示(特征)。在CNN中，对于某些层的任何元素$x$，其*接受字段*指的是在前向传播期间可能影响$x$的计算的所有元素(来自所有先前层)。请注意，接受字段可能大于输入的实际大小。\\n+\\n+让我们继续使用:numref:`fig_correlation`来解释接受领域。在给定$2 \\\\times 2$卷积核的情况下，着色输出元素(值为$19$)的接受字段是输入的着色部分中的四个元素。现在，让我们将$2 \\\\times 2$的输出表示为$\\\\mathbf{Y}$，并考虑更深层的cnn，该cnn具有附加的$2 \\\\times 2$卷积层，该卷积层将$\\\\mathbf{Y}$作为其输入，输出单个元素$z$。在这种情况下，$z$上的接受字段$\\\\mathbf{Y}$包括$\\\\mathbf{Y}$的所有四个元素，而输入上的接受字段包括所有九个输入元素。因此，当要素地图中的任何元素需要更大的接受域来检测更大范围内的输入要素时，我们可以构建更深层次的网络。\\n+\\n+## 摘要\\n+\\n+* 二维卷积层的核心计算是二维互相关运算。在其最简单的形式中，这对二维输入数据和内核执行互相关操作，然后添加偏差。\\n+* 我们可以设计一个核来检测图像中的边缘。\\n+* 我们可以从数据中了解内核的参数。\\n+* 使用从数据学习的核，卷积层的输出保持不受影响，而不管这些层执行的操作(严格卷积或互相关)。\\n+* 当要素地图中的任何元素需要更大的接受范围来检测输入上更广泛的要素时，可以考虑更深层次的网络。\\n+\\n+## 练习\\n+\\n+1. 构建具有对角线边缘的图像`X`。\\n+    1. 如果将本节中的内核`K`应用于它，会发生什么情况？\\n+    1. 如果你调换`X`会发生什么？\\n+    1. 如果你调换`K`会发生什么？\\n+1. 当您尝试自动查找我们创建的`Conv2D`类的渐变时，您看到了哪种错误消息？\\n+1. 如何通过更改输入和核张量将互相关操作表示为矩阵乘法？\\n+1. 手动设计一些内核。\\n+    1. 二阶导数的核的形式是什么？\\n+    1. 积分的核是什么？\\n+    1. 要获得$d$次导数，核的最小大小是多少？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/65)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/66)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/271)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-neural-networks/index_baidu.md b/chapter_convolutional-neural-networks/index_baidu.md\\nnew file mode 100644\\nindex 00000000..d900df75\\n--- /dev/null\\n+++ b/chapter_convolutional-neural-networks/index_baidu.md\\n@@ -0,0 +1,21 @@\\n+# 卷积神经网络\\n+:label:`chap_cnn`\\n+\\n+在前面的章节中，我们遇到了图像数据，对于图像数据，每个示例都由一个二维像素网格组成。根据我们处理的是黑白图像还是彩色图像，每个像素位置可能分别与一个或多个数值相关联。到目前为止，我们处理这一富有结构的方式还很不令人满意。我们简单地丢弃了每个图像的空间结构，将它们展平成一维向量，通过一个完全连接的MLP给它们提供信息。由于这些网络对特征的顺序是不变的，所以无论我们是否保持与像素空间结构相对应的顺序，还是在拟合MLP参数之前对设计矩阵的列进行置换，我们都可以得到相似的结果。最好，我们将利用我们的先验知识，即附近的像素通常是相互关联的，以建立从图像数据中学习的有效模型。\\n+\\n+本章介绍卷积神经网络（CNNs），一个强大的神经网络家族，正是为此目的而设计的。基于CNN的体系结构现在在计算机视觉领域中无处不在，并且已经变得非常占主导地位，以至于今天几乎没有人会开发商业应用程序或参与与图像识别、对象检测或语义分割相关的竞争，而不以这种方法为基础。\\n+\\n+现代cnn，俗称cnn，其设计得益于生物学、群论和健康的实验修补。除了在获得精确模型方面的采样效率外，cnn在计算上往往是高效的，这是因为它们需要的参数比完全连接的体系结构少，而且卷积很容易在GPU内核之间并行化。因此，实践者经常尽可能地应用cnn，并且他们越来越成为可信的竞争对手，即使是在具有一维序列结构的任务上，例如音频、文本和时间序列分析，在这些任务中，递归神经网络是常规使用的。cnn的一些巧妙的调整也使它们在图结构数据和推荐系统中发挥了作用。\\n+\\n+首先，我们将介绍构成所有卷积网络主干的基本操作。这些包括卷积层本身，包括填充和跨步在内的细节，用于在相邻空间区域聚集信息的汇集层，在每一层使用多个通道，以及对现代建筑结构的仔细讨论。在本章的最后，我们将以一个完整的工作实例来结束这一章，这是早在现代深度学习兴起之前就成功部署的第一个卷积网络。在下一章中，我们将深入研究一些流行的和相对较新的CNN架构的完整实现，这些架构的设计代表了现代实践者常用的大多数技术。\\n+\\n+```toc\\n+:maxdepth: 2\\n+\\n+why-conv\\n+conv-layer\\n+padding-and-strides\\n+channels\\n+pooling\\n+lenet\\n+```\\ndiff --git a/chapter_convolutional-neural-networks/index_tencent.md b/chapter_convolutional-neural-networks/index_tencent.md\\nnew file mode 100644\\nindex 00000000..056c9290\\n--- /dev/null\\n+++ b/chapter_convolutional-neural-networks/index_tencent.md\\n@@ -0,0 +1,21 @@\\n+# 卷积神经网络\\n+:label:`chap_cnn`\\n+\\n+在前面的章节中，我们遇到了图像数据，每个示例都由二维像素网格组成。根据我们处理的是黑白图像还是彩色图像，每个像素位置可能分别与一个或多个数值相关联。到目前为止，我们处理这种丰富结构的方式非常不令人满意。我们简单地丢弃了每个图像的空间结构，将它们展平成一维向量，通过完全连接的MLP馈送它们。因为这些网络对于特征的顺序是不变的，所以无论我们是保持与像素空间结构相对应的顺序，还是在拟合MLP参数之前对设计矩阵的列进行置换，我们都可以得到相似的结果。优选地，我们将利用我们的先验知识，即附近的像素通常彼此相关，以构建用于从图像数据中学习的有效模型。\\n+\\n+本章介绍卷积神经网络(CNNs)，这是一个功能强大的神经网络家族，正是为此目的而设计的。基于CNN的体系结构现在在计算机视觉领域无处不在，并且已经变得如此主导，以至于今天几乎没有人会在没有这种方法的基础上开发商业应用程序或参加与图像识别、对象检测或语义分割相关的竞争。\\n+\\n+现代的CNN，俗称它们，应归功于生物学、群论和有益剂量的实验修修补补等方面的启发，它们的设计要归功于生物学、群论和有益剂量的实验修修补补。除了在实现精确模型方面的样本效率外，CNN往往在计算上也很高效，这既是因为它们比完全连接的体系结构需要更少的参数，也是因为卷积很容易跨GPU内核并行。因此，实践者经常在任何可能的情况下应用CNN，并且即使在具有一维序列结构的任务(例如，音频、文本和时间序列分析)上，它们也越来越多地成为可信的竞争者，在这些任务中，常规地使用递归神经网络。CNN的一些巧妙调整也使其适用于图形结构的数据和推荐系统。\\n+\\n+首先，我们将演练构成所有卷积网络主干的基本运算。这些内容包括卷积层本身、包括填充和跨度在内的细节、用于跨相邻空间区域聚合信息的汇合层、在每一层使用多个通道，以及对现代架构结构的仔细讨论。我们将以LENet的完整工作示例来结束本章，这是在现代深度学习兴起之前很久就成功部署的第一个卷积网络。在下一章中，我们将深入研究一些流行且相对较新的CNN架构的完整实现，这些架构的设计代表了现代实践者常用的大多数技术。\\n+\\n+```toc\\n+:maxdepth: 2\\n+\\n+why-conv\\n+conv-layer\\n+padding-and-strides\\n+channels\\n+pooling\\n+lenet\\n+```\\ndiff --git a/chapter_convolutional-neural-networks/lenet_baidu.md b/chapter_convolutional-neural-networks/lenet_baidu.md\\nnew file mode 100644\\nindex 00000000..6a33bf09\\n--- /dev/null\\n+++ b/chapter_convolutional-neural-networks/lenet_baidu.md\\n@@ -0,0 +1,336 @@\\n+# 卷积神经网络\\n+:label:`sec_lenet`\\n+\\n+我们现在有了组装一个功能齐全的CNN所需的所有成分。在我们之前遇到的图像数据中，我们将softmax回归模型（:numref:`sec_softmax_scratch`）和MLP模型（:numref:`sec_mlp_scratch`）应用于时装MNIST数据集中的服装图片。为了使这些数据适用于softmax回归和MLPs，我们首先将$28\\\\times28$矩阵中的每个图像展平为一个固定长度的$784$维向量，然后用全连通层对其进行处理。现在我们已经掌握了卷积层的处理方法，我们可以在图像中保留空间结构。作为用卷积层代替完全连接层的另一个好处，我们将享受到需要更少参数的更简洁模型。\\n+\\n+在本节中，我们将介绍*LeNet*，它是最早发布的cnn之一，因其在计算机视觉任务中的性能而受到广泛关注。这个模型是由当时at&T贝尔实验室的研究员Yann LeCun提出的（并以其命名），目的是识别图像:cite:`LeCun.Bottou.Bengio.ea.1998`中的手写数字。这项工作代表了十年来发展这项技术的研究的高潮。1989年，LeCun发表了第一篇通过反向传播成功训练CNN的研究。\\n+\\n+当时LeNet取得了与支持向量机性能相匹配的优秀结果，成为有监督学习的主流方法。LeNet最终被用于识别ATM机中处理存款的数字。时至今日，一些自动取款机仍在运行Yann和他的同事Leon Bottou在上世纪90年代写的代码！\\n+\\n+## 列奈\\n+\\n+在高层次上，LeNet（LeNet-5）由两部分组成：（i）由两个卷积层组成的卷积编码器；和（ii）由三个完全连接的层组成的密集块；该体系结构在:numref:`img_lenet`中进行了总结。\\n+\\n+![Data flow in LeNet. The input is a handwritten digit, the output a probability over 10 possible outcomes.](../img/lenet.svg)\\n+:label:`img_lenet`\\n+\\n+每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和随后的平均池操作。请注意，虽然ReLUs和max pooling工作得更好，但这些发现在20世纪90年代还没有出现。每个卷积层使用$5\\\\times 5$内核和sigmoid激活函数。这些层将空间排列的输入映射到多个二维特征映射，通常会增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个$2\\\\times2$池操作（步骤2）通过空间下采样将维数减少$4$倍。卷积块发出形状由（批大小、通道数、高度、宽度）给定的输出。\\n+\\n+为了将输出从卷积块传递到稠密块，我们必须在小批量中展平每个示例。换言之，我们将这个四维输入转换成完全连接层所期望的二维输入：作为提醒，我们所希望的二维表示使用第一个维度索引小批量中的示例，第二个维度给出每个示例的平面向量表示。LeNet的密集块有三个完全连接的层，分别有120、84和10个输出。因为我们仍在执行分类，所以10维输出层对应于可能的输出类的数量。\\n+\\n+虽然要真正理解LeNet内部的情况可能需要一些工作，但希望下面的代码片段能让您相信，用现代深度学习框架实现此类模型非常简单。我们只需要实例化一个`Sequential`块并将适当的层链接在一起。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import autograd, gluon, init, np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+net = nn.Sequential()\\n+net.add(nn.Conv2D(channels=6, kernel_size=5, padding=2, activation=\\'sigmoid\\'),\\n+        nn.AvgPool2D(pool_size=2, strides=2),\\n+        nn.Conv2D(channels=16, kernel_size=5, activation=\\'sigmoid\\'),\\n+        nn.AvgPool2D(pool_size=2, strides=2),\\n+        # `Dense` will transform an input of the shape (batch size, number of\\n+        # channels, height, width) into an input of the shape (batch size,\\n+        # number of channels * height * width) automatically by default\\n+        nn.Dense(120, activation=\\'sigmoid\\'),\\n+        nn.Dense(84, activation=\\'sigmoid\\'),\\n+        nn.Dense(10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+\\n+class Reshape(torch.nn.Module):\\n+    def forward(self, x):\\n+        return x.view(-1,1,28,28)\\n+\\n+net = torch.nn.Sequential(\\n+    Reshape(),\\n+    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),\\n+    nn.AvgPool2d(kernel_size=2, stride=2),\\n+    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\\n+    nn.AvgPool2d(kernel_size=2, stride=2),\\n+    nn.Flatten(),\\n+    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\\n+    nn.Linear(120, 84), nn.Sigmoid(),\\n+    nn.Linear(84, 10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+from tensorflow.distribute import MirroredStrategy, OneDeviceStrategy\\n+\\n+def net():\\n+    return tf.keras.models.Sequential([\\n+        tf.keras.layers.Conv2D(filters=6, kernel_size=5, activation=\\'sigmoid\\',\\n+                               padding=\\'same\\'),\\n+        tf.keras.layers.AvgPool2D(pool_size=2, strides=2),\\n+        tf.keras.layers.Conv2D(filters=16, kernel_size=5,\\n+                               activation=\\'sigmoid\\'),\\n+        tf.keras.layers.AvgPool2D(pool_size=2, strides=2),\\n+        tf.keras.layers.Flatten(),\\n+        tf.keras.layers.Dense(120, activation=\\'sigmoid\\'),\\n+        tf.keras.layers.Dense(84, activation=\\'sigmoid\\'),\\n+        tf.keras.layers.Dense(10)])\\n+```\\n+\\n+我们对原始模型做了一点小小的改动，去掉了最后一层的高斯激活。除此之外，这个网络与最初的LeNet-5体系结构相匹配。\\n+\\n+通过将一个单通道（黑白）$28 \\\\times 28$图像通过网络并在每一层打印输出形状，我们可以检查模型，以确保其操作与我们期望的:numref:`img_lenet_vert`一致。\\n+\\n+![Compressed notation for LeNet-5.](../img/lenet-vert.svg)\\n+:label:`img_lenet_vert`\\n+\\n+```{.python .input}\\n+X = np.random.uniform(size=(1, 1, 28, 28))\\n+net.initialize()\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.name, \\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+X = torch.randn(size=(1, 1, 28, 28), dtype=torch.float32)\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__,\\'output shape: \\\\t\\',X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.random.uniform((1, 28, 28, 1))\\n+for layer in net().layers:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__, \\'output shape: \\\\t\\', X.shape)\\n+```\\n+\\n+注意，在整个卷积块中的每一层的表示的高度和宽度都减小了（与前一层相比）。第一卷积层使用2个像素的填充来补偿由于使用$5 \\\\times 5$内核而导致的高度和宽度的减少。相反，第二卷积层放弃填充，因此高度和宽度都减少了4个像素。当我们往上一层的时候，通道的数量从输入的1层一层增加到第一层卷积层之后的6层和第二层卷积层之后的16个。但是，每个池层的高度和宽度都减半。最后，每一个完全连接的层都会减少维数，最终输出一个维数与类数相匹配的输出。\\n+\\n+## 培训\\n+\\n+现在我们已经实现了这个模型，让我们来做一个实验，看看LeNet在时尚MNIST上的表现。\\n+\\n+```{.python .input}\\n+#@tab all\\n+batch_size = 256\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\\n+```\\n+\\n+虽然cnn的参数较少，但它们的计算成本仍可能高于类似的深度mlp，因为每个参数都参与更多的乘法运算。如果你有机会使用GPU，这可能是一个很好的时机将它付诸行动，以加快培训。\\n+\\n+:begin_tab:`mxnet, pytorch`\\n+为了进行评估，我们需要对:numref:`sec_softmax_scratch`中描述的`evaluate_accuracy`函数进行一点修改。因为完整的数据集在主内存中，所以在模型使用GPU计算数据集之前，我们需要将其复制到GPU内存中。\\n+:end_tab:\\n+\\n+```{.python .input}\\n+def evaluate_accuracy_gpu(net, data_iter, device=None):  #@save\\n+    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\\n+    if not device:  # Query the first device where the first parameter is on\\n+        device = list(net.collect_params().values())[0].list_ctx()[0]\\n+    # No. of correct predictions, no. of predictions\\n+    metric = d2l.Accumulator(2)\\n+    for X, y in data_iter:\\n+        X, y = X.as_in_ctx(device), y.as_in_ctx(device)\\n+        metric.add(d2l.accuracy(net(X), y), d2l.size(y))\\n+    return metric[0]/metric[1]\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def evaluate_accuracy_gpu(net, data_iter, device=None): #@save\\n+    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\\n+    net.eval()  # Set the model to evaluation mode\\n+    if not device:\\n+        device = next(iter(net.parameters())).device\\n+    # No. of correct predictions, no. of predictions\\n+    metric = d2l.Accumulator(2)\\n+    for X, y in data_iter:\\n+        X, y = X.to(device), y.to(device)\\n+        metric.add(d2l.accuracy(net(X), y), d2l.size(y))\\n+    return metric[0] / metric[1]\\n+```\\n+\\n+我们还需要更新我们的培训功能来处理GPU。与:numref:`sec_softmax_scratch`中定义的`train_epoch_ch3`不同，在进行前向和后向传播之前，我们需要将每一小批数据移动到我们指定的设备（希望是GPU）。\\n+\\n+训练功能`train_ch6`也类似于:numref:`sec_softmax_scratch`中定义的`train_ch3`。由于我们将实现多层网络，因此我们将主要依赖于高级api。下面的训练函数假设从高级api创建的模型作为输入，并进行相应的优化。我们使用`device`中介绍的Xavier初始化初始化`device`参数指示的设备上的模型参数。与MLPs一样，我们的损失函数是交叉熵，我们通过小批量随机梯度下降使其最小化。因为每一个纪元都需要几十秒的时间，所以我们更频繁地看到训练的失败。\\n+\\n+```{.python .input}\\n+#@save\\n+def train_ch6(net, train_iter, test_iter, num_epochs, lr,\\n+              device=d2l.try_gpu()):\\n+    \"\"\"Train a model with a GPU (defined in Chapter 6).\"\"\"\\n+    net.initialize(force_reinit=True, ctx=device, init=init.Xavier())\\n+    loss = gluon.loss.SoftmaxCrossEntropyLoss()\\n+    trainer = gluon.Trainer(net.collect_params(),\\n+                            \\'sgd\\', {\\'learning_rate\\': lr})\\n+    animator = d2l.Animator(xlabel=\\'epoch\\', xlim=[0, num_epochs],\\n+                            legend=[\\'train loss\\', \\'train acc\\', \\'test acc\\'])\\n+    timer = d2l.Timer()\\n+    for epoch in range(num_epochs):\\n+        # Sum of training loss, sum of training accuracy, no. of examples\\n+        metric = d2l.Accumulator(3)\\n+        for i, (X, y) in enumerate(train_iter):\\n+            timer.start()\\n+            # Here is the major difference compared with `d2l.train_epoch_ch3`\\n+            X, y = X.as_in_ctx(device), y.as_in_ctx(device)\\n+            with autograd.record():\\n+                y_hat = net(X)\\n+                l = loss(y_hat, y)\\n+            l.backward()\\n+            trainer.step(X.shape[0])\\n+            metric.add(l.sum(), d2l.accuracy(y_hat, y), X.shape[0])\\n+            timer.stop()\\n+            train_loss = metric[0] / metric[2]\\n+            train_acc = metric[1] / metric[2]\\n+            if (i + 1) % 50 == 0:\\n+                animator.add(epoch + i / len(train_iter),\\n+                             (train_loss, train_acc, None))\\n+        test_acc = evaluate_accuracy_gpu(net, test_iter)\\n+        animator.add(epoch + 1, (None, None, test_acc))\\n+    print(f\\'loss {train_loss:.3f}, train acc {train_acc:.3f}, \\'\\n+          f\\'test acc {test_acc:.3f}\\')\\n+    print(f\\'{metric[2] * num_epochs / timer.sum():.1f} examples/sec \\'\\n+          f\\'on {str(device)}\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+#@save\\n+def train_ch6(net, train_iter, test_iter, num_epochs, lr,\\n+              device=d2l.try_gpu()):\\n+    \"\"\"Train a model with a GPU (defined in Chapter 6).\"\"\"\\n+    def init_weights(m):\\n+        if type(m) == nn.Linear or type(m) == nn.Conv2d:\\n+            torch.nn.init.xavier_uniform_(m.weight)\\n+    net.apply(init_weights)\\n+    print(\\'training on\\', device)\\n+    net.to(device)\\n+    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\\n+    loss = nn.CrossEntropyLoss()\\n+    animator = d2l.Animator(xlabel=\\'epoch\\', xlim=[0, num_epochs],\\n+                            legend=[\\'train loss\\', \\'train acc\\', \\'test acc\\'])\\n+    timer = d2l.Timer()\\n+    for epoch in range(num_epochs):\\n+        # Sum of training loss, sum of training accuracy, no. of examples\\n+        metric = d2l.Accumulator(3)\\n+        for i, (X, y) in enumerate(train_iter):\\n+            timer.start()\\n+            net.train()\\n+            optimizer.zero_grad()\\n+            X, y = X.to(device), y.to(device)\\n+            y_hat = net(X)\\n+            l = loss(y_hat, y)\\n+            l.backward()\\n+            optimizer.step()\\n+            with torch.no_grad():\\n+                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\\n+            timer.stop()\\n+            train_loss = metric[0]/metric[2]\\n+            train_acc = metric[1]/metric[2]\\n+            if (i + 1) % 50 == 0:\\n+                animator.add(epoch + i / len(train_iter),\\n+                             (train_loss, train_acc, None))\\n+        test_acc = evaluate_accuracy_gpu(net, test_iter)\\n+        animator.add(epoch+1, (None, None, test_acc))\\n+    print(f\\'loss {train_loss:.3f}, train acc {train_acc:.3f}, \\'\\n+          f\\'test acc {test_acc:.3f}\\')\\n+    print(f\\'{metric[2] * num_epochs / timer.sum():.1f} examples/sec \\'\\n+          f\\'on {str(device)}\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class TrainCallback(tf.keras.callbacks.Callback):  #@save\\n+    \"\"\"A callback to visiualize the training progress.\"\"\"\\n+    def __init__(self, net, train_iter, test_iter, num_epochs, device_name):\\n+        self.timer = d2l.Timer()\\n+        self.animator = d2l.Animator(\\n+            xlabel=\\'epoch\\', xlim=[0, num_epochs], legend=[\\n+                \\'train loss\\', \\'train acc\\', \\'test acc\\'])\\n+        self.net = net\\n+        self.train_iter = train_iter\\n+        self.test_iter = test_iter\\n+        self.num_epochs = num_epochs\\n+        self.device_name = device_name\\n+    def on_epoch_begin(self, epoch, logs=None):\\n+        self.timer.start()\\n+    def on_epoch_end(self, epoch, logs):\\n+        self.timer.stop()\\n+        test_acc = self.net.evaluate(\\n+            self.test_iter, verbose=0, return_dict=True)[\\'accuracy\\']\\n+        metrics = (logs[\\'loss\\'], logs[\\'accuracy\\'], test_acc)\\n+        self.animator.add(epoch+1, metrics)\\n+        if epoch == self.num_epochs - 1:\\n+            batch_size = next(iter(self.train_iter))[0].shape[0]\\n+            num_examples = batch_size * tf.data.experimental.cardinality(\\n+                self.train_iter).numpy()\\n+            print(f\\'loss {metrics[0]:.3f}, train acc {metrics[1]:.3f}, \\'\\n+                  f\\'test acc {metrics[2]:.3f}\\')\\n+            print(f\\'{num_examples / self.timer.avg():.1f} examples/sec on \\'\\n+                  f\\'{str(self.device_name)}\\')\\n+\\n+#@save\\n+def train_ch6(net_fn, train_iter, test_iter, num_epochs, lr,\\n+              device=d2l.try_gpu()):\\n+    \"\"\"Train a model with a GPU (defined in Chapter 6).\"\"\"\\n+    device_name = device._device_name\\n+    strategy = tf.distribute.OneDeviceStrategy(device_name)\\n+    with strategy.scope():\\n+        optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\\n+        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\\n+        net = net_fn()\\n+        net.compile(optimizer=optimizer, loss=loss, metrics=[\\'accuracy\\'])\\n+    callback = TrainCallback(net, train_iter, test_iter, num_epochs,\\n+                             device_name)\\n+    net.fit(train_iter, epochs=num_epochs, verbose=0, callbacks=[callback])\\n+    return net\\n+```\\n+\\n+现在让我们训练和评估LeNet-5模型。\\n+\\n+```{.python .input}\\n+#@tab all\\n+lr, num_epochs = 0.9, 10\\n+train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+## 摘要\\n+\\n+* CNN是一个使用卷积层的网络。\\n+* 在CNN中，我们交织卷积、非线性和（通常）池操作。\\n+* 在CNN中，卷积层通常被布置成这样，它们逐渐降低表示的空间分辨率，同时增加信道的数量。\\n+* 在传统的cnn中，由卷积块编码的表示在发射输出之前由一个或多个完全连接的层处理。\\n+* LeNet可以说是这类网络的第一次成功部署。\\n+\\n+## 练习\\n+\\n+1. 将平均池替换为最大池。会发生什么？\\n+1. 尝试基于LeNet构造一个更复杂的网络来提高其精度。\\n+    1. 调整卷积窗口大小。\\n+    1. 调整输出通道的数量。\\n+    1. 调整激活功能（如ReLU）。\\n+    1. 调整卷积层数。\\n+    1. 调整完全连接层的数量。\\n+    1. 调整学习率和其他培训细节（例如初始化和时段数）\\n+1. 在原始MNIST数据集上试用改进后的网络。\\n+1. 显示不同输入（例如毛衣和外套）的第一层和第二层LeNet的激活。\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/73)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/74)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/275)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-neural-networks/lenet_tencent.md b/chapter_convolutional-neural-networks/lenet_tencent.md\\nnew file mode 100644\\nindex 00000000..e68cf3fb\\n--- /dev/null\\n+++ b/chapter_convolutional-neural-networks/lenet_tencent.md\\n@@ -0,0 +1,336 @@\\n+# 卷积神经网络(LENet)\\n+:label:`sec_lenet`\\n+\\n+我们现在已经具备了组装一个功能齐全的CNN所需的所有材料。在我们之前遇到的图像数据中，我们将Softmax回归模型(:numref:`sec_softmax_scratch`)和mlp模型(:numref:`sec_mlp_scratch`)应用于Fashion-MNIST数据集中的服装图片。为了使这些数据服从Softmax回归和MLP，我们首先将$28\\\\times28$矩阵中的每个图像展平成固定长度的$784$维向量，然后用完全连通的层对它们进行处理。既然我们已经掌握了卷积图层，我们就可以在图像中保留空间结构。作为用卷积层替换完全连通层的另一个好处，我们将享受到需要更少参数的更简约的模型。\\n+\\n+在这一部分中，我们将介绍*LENet*，这是最早发布的CNN之一，它因其在计算机视觉任务中的性能而引起广泛关注。该模型是由当时在AT&T贝尔实验室担任研究员的严乐村(并以他的名字命名)提出的，目的是识别图像:cite:`LeCun.Bottou.Bengio.ea.1998`中的手写数字。这项工作代表了十年来开发这项技术的研究的顶峰。1989年，LeCun发表了第一个通过反向传播成功训练CNN的研究。\\n+\\n+当时，LeNet取得了与支持向量机性能相当的突出结果，当时在监督学习中占据主导地位。LeNet最终被改造成识别用于处理ATM机存款的数字。时至今日，一些ATM机还在运行严恩和他的同事里昂·博图在20世纪90年代写的代码！\\n+\\n+## 乐网\\n+\\n+从高层次上讲，LENET(LENET-5)由两个部分组成：(I)由两个卷积层组成的卷积编码器；(Ii)由三个完全连接层组成的密集挡路；其体系结构在:numref:`img_lenet`中总结。\\n+\\n+![Data flow in LeNet. The input is a handwritten digit, the output a probability over 10 possible outcomes.](../img/lenet.svg)\\n+:label:`img_lenet`\\n+\\n+每个卷积挡路中的基本单元是卷积层、S形激活函数和随后的平均合用操作。请注意，虽然RELU和MAX-Pooling工作得更好，但这些发现在20世纪90年代还没有完成。每个卷积层使用$5\\\\times 5$核和Sigmoid激活函数。这些图层将空间排列的输入映射到多个二维要素地图，通常会增加通道数量。第一卷积层有6个输出通道，第二层有16个输出通道。每次$2\\\\times2$的合并操作(步长2)通过空间下采样将维数降低$4$倍。卷积挡路发出具有(批次大小、通道数、高度、宽度)给定形状的输出。\\n+\\n+为了将卷积挡路的输出传递给密集的挡路，我们必须将小批量中的每个示例都展平。换句话说，我们接受这个四维输入，并将其转换为完全连接层所期望的二维输入：提醒一下，我们所需的二维表示使用第一维索引小批量中的示例，第二维给出每个示例的平面向量表示。乐网的密集挡路有三个全连接层，分别有120、84和10个输出。因为我们仍在执行分类，所以10维输出层对应于可能的输出类的数量。\\n+\\n+虽然要真正理解LeNet内部发生的事情可能需要做一些工作，但希望下面的代码片段能让您相信，使用现代深度学习框架实现这样的模型非常简单。我们只需要实例化一个`Sequential`的挡路，并将适当的层链接在一起。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import autograd, gluon, init, np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+net = nn.Sequential()\\n+net.add(nn.Conv2D(channels=6, kernel_size=5, padding=2, activation=\\'sigmoid\\'),\\n+        nn.AvgPool2D(pool_size=2, strides=2),\\n+        nn.Conv2D(channels=16, kernel_size=5, activation=\\'sigmoid\\'),\\n+        nn.AvgPool2D(pool_size=2, strides=2),\\n+        # `Dense` will transform an input of the shape (batch size, number of\\n+        # channels, height, width) into an input of the shape (batch size,\\n+        # number of channels * height * width) automatically by default\\n+        nn.Dense(120, activation=\\'sigmoid\\'),\\n+        nn.Dense(84, activation=\\'sigmoid\\'),\\n+        nn.Dense(10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+\\n+class Reshape(torch.nn.Module):\\n+    def forward(self, x):\\n+        return x.view(-1,1,28,28)\\n+\\n+net = torch.nn.Sequential(\\n+    Reshape(),\\n+    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),\\n+    nn.AvgPool2d(kernel_size=2, stride=2),\\n+    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\\n+    nn.AvgPool2d(kernel_size=2, stride=2),\\n+    nn.Flatten(),\\n+    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\\n+    nn.Linear(120, 84), nn.Sigmoid(),\\n+    nn.Linear(84, 10))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+from tensorflow.distribute import MirroredStrategy, OneDeviceStrategy\\n+\\n+def net():\\n+    return tf.keras.models.Sequential([\\n+        tf.keras.layers.Conv2D(filters=6, kernel_size=5, activation=\\'sigmoid\\',\\n+                               padding=\\'same\\'),\\n+        tf.keras.layers.AvgPool2D(pool_size=2, strides=2),\\n+        tf.keras.layers.Conv2D(filters=16, kernel_size=5,\\n+                               activation=\\'sigmoid\\'),\\n+        tf.keras.layers.AvgPool2D(pool_size=2, strides=2),\\n+        tf.keras.layers.Flatten(),\\n+        tf.keras.layers.Dense(120, activation=\\'sigmoid\\'),\\n+        tf.keras.layers.Dense(84, activation=\\'sigmoid\\'),\\n+        tf.keras.layers.Dense(10)])\\n+```\\n+\\n+我们对原始模型做了一个小改动，去掉了最后一层的高斯激活。除此之外，该网络与最初的LeNet-5架构相匹配。\\n+\\n+通过通过网络传递单通道(黑白)$28 \\\\times 28$图像并在每一层打印输出形状，我们可以检查模型以确保其操作符合我们对:numref:`img_lenet_vert`的预期。\\n+\\n+![Compressed notation for LeNet-5.](../img/lenet-vert.svg)\\n+:label:`img_lenet_vert`\\n+\\n+```{.python .input}\\n+X = np.random.uniform(size=(1, 1, 28, 28))\\n+net.initialize()\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.name, \\'output shape:\\\\t\\', X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+X = torch.randn(size=(1, 1, 28, 28), dtype=torch.float32)\\n+for layer in net:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__,\\'output shape: \\\\t\\',X.shape)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.random.uniform((1, 28, 28, 1))\\n+for layer in net().layers:\\n+    X = layer(X)\\n+    print(layer.__class__.__name__, \\'output shape: \\\\t\\', X.shape)\\n+```\\n+\\n+注意，在整个卷积挡路的每一层处的表示的高度和宽度减小(与前一层相比)。第一卷积层使用2个像素的填充来补偿因使用$5 \\\\times 5$内核而导致的高度和宽度的减小。相反，第二卷积层放弃填充，因此高度和宽度都减少了4个像素。当我们向上层叠时，通道数逐层增加，从输入中的1个增加到第一个卷积层之后的6个和第二个卷积层之后的16个。但是，每个池化层的高度和宽度都减半。最后，每个完全连接的层都会降低维数，最终输出其维数与类的数量相匹配的输出。\\n+\\n+## 培训\\n+\\n+现在我们已经实现了这个模型，让我们来做一个实验，看看LeNet在Fashion-MNIST上的表现如何。\\n+\\n+```{.python .input}\\n+#@tab all\\n+batch_size = 256\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\\n+```\\n+\\n+虽然CNN的参数较少，但它们的计算成本仍可能比类似的深度MLP更高，因为每个参数参与的乘法要多得多。如果您可以使用GPU，这可能是将其付诸实施以加快培训的好时机。\\n+\\n+:begin_tab:`mxnet, pytorch`\\n+为了进行评估，我们需要对`evaluate_accuracy`中描述的:numref:`sec_softmax_scratch`函数稍作修改。由于完整的数据集在主内存中，因此在模型使用GPU计算数据集之前，我们需要将其复制到GPU内存中。\\n+:end_tab:\\n+\\n+```{.python .input}\\n+def evaluate_accuracy_gpu(net, data_iter, device=None):  #@save\\n+    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\\n+    if not device:  # Query the first device where the first parameter is on\\n+        device = list(net.collect_params().values())[0].list_ctx()[0]\\n+    # No. of correct predictions, no. of predictions\\n+    metric = d2l.Accumulator(2)\\n+    for X, y in data_iter:\\n+        X, y = X.as_in_ctx(device), y.as_in_ctx(device)\\n+        metric.add(d2l.accuracy(net(X), y), d2l.size(y))\\n+    return metric[0]/metric[1]\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def evaluate_accuracy_gpu(net, data_iter, device=None): #@save\\n+    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\\n+    net.eval()  # Set the model to evaluation mode\\n+    if not device:\\n+        device = next(iter(net.parameters())).device\\n+    # No. of correct predictions, no. of predictions\\n+    metric = d2l.Accumulator(2)\\n+    for X, y in data_iter:\\n+        X, y = X.to(device), y.to(device)\\n+        metric.add(d2l.accuracy(net(X), y), d2l.size(y))\\n+    return metric[0] / metric[1]\\n+```\\n+\\n+我们还需要更新我们的培训功能来处理GPU。与:numref:`sec_softmax_scratch`中定义的`train_epoch_ch3`不同，在进行向前和向后传播之前，我们现在需要将每个小批量数据移动到我们指定的设备(希望是图形处理器)。\\n+\\n+训练函数`train_ch6`也类似于:numref:`sec_softmax_scratch`中定义的`train_ch3`。由于我们未来将实施具有多个层的网络，因此我们将主要依靠高级API。以下训练函数假定从高级API创建的模型作为输入，并相应地进行优化。我们使用`device`中介绍的哈维尔初始化来初始化`device`参数所指示的设备上的模型参数。就像MLP一样，我们的损失函数是交叉熵，我们通过小批量随机梯度下降来最小化它。因为每个纪元需要几十秒来运行，所以我们更频繁地将训练损失可视化。\\n+\\n+```{.python .input}\\n+#@save\\n+def train_ch6(net, train_iter, test_iter, num_epochs, lr,\\n+              device=d2l.try_gpu()):\\n+    \"\"\"Train a model with a GPU (defined in Chapter 6).\"\"\"\\n+    net.initialize(force_reinit=True, ctx=device, init=init.Xavier())\\n+    loss = gluon.loss.SoftmaxCrossEntropyLoss()\\n+    trainer = gluon.Trainer(net.collect_params(),\\n+                            \\'sgd\\', {\\'learning_rate\\': lr})\\n+    animator = d2l.Animator(xlabel=\\'epoch\\', xlim=[0, num_epochs],\\n+                            legend=[\\'train loss\\', \\'train acc\\', \\'test acc\\'])\\n+    timer = d2l.Timer()\\n+    for epoch in range(num_epochs):\\n+        # Sum of training loss, sum of training accuracy, no. of examples\\n+        metric = d2l.Accumulator(3)\\n+        for i, (X, y) in enumerate(train_iter):\\n+            timer.start()\\n+            # Here is the major difference compared with `d2l.train_epoch_ch3`\\n+            X, y = X.as_in_ctx(device), y.as_in_ctx(device)\\n+            with autograd.record():\\n+                y_hat = net(X)\\n+                l = loss(y_hat, y)\\n+            l.backward()\\n+            trainer.step(X.shape[0])\\n+            metric.add(l.sum(), d2l.accuracy(y_hat, y), X.shape[0])\\n+            timer.stop()\\n+            train_loss = metric[0] / metric[2]\\n+            train_acc = metric[1] / metric[2]\\n+            if (i + 1) % 50 == 0:\\n+                animator.add(epoch + i / len(train_iter),\\n+                             (train_loss, train_acc, None))\\n+        test_acc = evaluate_accuracy_gpu(net, test_iter)\\n+        animator.add(epoch + 1, (None, None, test_acc))\\n+    print(f\\'loss {train_loss:.3f}, train acc {train_acc:.3f}, \\'\\n+          f\\'test acc {test_acc:.3f}\\')\\n+    print(f\\'{metric[2] * num_epochs / timer.sum():.1f} examples/sec \\'\\n+          f\\'on {str(device)}\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+#@save\\n+def train_ch6(net, train_iter, test_iter, num_epochs, lr,\\n+              device=d2l.try_gpu()):\\n+    \"\"\"Train a model with a GPU (defined in Chapter 6).\"\"\"\\n+    def init_weights(m):\\n+        if type(m) == nn.Linear or type(m) == nn.Conv2d:\\n+            torch.nn.init.xavier_uniform_(m.weight)\\n+    net.apply(init_weights)\\n+    print(\\'training on\\', device)\\n+    net.to(device)\\n+    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\\n+    loss = nn.CrossEntropyLoss()\\n+    animator = d2l.Animator(xlabel=\\'epoch\\', xlim=[0, num_epochs],\\n+                            legend=[\\'train loss\\', \\'train acc\\', \\'test acc\\'])\\n+    timer = d2l.Timer()\\n+    for epoch in range(num_epochs):\\n+        # Sum of training loss, sum of training accuracy, no. of examples\\n+        metric = d2l.Accumulator(3)\\n+        for i, (X, y) in enumerate(train_iter):\\n+            timer.start()\\n+            net.train()\\n+            optimizer.zero_grad()\\n+            X, y = X.to(device), y.to(device)\\n+            y_hat = net(X)\\n+            l = loss(y_hat, y)\\n+            l.backward()\\n+            optimizer.step()\\n+            with torch.no_grad():\\n+                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\\n+            timer.stop()\\n+            train_loss = metric[0]/metric[2]\\n+            train_acc = metric[1]/metric[2]\\n+            if (i + 1) % 50 == 0:\\n+                animator.add(epoch + i / len(train_iter),\\n+                             (train_loss, train_acc, None))\\n+        test_acc = evaluate_accuracy_gpu(net, test_iter)\\n+        animator.add(epoch+1, (None, None, test_acc))\\n+    print(f\\'loss {train_loss:.3f}, train acc {train_acc:.3f}, \\'\\n+          f\\'test acc {test_acc:.3f}\\')\\n+    print(f\\'{metric[2] * num_epochs / timer.sum():.1f} examples/sec \\'\\n+          f\\'on {str(device)}\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class TrainCallback(tf.keras.callbacks.Callback):  #@save\\n+    \"\"\"A callback to visiualize the training progress.\"\"\"\\n+    def __init__(self, net, train_iter, test_iter, num_epochs, device_name):\\n+        self.timer = d2l.Timer()\\n+        self.animator = d2l.Animator(\\n+            xlabel=\\'epoch\\', xlim=[0, num_epochs], legend=[\\n+                \\'train loss\\', \\'train acc\\', \\'test acc\\'])\\n+        self.net = net\\n+        self.train_iter = train_iter\\n+        self.test_iter = test_iter\\n+        self.num_epochs = num_epochs\\n+        self.device_name = device_name\\n+    def on_epoch_begin(self, epoch, logs=None):\\n+        self.timer.start()\\n+    def on_epoch_end(self, epoch, logs):\\n+        self.timer.stop()\\n+        test_acc = self.net.evaluate(\\n+            self.test_iter, verbose=0, return_dict=True)[\\'accuracy\\']\\n+        metrics = (logs[\\'loss\\'], logs[\\'accuracy\\'], test_acc)\\n+        self.animator.add(epoch+1, metrics)\\n+        if epoch == self.num_epochs - 1:\\n+            batch_size = next(iter(self.train_iter))[0].shape[0]\\n+            num_examples = batch_size * tf.data.experimental.cardinality(\\n+                self.train_iter).numpy()\\n+            print(f\\'loss {metrics[0]:.3f}, train acc {metrics[1]:.3f}, \\'\\n+                  f\\'test acc {metrics[2]:.3f}\\')\\n+            print(f\\'{num_examples / self.timer.avg():.1f} examples/sec on \\'\\n+                  f\\'{str(self.device_name)}\\')\\n+\\n+#@save\\n+def train_ch6(net_fn, train_iter, test_iter, num_epochs, lr,\\n+              device=d2l.try_gpu()):\\n+    \"\"\"Train a model with a GPU (defined in Chapter 6).\"\"\"\\n+    device_name = device._device_name\\n+    strategy = tf.distribute.OneDeviceStrategy(device_name)\\n+    with strategy.scope():\\n+        optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\\n+        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\\n+        net = net_fn()\\n+        net.compile(optimizer=optimizer, loss=loss, metrics=[\\'accuracy\\'])\\n+    callback = TrainCallback(net, train_iter, test_iter, num_epochs,\\n+                             device_name)\\n+    net.fit(train_iter, epochs=num_epochs, verbose=0, callbacks=[callback])\\n+    return net\\n+```\\n+\\n+现在让我们训练和评估LeNet-5模型。\\n+\\n+```{.python .input}\\n+#@tab all\\n+lr, num_epochs = 0.9, 10\\n+train_ch6(net, train_iter, test_iter, num_epochs, lr)\\n+```\\n+\\n+## 摘要\\n+\\n+* CNN是采用卷积层的网络。\\n+* 在CNN中，我们交织卷积、非线性和(通常)合并操作。\\n+* 在CNN中，卷积层通常被安排成使它们逐渐降低表示的空间分辨率，同时增加通道的数量。\\n+* 在传统的CNN中，由卷积块编码的表示在输出之前由一个或多个全连接层处理。\\n+* LeNet可以说是第一个成功部署这种网络的公司。\\n+\\n+## 练习\\n+\\n+1. 将平均池化替换为最大池化。会发生什么事？\\n+1. 尝试在LENet的基础上构建更复杂的网络，以提高其准确性。\\n+    1. 调整卷积窗口大小。\\n+    1. 调整输出通道数。\\n+    1. 调整激活功能(例如，RELU)。\\n+    1. 调整卷积层数。\\n+    1. 调整完全连接的层数。\\n+    1. 调整学习速率和其他训练细节(例如，初始化和历元数)。\\n+1. 在原始MNIST数据集上试用改进后的网络。\\n+1. 显示不同输入(例如毛衣和外套)的第一层和第二层LENET的激活。\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/73)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/74)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/275)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-neural-networks/padding-and-strides_baidu.md b/chapter_convolutional-neural-networks/padding-and-strides_baidu.md\\nnew file mode 100644\\nindex 00000000..2706614b\\n--- /dev/null\\n+++ b/chapter_convolutional-neural-networks/padding-and-strides_baidu.md\\n@@ -0,0 +1,212 @@\\n+# 垫步和跨步\\n+:label:`sec_padding`\\n+\\n+在前面的:numref:`fig_correlation`示例中，我们的输入高度和宽度都是3，卷积核的高度和宽度都是2，生成的输出表示的维数为$2\\\\times2$。正如我们在:numref:`sec_conv_layer`中所概括的，假设输入形状是$n_h\\\\times n_w$，卷积核形状是$k_h\\\\times k_w$，那么输出形状将是$(n_h-k_h+1) \\\\times (n_w-k_w+1)$。因此，卷积层的输出形状由输入的形状和卷积核的形状决定。\\n+\\n+在一些情况下，我们结合了一些技术，包括填充和跨步卷积，这些技术会影响输出的大小。作为动机，请注意，由于内核的宽度和高度通常大于$1$，在应用了许多连续的卷积之后，我们最终得到的输出往往远远小于我们的输入。如果我们从一个$240 \\\\times 240$像素的图像开始，$10$层$5 \\\\times 5$卷积将图像减少到$200 \\\\times 200$像素，将图像切成$30\\\\%$，并消除原始图像边界上的任何有趣信息。\\n+*Padding*是处理此问题最流行的工具。\\n+\\n+在其他情况下，我们可能希望大幅降低维度，例如，如果我们发现原始的输入分辨率难以处理。\\n+*快速卷积*是一种流行的技术，可以在这些情况下提供帮助。\\n+\\n+## 衬垫\\n+\\n+如上所述，在应用卷积层时，一个棘手的问题是，我们往往会丢失图像周长上的像素。由于我们通常使用小内核，对于任何给定的卷积，我们可能只会损失几个像素，但是当我们应用许多连续的卷积层时，这会累积起来。解决这个问题的一个简单方法是在输入图像的边界周围添加额外的填充像素，从而增加图像的有效大小。通常，我们将额外像素的值设置为零。在:numref:`img_conv_pad`中，我们填充$3 \\\\times 3$输入，将其大小增加到$5 \\\\times 5$。相应的输出随后增加到$4 \\\\times 4$矩阵。阴影部分是第一个输出元素以及用于输出计算的输入和核张量元素：$0\\\\times0+0\\\\times1+0\\\\times2+0\\\\times3=0$。\\n+\\n+![Two-dimensional cross-correlation with padding.](../img/conv-pad.svg)\\n+:label:`img_conv_pad`\\n+\\n+通常，如果我们添加$p_h$行填充（大约一半在顶部，一半在底部）和$p_w$列填充（大约一半在左侧，一半在右侧），则输出形状将为\\n+\\n+$$(n_h-k_h+p_h+1)\\\\times(n_w-k_w+p_w+1).$$\\n+\\n+这意味着输出的高度和宽度将分别增加$p_h$和$p_w$。\\n+\\n+在许多情况下，我们需要设置$p_h=k_h-1$和$p_w=k_w-1$，使输入和输出具有相同的高度和宽度。这将使得在构建网络时更容易预测每个层的输出形状。假设$k_h$在这里是奇数，我们将在高度的两侧填充$p_h/2$行。如果$k_h$是偶数，一种可能是在输入的顶部填充$\\\\lceil p_h/2\\\\rceil$行，在底部填充$\\\\lfloor p_h/2\\\\rfloor$行。我们将用同样的方法垫宽两边。\\n+\\n+cnn通常使用具有奇数个高度和宽度值的卷积核，例如1、3、5或7。选择奇数个内核大小的好处是，我们可以保留空间维度，同时在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。\\n+\\n+此外，这种使用奇数核和填充来精确保持维度的做法提供了一个文书上的好处。对于任意二维张量`X`，当核的大小为奇数且所有边上的填充行数和列数相同时，产生与输入相同高和宽的输出，我们知道输出`Y[i, j]`是通过输入核和卷积核与以`X[i, j]`为中心的窗口的互相关来计算的。\\n+\\n+在下面的示例中，我们创建一个高度和宽度为3的二维卷积层，并在所有边上应用1个像素的填充。给定一个高度和宽度为8的输入，我们发现输出的高度和宽度也是8。\\n+\\n+```{.python .input}\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+# For convenience, we define a function to calculate the convolutional layer.\\n+# This function initializes the convolutional layer weights and performs\\n+# corresponding dimensionality elevations and reductions on the input and\\n+# output\\n+def comp_conv2d(conv2d, X):\\n+    conv2d.initialize()\\n+    # Here (1, 1) indicates that the batch size and the number of channels\\n+    # are both 1\\n+    X = X.reshape((1, 1) + X.shape)\\n+    Y = conv2d(X)\\n+    # Exclude the first two dimensions that do not interest us: examples and\\n+    # channels\\n+    return Y.reshape(Y.shape[2:])\\n+\\n+# Note that here 1 row or column is padded on either side, so a total of 2\\n+# rows or columns are added\\n+conv2d = nn.Conv2D(1, kernel_size=3, padding=1)\\n+X = np.random.uniform(size=(8, 8))\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+import torch\\n+from torch import nn\\n+\\n+# We define a convenience function to calculate the convolutional layer. This\\n+# function initializes the convolutional layer weights and performs\\n+# corresponding dimensionality elevations and reductions on the input and\\n+# output\\n+def comp_conv2d(conv2d, X):\\n+    # Here (1, 1) indicates that the batch size and the number of channels\\n+    # are both 1\\n+    X = X.reshape((1, 1) + X.shape)\\n+    Y = conv2d(X)\\n+    # Exclude the first two dimensions that do not interest us: examples and\\n+    # channels\\n+    return Y.reshape(Y.shape[2:])\\n+# Note that here 1 row or column is padded on either side, so a total of 2\\n+# rows or columns are added\\n+conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)\\n+X = torch.rand(size=(8, 8))\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+import tensorflow as tf\\n+\\n+# We define a convenience function to calculate the convolutional layer. This\\n+# function initializes the convolutional layer weights and performs\\n+# corresponding dimensionality elevations and reductions on the input and\\n+# output\\n+def comp_conv2d(conv2d, X):\\n+    # Here (1, 1) indicates that the batch size and the number of channels\\n+    # are both 1\\n+    X = tf.reshape(X, (1, ) + X.shape + (1, ))\\n+    Y = conv2d(X)\\n+    # Exclude the first two dimensions that do not interest us: examples and\\n+    # channels\\n+    return tf.reshape(Y, Y.shape[1:3])\\n+# Note that here 1 row or column is padded on either side, so a total of 2\\n+# rows or columns are added\\n+conv2d = tf.keras.layers.Conv2D(1, kernel_size=3, padding=\\'same\\')\\n+X = tf.random.uniform(shape=(8, 8))\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+当卷积核的高度和宽度不同时，通过设置不同的高度和宽度填充数，可以使输出和输入具有相同的高度和宽度。\\n+\\n+```{.python .input}\\n+# Here, we use a convolution kernel with a height of 5 and a width of 3. The\\n+# padding numbers on either side of the height and width are 2 and 1,\\n+# respectively\\n+conv2d = nn.Conv2D(1, kernel_size=(5, 3), padding=(2, 1))\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+# Here, we use a convolution kernel with a height of 5 and a width of 3. The\\n+# padding numbers on either side of the height and width are 2 and 1,\\n+# respectively\\n+conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+# Here, we use a convolution kernel with a height of 5 and a width of 3. The\\n+# padding numbers on either side of the height and width are 2 and 1,\\n+# respectively\\n+conv2d = tf.keras.layers.Conv2D(1, kernel_size=(5, 3), padding=\\'valid\\')\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+## 跨步\\n+\\n+在计算互相关时，我们从输入张量左上角的卷积窗口开始，然后将其向下和向右滑动到所有位置。在前面的例子中，我们默认一次滑动一个元素。然而，有时，无论是为了计算效率还是因为我们希望减少采样，我们一次移动窗口不止一个元素，跳过中间位置。\\n+\\n+我们将每张幻灯片遍历的行数和列数称为*跨距*。到目前为止，我们对高度和宽度都使用了1的步幅。有时，我们可能需要更大的步幅。:numref:`img_conv_stride`显示了一个二维互相关运算，垂直步长为3，水平步长为2。阴影部分是输出元素以及用于输出计算的输入和核张量元素：$0\\\\times0+0\\\\times1+1\\\\times2+2\\\\times3=8$、$0\\\\times0+6\\\\times1+0\\\\times2+0\\\\times3=6$。我们可以看到，当输出第一列的第二个元素时，卷积窗口向下滑动三行。当输出第一行的第二个元素时，卷积窗口向右滑动两列。当卷积窗口继续在输入上向右滑动两列时，没有输出，因为输入元素无法填充窗口（除非我们添加另一列填充）。\\n+\\n+![Cross-correlation with strides of 3 and 2 for height and width, respectively.](../img/conv-stride.svg)\\n+:label:`img_conv_stride`\\n+\\n+通常，当高度的步幅为$s_h$，宽度为$s_w$时，输出形状为\\n+\\n+$$\\\\lfloor(n_h-k_h+p_h+s_h)/s_h\\\\rfloor \\\\times \\\\lfloor(n_w-k_w+p_w+s_w)/s_w\\\\rfloor.$$\\n+\\n+如果设置$p_h=k_h-1$和$p_w=k_w-1$，则输出形状将简化为$\\\\lfloor(n_h+s_h-1)/s_h\\\\rfloor \\\\times \\\\lfloor(n_w+s_w-1)/s_w\\\\rfloor$。更进一步，如果输入的高度和宽度可以被高度和宽度上的跨距整除，那么输出形状将是$(n_h/s_h) \\\\times (n_w/s_w)$。\\n+\\n+下面，我们将height和width上的跨步设置为2，从而将输入的height和width减半。\\n+\\n+```{.python .input}\\n+conv2d = nn.Conv2D(1, kernel_size=3, padding=1, strides=2)\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+conv2d = tf.keras.layers.Conv2D(1, kernel_size=3, padding=\\'same\\', strides=2)\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+接下来，我们将看一个稍微复杂一些的例子。\\n+\\n+```{.python .input}\\n+conv2d = nn.Conv2D(1, kernel_size=(3, 5), padding=(0, 1), strides=(3, 4))\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+conv2d = tf.keras.layers.Conv2D(1, kernel_size=(3,5), padding=\\'valid\\', strides=(3, 4))\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+为了简洁起见，当输入高度和宽度两边的填充数分别为$p_h$和$p_w$时，我们称之为$(p_h, p_w)$。具体地说，当$p_h = p_w = p$时，填充是$p$。当步幅的高度和宽度分别为$s_h$和$s_w$时，我们称之为$(s_h, s_w)$。具体来说，当$s_h = s_w = s$时，步幅是$s$。默认情况下，填充为0，步幅为1。在实践中，我们很少使用不均匀的步幅或填充，也就是说，我们通常有$p_h = p_w$和$s_h = s_w$。\\n+\\n+## 摘要\\n+\\n+* 填充可以增加输出的高度和宽度。这通常用于使输出与输入具有相同的高度和宽度。\\n+* 跨步可以降低输出的分辨率，例如将输出的高度和宽度减少到输入的高度和宽度的$1/n$（$n$是大于$1$的整数）。\\n+* 填充和跨距可以有效地调整数据的维数。\\n+\\n+## 练习\\n+\\n+1. 对于本节的最后一个例子，使用数学方法计算输出形状，看看它是否与实验结果一致。\\n+1. 在本节的实验中尝试其他填充和步幅组合。\\n+1. 对于音频信号，2的步幅对应什么？\\n+1. 步幅大于1有什么计算上的好处？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/67)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/68)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/272)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-neural-networks/padding-and-strides_tencent.md b/chapter_convolutional-neural-networks/padding-and-strides_tencent.md\\nnew file mode 100644\\nindex 00000000..5e5e5131\\n--- /dev/null\\n+++ b/chapter_convolutional-neural-networks/padding-and-strides_tencent.md\\n@@ -0,0 +1,212 @@\\n+# 填充和跨度\\n+:label:`sec_padding`\\n+\\n+在前面的:numref:`fig_correlation`示例中，我们的输入高度和宽度都是3，而卷积核的高度和宽度都是2，从而产生了维度为$2\\\\times2$的输出表示。正如我们在:numref:`sec_conv_layer`中概括的那样，假设输入形状为$n_h\\\\times n_w$，卷积核形状为$k_h\\\\times k_w$，则输出形状将为$(n_h-k_h+1) \\\\times (n_w-k_w+1)$。因此，卷积层的输出形状由输入的形状和卷积核的形状确定。\\n+\\n+在某些情况下，我们采用了影响输出大小的技术，包括填充和跨步卷积。作为动机，请注意，由于内核的宽度和高度通常大于$1$，因此在应用多次连续卷积之后，我们最终得到的输出往往比输入小得多。如果我们从一幅$240 \\\\times 240$像素的图像开始，$10$层的$5 \\\\times 5$次卷积将图像减少到$200 \\\\times 200$像素，将图像切下$30\\\\$，从而抹去了原始图像边界上任何有趣的信息。\\n+*填充*是处理此问题的最流行工具。\\n+\\n+在其他情况下，我们可能想要大幅降低维数，例如，如果我们发现原始输入分辨率很笨拙。\\n+*步进卷积*是一种流行的技术，可以在这些情况下提供帮助。\\n+\\n+## 填充\\n+\\n+如上所述，应用卷积层时的一个棘手问题是，我们往往会丢失图像周边的像素。由于我们通常使用较小的内核，对于任何给定的卷积，我们可能只丢失几个像素，但当我们应用许多连续的卷积层时，这可能会累积起来。这个问题的一个直接解决方案是在输入图像的边界周围添加额外像素的填充物，从而增加图像的有效大小。通常，我们将额外像素的值设置为零。在:numref:`img_conv_pad`中，我们填充$3 \\\\times 3$个输入，将其大小增加到$5 \\\\times 5$。然后，相应的输出增加到$4 \\\\times 4$矩阵。阴影部分是第一个输出元素以及用于输出计算的输入和核张量元素：$0\\\\times0+0\\\\times1+0\\\\times2+0\\\\times3=0$。\\n+\\n+![Two-dimensional cross-correlation with padding.](../img/conv-pad.svg)\\n+:label:`img_conv_pad`\\n+\\n+通常，如果我们总共添加$p_h$行填充(大约一半在顶部，一半在底部)和总共$p_w$列填充(大约一半在左边，一半在右边)，输出形状将是\\n+\\n+$$(n_h-k_h+p_h+1)\\\\times(n_w-k_w+p_w+1).$$\\n+\\n+这意味着输出的高度和宽度将分别增加$p_h$和$p_w$。\\n+\\n+在许多情况下，我们需要设置$p_h=k_h-1$和$p_w=k_w-1$以使输入和输出具有相同的高度和宽度。这将使构建网络时更容易预测每一层的输出形状。假设这里$k_h$是奇数，我们将在高度两侧填充$p_h/2$行。如果$k_h$是偶数，一种可能是在输入的顶部填充$\\\\lceil p_h/2\\\\rceil$行，在底部填充$\\\\lfloor p_h/2\\\\rfloor$行。我们将用同样的方式填充宽度的两边。\\n+\\n+CNN通常使用高度和宽度值为奇数的卷积核，例如1、3、5或7。选择奇数核大小的好处是，我们可以在填充相同行数和相同行数、相同列数的情况下保留空间维度。\\n+\\n+此外，这种使用奇数内核和填充来精确保留维度的做法提供了文书方面的好处。对于任何二维张量`X`，当核的大小是奇数并且所有边的填充行数和列数相同时，产生与输入具有相同高度和宽度的输出时，我们知道输出`Y[i, j]`是通过输入和卷积核的互相关来计算的，窗口以`X[i, j]`为中心。\\n+\\n+在下面的示例中，我们创建一个高度和宽度为3的二维卷积层，并在所有侧面应用1像素的填充。给定高度和宽度为8的输入，我们发现输出的高度和宽度也是8。\\n+\\n+```{.python .input}\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+# For convenience, we define a function to calculate the convolutional layer.\\n+# This function initializes the convolutional layer weights and performs\\n+# corresponding dimensionality elevations and reductions on the input and\\n+# output\\n+def comp_conv2d(conv2d, X):\\n+    conv2d.initialize()\\n+    # Here (1, 1) indicates that the batch size and the number of channels\\n+    # are both 1\\n+    X = X.reshape((1, 1) + X.shape)\\n+    Y = conv2d(X)\\n+    # Exclude the first two dimensions that do not interest us: examples and\\n+    # channels\\n+    return Y.reshape(Y.shape[2:])\\n+\\n+# Note that here 1 row or column is padded on either side, so a total of 2\\n+# rows or columns are added\\n+conv2d = nn.Conv2D(1, kernel_size=3, padding=1)\\n+X = np.random.uniform(size=(8, 8))\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+import torch\\n+from torch import nn\\n+\\n+# We define a convenience function to calculate the convolutional layer. This\\n+# function initializes the convolutional layer weights and performs\\n+# corresponding dimensionality elevations and reductions on the input and\\n+# output\\n+def comp_conv2d(conv2d, X):\\n+    # Here (1, 1) indicates that the batch size and the number of channels\\n+    # are both 1\\n+    X = X.reshape((1, 1) + X.shape)\\n+    Y = conv2d(X)\\n+    # Exclude the first two dimensions that do not interest us: examples and\\n+    # channels\\n+    return Y.reshape(Y.shape[2:])\\n+# Note that here 1 row or column is padded on either side, so a total of 2\\n+# rows or columns are added\\n+conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)\\n+X = torch.rand(size=(8, 8))\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+import tensorflow as tf\\n+\\n+# We define a convenience function to calculate the convolutional layer. This\\n+# function initializes the convolutional layer weights and performs\\n+# corresponding dimensionality elevations and reductions on the input and\\n+# output\\n+def comp_conv2d(conv2d, X):\\n+    # Here (1, 1) indicates that the batch size and the number of channels\\n+    # are both 1\\n+    X = tf.reshape(X, (1, ) + X.shape + (1, ))\\n+    Y = conv2d(X)\\n+    # Exclude the first two dimensions that do not interest us: examples and\\n+    # channels\\n+    return tf.reshape(Y, Y.shape[1:3])\\n+# Note that here 1 row or column is padded on either side, so a total of 2\\n+# rows or columns are added\\n+conv2d = tf.keras.layers.Conv2D(1, kernel_size=3, padding=\\'same\\')\\n+X = tf.random.uniform(shape=(8, 8))\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+当卷积核的高度和宽度不同时，通过设置不同的高度和宽度填充数，可以使输出和输入具有相同的高度和宽度。\\n+\\n+```{.python .input}\\n+# Here, we use a convolution kernel with a height of 5 and a width of 3. The\\n+# padding numbers on either side of the height and width are 2 and 1,\\n+# respectively\\n+conv2d = nn.Conv2D(1, kernel_size=(5, 3), padding=(2, 1))\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+# Here, we use a convolution kernel with a height of 5 and a width of 3. The\\n+# padding numbers on either side of the height and width are 2 and 1,\\n+# respectively\\n+conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+# Here, we use a convolution kernel with a height of 5 and a width of 3. The\\n+# padding numbers on either side of the height and width are 2 and 1,\\n+# respectively\\n+conv2d = tf.keras.layers.Conv2D(1, kernel_size=(5, 3), padding=\\'valid\\')\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+## 大步走\\n+\\n+在计算互相关时，我们从输入张量左上角的卷积窗口开始，然后将其向下和向右滑动到所有位置。在前面的示例中，我们默认一次滑动一个图元。然而，有时，为了提高计算效率，或者因为我们希望下采样，我们一次移动窗口的元素超过一个，跳过中间位置。\\n+\\n+我们将每张幻灯片遍历的行数和列数称为*跨度*。到目前为止，我们已经对高度和宽度使用了步幅1。有时候，我们可能想用更大的步幅。:numref:`img_conv_stride`表示垂直跨度为3，水平跨度为2的二维互相关运算。阴影部分是输出元素以及用于输出计算的输入和核张量元素：$0\\\\times0+0\\\\times1+1\\\\times2+2\\\\times3=8$、$0\\\\times0+6\\\\times1+0\\\\times2+0\\\\times3=6$。我们可以看到，当输出第一列的第二个元素时，卷积窗口向下滑动三行。输出第一行的第二个元素时，卷积窗口向右滑动两列。当卷积窗口在输入上继续向右滑动两列时，没有输出，因为输入元素不能填满窗口(除非我们添加另一列填充)。\\n+\\n+![Cross-correlation with strides of 3 and 2 for height and width, respectively.](../img/conv-stride.svg)\\n+:label:`img_conv_stride`\\n+\\n+通常，当高度的步幅为$s_h$，宽度的步幅为$s_w$时，输出形状为\\n+\\n+$$\\\\lfloor(n_h-k_h+p_h+s_h)/s_h\\\\rfloor \\\\times \\\\lfloor(n_w-k_w+p_w+s_w)/s_w\\\\rfloor.$$\\n+\\n+如果设置为$p_h=k_h-1$和$p_w=k_w-1$，则输出形状将简化为$\\\\lfloor(n_h+s_h-1)/s_h\\\\rfloor \\\\times \\\\lfloor(n_w+s_w-1)/s_w\\\\rfloor$。更进一步，如果输入高度和宽度可以被高度和宽度上的跨度整除，那么输出形状将是$(n_h/s_h) \\\\times (n_w/s_w)$。\\n+\\n+下面，我们将高度和宽度上的步幅都设置为2，从而将输入高度和宽度减半。\\n+\\n+```{.python .input}\\n+conv2d = nn.Conv2D(1, kernel_size=3, padding=1, strides=2)\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+conv2d = tf.keras.layers.Conv2D(1, kernel_size=3, padding=\\'same\\', strides=2)\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+接下来，我们将看一个稍微复杂一些的示例。\\n+\\n+```{.python .input}\\n+conv2d = nn.Conv2D(1, kernel_size=(3, 5), padding=(0, 1), strides=(3, 4))\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+conv2d = tf.keras.layers.Conv2D(1, kernel_size=(3,5), padding=\\'valid\\', strides=(3, 4))\\n+comp_conv2d(conv2d, X).shape\\n+```\\n+\\n+为简洁起见，当输入高度和宽度两侧的填充数分别为$p_h$和$p_w$时，我们称填充数为$(p_h, p_w)$。具体地说，如果为$p_h = p_w = p$，则填充为$p$。当高度和宽度上的步幅分别为$s_h$和$s_w$时，我们称步幅为$(s_h, s_w)$。具体地说，值为$s_h = s_w = s$时，步幅为$s$。默认情况下，填充为0，跨度为1。实际上，我们很少使用不均匀的跨度或填充，即通常为$p_h = p_w$和$s_h = s_w$。\\n+\\n+## 摘要\\n+\\n+* 填充可以增加输出的高度和宽度。这通常用于使输出具有与输入相同的高度和宽度。\\n+* 步幅可能会降低输出的分辨率，例如，将输出的高度和宽度减少到仅为输入高度和宽度的$1/n$($n$是大于$1$的整数)。\\n+* 填充和跨度可以有效地调整数据的维数。\\n+\\n+## 练习\\n+\\n+1. 对于本节中的最后一个示例，使用数学计算输出形状，以查看其是否与实验结果一致。\\n+1. 在本节的实验中尝试其他填充和跨距组合。\\n+1. 对于音频信号，步幅2对应的是什么？\\n+1. 步幅大于1的计算优势是什么？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/67)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/68)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/272)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-neural-networks/pooling_baidu.md b/chapter_convolutional-neural-networks/pooling_baidu.md\\nnew file mode 100644\\nindex 00000000..d5d3e540\\n--- /dev/null\\n+++ b/chapter_convolutional-neural-networks/pooling_baidu.md\\n@@ -0,0 +1,233 @@\\n+# 联营\\n+:label:`sec_pooling`\\n+\\n+通常，当我们处理图像时，我们希望逐渐降低隐藏表示的空间分辨率，聚集信息，这样我们在网络中的位置越高，每个隐藏节点对其敏感的接收区域（输入端）就越大。\\n+\\n+我们的最终任务通常会问一些关于图像的全局性问题，例如，*它是否包含一只猫？*所以通常我们最后一层的单元应该对整个输入敏感。通过逐渐聚合信息，生成越来越粗的映射，我们最终实现了最终学习全局表示的目标，同时在处理的中间层保留卷积层的所有优点。\\n+\\n+此外，当检测较低层次的特征时，例如边缘（如:numref:`sec_conv_layer`中所讨论的），我们通常希望我们的表示在某种程度上对平移保持不变。例如，如果我们拍摄黑白之间轮廓清晰的图像`X`，并将整个图像向右移动一个像素，即`Z[i, j] = X[i, j + 1]`，则新图像`Z`的输出可能大不相同。边缘将移动一个像素。几乎不可能发生在同一个地方。事实上，即使是三脚架和一个静止的物体，由于快门的移动而引起的相机振动可能会使所有物体移动一个像素左右（高端相机配备了特殊功能来解决这个问题）。\\n+\\n+本节介绍了*池层*，它具有降低卷积层对位置和空间上下采样表示的敏感性的双重目的。\\n+\\n+## 最大池和平均池\\n+\\n+与卷积层一样，*pooling*操作符由一个固定形状的窗口组成，该窗口根据其跨距在输入的所有区域上滑动，为固定形状窗口（有时称为*池窗口*）遍历的每个位置计算一个输出。然而，与卷积层中输入和核的互相关计算不同，池层不包含参数（没有*kernel*）。相反，池运算符是确定性的，通常计算池窗口中元素的最大值或平均值。这些操作分别称为*最大池*和*平均池*。\\n+\\n+在这两种情况下，与互相关运算符一样，我们可以将池窗口视为从输入张量的左上角开始，从左到右和从上到下在输入张量之间滑动。在池窗口到达的每个位置，它计算窗口中输入子传感器的最大值或平均值，具体取决于使用的是最大值还是平均值。\\n+\\n+![Maximum pooling with a pooling window shape of $2\\\\times 2$. The shaded portions are the first output element as well as the input tensor elements used for the output computation: $\\\\max(0, 1, 3, 4)=4$.](../img/pooling.svg)\\n+:label:`fig_pooling`\\n+\\n+:numref:`fig_pooling`中的输出张量的高度为2，宽度为2。这四个元素来自每个池窗口中的最大值：\\n+\\n+$$\\n+\\\\max(0, 1, 3, 4)=4,\\\\\\\\\\n+\\\\max(1, 2, 4, 5)=5,\\\\\\\\\\n+\\\\max(3, 4, 6, 7)=7,\\\\\\\\\\n+\\\\max(4, 5, 7, 8)=8.\\\\\\\\\\n+$$\\n+\\n+池窗口形状为$p \\\\times q$的池层称为$p \\\\times q$池层。池操作称为$p \\\\times q$池。\\n+\\n+让我们回到本节开头提到的对象边缘检测示例。现在我们将使用卷积层的输出作为$2\\\\times 2$最大池的输入。设置卷积层输入为`X`，池层输出为`Y`。无论`X[i, j]`和`X[i, j + 1]`的值是否不同，或`X[i, j + 1]`和`X[i, j + 2]`的值是否不同，池层始终输出`Y[i, j] = 1`。也就是说，使用$2\\\\times 2$最大池层，我们仍然可以检测到卷积层识别的模式在高度或宽度上是否移动不超过一个元素。\\n+\\n+在下面的代码中，我们在`pool2d`函数中实现了池层的前向传播。此功能类似于:numref:`sec_conv_layer`中的`corr2d`功能。然而，这里我们没有内核，将输出计算为输入中每个区域的最大值或平均值。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+```\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+def pool2d(X, pool_size, mode=\\'max\\'):\\n+    p_h, p_w = pool_size\\n+    Y = d2l.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\\n+    for i in range(Y.shape[0]):\\n+        for j in range(Y.shape[1]):\\n+            if mode == \\'max\\':\\n+                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\\n+            elif mode == \\'avg\\':\\n+                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\\n+    return Y\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+import tensorflow as tf\\n+\\n+def pool2d(X, pool_size, mode=\\'max\\'):\\n+    p_h, p_w = pool_size\\n+    Y = tf.Variable(tf.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w +1)))\\n+    for i in range(Y.shape[0]):\\n+        for j in range(Y.shape[1]):\\n+            if mode == \\'max\\':\\n+                Y[i, j].assign(tf.reduce_max(X[i: i + p_h, j: j + p_w]))\\n+            elif mode ==\\'avg\\':\\n+                Y[i, j].assign(tf.reduce_mean(X[i: i + p_h, j: j + p_w]))\\n+    return Y\\n+```\\n+\\n+我们可以在:numref:`fig_pooling`中构造输入张量`X`来验证二维最大池层的输出。\\n+\\n+```{.python .input}\\n+#@tab all\\n+X = d2l.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\\n+pool2d(X, (2, 2))\\n+```\\n+\\n+另外，我们对平均池层进行了实验。\\n+\\n+```{.python .input}\\n+#@tab all\\n+pool2d(X, (2, 2), \\'avg\\')\\n+```\\n+\\n+## 垫步和跨步\\n+\\n+与卷积层一样，合并层也可以改变输出形状。和以前一样，我们可以通过填充输入和调整步幅来改变操作以获得所需的输出形状。我们可以通过deep learning框架中内置的二维最大池层来演示池层中填充和跨步的使用。我们首先构造了一个输入张量`X`，它的形状有四个维度，其中例子数和通道数都是1。\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+X = d2l.reshape(d2l.arange(16, dtype=d2l.float32), (1, 1, 4, 4))\\n+X\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = d2l.reshape(d2l.arange(16, dtype=d2l.float32), (1, 4, 4, 1))\\n+X\\n+```\\n+\\n+默认情况下，在同一个窗体中构建了同一个窗体和窗体池。下面，我们使用一个形状为`(3, 3)`的合用窗口，因此默认情况下步幅形状为`(3, 3)`。\\n+\\n+```{.python .input}\\n+pool2d = nn.MaxPool2D(3)\\n+# Because there are no model parameters in the pooling layer, we do not need\\n+# to call the parameter initialization function\\n+pool2d(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+pool2d = nn.MaxPool2d(3)\\n+pool2d(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+pool2d = tf.keras.layers.MaxPool2D(pool_size=[3, 3])\\n+pool2d(X)\\n+```\\n+\\n+可以手动指定步幅和填充。\\n+\\n+```{.python .input}\\n+pool2d = nn.MaxPool2D(3, padding=1, strides=2)\\n+pool2d(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+pool2d = nn.MaxPool2d(3, padding=1, stride=2)\\n+pool2d(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+pool2d = tf.keras.layers.MaxPool2D(pool_size=[3, 3], padding=\\'same\\',\\n+                                   strides=2)\\n+pool2d(X)\\n+```\\n+\\n+当然，我们可以指定任意的矩形池窗口，并分别指定高度和宽度的填充和跨距。\\n+\\n+```{.python .input}\\n+pool2d = nn.MaxPool2D((2, 3), padding=(1, 2), strides=(2, 3))\\n+pool2d(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+pool2d = nn.MaxPool2d((2, 3), padding=(1, 1), stride=(2, 3))\\n+pool2d(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+pool2d = tf.keras.layers.MaxPool2D(pool_size=[2, 3], padding=\\'same\\',\\n+                                   strides=(2, 3))\\n+pool2d(X)\\n+```\\n+\\n+## 多通道\\n+\\n+当处理多信道输入数据时，池层将每个输入信道单独地汇集在一起，而不是像在卷积层中那样将信道上的输入相加。这意味着池层的输出通道数与输入通道数相同。下面，我们将在通道维度上连接张量`X`和`X + 1`，以构造具有2个通道的输入。\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+X = d2l.concat((X, X + 1), 1)\\n+X\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.reshape(tf.stack([X, X+1], 0), (1, 2, 4, 4))\\n+```\\n+\\n+如我们所见，共用后输出通道的数量仍然是2个。\\n+\\n+```{.python .input}\\n+pool2d = nn.MaxPool2D(3, padding=1, strides=2)\\n+pool2d(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+pool2d = nn.MaxPool2d(3, padding=1, stride=2)\\n+pool2d(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+pool2d = tf.keras.layers.MaxPool2D(3, padding=\\'same\\', strides=2)\\n+pool2d(X)\\n+```\\n+\\n+## 摘要\\n+\\n+* 以池窗口中的输入元素为输入元素，最大池操作将最大值指定为输出，平均池操作将平均值指定为输出。\\n+* 池层的主要好处之一是减轻卷积层对位置的过度敏感。\\n+* 我们可以指定池层的填充和跨距。\\n+* 最大池，结合一个大于1的步幅可以用来减少空间维度（例如，宽度和高度）。\\n+* 池层的输出通道数与输入通道数相同。\\n+\\n+## 练习\\n+\\n+1. 你能把平均池作为卷积层的一个特例来实现吗？如果是这样，那就去做。\\n+1. 你能把最大池作为卷积层的一个特例来实现吗？如果是这样，那就去做。\\n+1. 池层的计算成本是多少？假设池层的输入大小为$c\\\\times h\\\\times w$，池窗口的形状为$p_h\\\\times p_w$，填充为$(p_h, p_w)$，跨距为$(s_h, s_w)$。\\n+1. 为什么您期望最大池和平均池的工作方式不同？\\n+1. 我们需要一个单独的最小池层吗？你能用另一个手术代替它吗？\\n+1. 在平均池和最大池之间还有其他操作可以考虑吗（提示：回忆一下softmax）？为什么它不那么受欢迎？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/71)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/72)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/274)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-neural-networks/pooling_tencent.md b/chapter_convolutional-neural-networks/pooling_tencent.md\\nnew file mode 100644\\nindex 00000000..94cb8ce2\\n--- /dev/null\\n+++ b/chapter_convolutional-neural-networks/pooling_tencent.md\\n@@ -0,0 +1,233 @@\\n+# 池化\\n+:label:`sec_pooling`\\n+\\n+通常，当我们处理图像时，我们希望逐渐降低隐藏表示的空间分辨率，聚合信息，以便我们在网络中位置越高，每个隐藏节点对其敏感的接受域(在输入中)就越大。\\n+\\n+我们的最终任务通常会询问一些关于图像的全局问题，例如，*它是否包含一只猫？*所以通常情况下，我们最后一层的单位应该对整个输入敏感。通过逐步聚合信息，产生越来越粗糙的地图，我们实现了最终学习全局表示的目标，同时在处理的中间层保持了卷积层的所有优势。\\n+\\n+此外，当检测较低级别的特征时，例如边(如:numref:`sec_conv_layer`中所讨论的)，我们通常希望我们的表示在一定程度上不受转换的影响。例如，如果我们取具有在黑白之间清晰描绘的图像`X`，并且将整个图像向右移动一个像素，即`Z[i, j] = X[i, j + 1]`，则新图像`Z`的输出可能有很大的不同。边缘将偏移一个像素。在现实中，物体几乎不会完全出现在同一地点。事实上，即使是三脚架和静止物体，快门移动引起的相机振动也可能会使一切移动一个像素左右(高端相机都有特殊功能来解决这个问题)。\\n+\\n+本节介绍“合并图层”，它具有降低卷积图层对位置和空间下采样表示的敏感性的双重目的。\\n+\\n+## 最大池化和平均池化\\n+\\n+与卷积层类似，*Pooling*运算符由固定形状窗口组成，该窗口根据其步幅在输入中的所有区域上滑动，为固定形状窗口(有时称为*Pooling窗口*)遍历的每个位置计算单个输出。然而，与卷积层中输入和核的互相关计算不同，池化层不包含参数(没有*核*)。相反，池运算符是确定性的，通常计算池化窗口中元素的最大值或平均值。这些操作分别称为“最大池化”(简称“最大池化”)和“平均池化”。\\n+\\n+在这两种情况下，就像使用互相关运算符一样，我们可以认为合并窗口从输入张量的左上角开始，并从左到右和从上到下在输入张量上滑动。在合并窗口命中的每个位置，它根据使用的是最大还是平均合并来计算窗口中的输入子张量的最大值或平均值。\\n+\\n+![Maximum pooling with a pooling window shape of $2\\\\times 2$. The shaded portions are the first output element as well as the input tensor elements used for the output computation: $\\\\max(0, 1, 3, 4)=4$.](../img/pooling.svg)\\n+:label:`fig_pooling`\\n+\\n+:numref:`fig_pooling`中的输出张量的高度为2，宽度为2。这四个元素从每个池窗口中的最大值导出：\\n+\\n+$$\\n+\\\\max(0, 1, 3, 4)=4,\\\\\\\\\\n+\\\\max(1, 2, 4, 5)=5,\\\\\\\\\\n+\\\\max(3, 4, 6, 7)=7,\\\\\\\\\\n+\\\\max(4, 5, 7, 8)=8.\\\\\\\\\\n+$$\\n+\\n+具有$p \\\\times q$的合用窗口形状的合用图层称为$p \\\\times q$合用图层。池化操作称为$p \\\\times q$池化。\\n+\\n+让我们回到本节开头提到的对象边缘检测示例。现在，我们将使用卷积层的输出作为$2\\\\times 2$最大池化的输入。将卷积层输入设置为`X`，将池层输出设置为`Y`。无论是`X[i, j]`和`X[i, j + 1]`的值不同，还是`X[i, j + 1]`和`X[i, j + 2]`的值不同，合并层始终输出`Y[i, j] = 1`。也就是说，使用$2\\\\times 2$的最大汇聚层，我们仍然可以检测由卷积层识别的图案是否在高度或宽度上移动不超过一个元素。\\n+\\n+在下面的代码中，我们实现了`pool2d`函数中池层的正向传播。此函数类似于:numref:`sec_conv_layer`中的`corr2d`函数。但是，这里没有内核，将输出计算为输入中每个区域的最大值或平均值。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+```\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+def pool2d(X, pool_size, mode=\\'max\\'):\\n+    p_h, p_w = pool_size\\n+    Y = d2l.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\\n+    for i in range(Y.shape[0]):\\n+        for j in range(Y.shape[1]):\\n+            if mode == \\'max\\':\\n+                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\\n+            elif mode == \\'avg\\':\\n+                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\\n+    return Y\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+import tensorflow as tf\\n+\\n+def pool2d(X, pool_size, mode=\\'max\\'):\\n+    p_h, p_w = pool_size\\n+    Y = tf.Variable(tf.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w +1)))\\n+    for i in range(Y.shape[0]):\\n+        for j in range(Y.shape[1]):\\n+            if mode == \\'max\\':\\n+                Y[i, j].assign(tf.reduce_max(X[i: i + p_h, j: j + p_w]))\\n+            elif mode ==\\'avg\\':\\n+                Y[i, j].assign(tf.reduce_mean(X[i: i + p_h, j: j + p_w]))\\n+    return Y\\n+```\\n+\\n+我们可以在:numref:`fig_pooling`中构造输入张量:numref:`fig_pooling`以验证二维最大汇聚层的输出。\\n+\\n+```{.python .input}\\n+#@tab all\\n+X = d2l.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\\n+pool2d(X, (2, 2))\\n+```\\n+\\n+此外，我们还使用平均池层进行了实验。\\n+\\n+```{.python .input}\\n+#@tab all\\n+pool2d(X, (2, 2), \\'avg\\')\\n+```\\n+\\n+## 填充和跨度\\n+\\n+与卷积图层一样，合并图层也可以更改输出形状。和前面一样，我们可以通过填充输入和调整步幅来改变操作以获得所需的输出形状。我们可以通过深度学习框架中内置的二维最大汇聚层来演示填充和跨度在汇聚层中的使用。我们首先构造其形状具有四个维度的输入张量`X`，其中示例数量和通道数量都是1。\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+X = d2l.reshape(d2l.arange(16, dtype=d2l.float32), (1, 1, 4, 4))\\n+X\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = d2l.reshape(d2l.arange(16, dtype=d2l.float32), (1, 4, 4, 1))\\n+X\\n+```\\n+\\n+默认情况下，来自框架内置类的实例中的Stride和Pooling窗口具有相同的形状。下面，我们使用形状为`(3, 3)`的合并窗口，因此默认情况下我们得到的步幅形状为`(3, 3)`。\\n+\\n+```{.python .input}\\n+pool2d = nn.MaxPool2D(3)\\n+# Because there are no model parameters in the pooling layer, we do not need\\n+# to call the parameter initialization function\\n+pool2d(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+pool2d = nn.MaxPool2d(3)\\n+pool2d(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+pool2d = tf.keras.layers.MaxPool2D(pool_size=[3, 3])\\n+pool2d(X)\\n+```\\n+\\n+步距和填充可以手动指定。\\n+\\n+```{.python .input}\\n+pool2d = nn.MaxPool2D(3, padding=1, strides=2)\\n+pool2d(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+pool2d = nn.MaxPool2d(3, padding=1, stride=2)\\n+pool2d(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+pool2d = tf.keras.layers.MaxPool2D(pool_size=[3, 3], padding=\\'same\\',\\n+                                   strides=2)\\n+pool2d(X)\\n+```\\n+\\n+当然，我们可以指定一个任意的矩形池窗口，并分别指定高度和宽度的填充和跨度。\\n+\\n+```{.python .input}\\n+pool2d = nn.MaxPool2D((2, 3), padding=(1, 2), strides=(2, 3))\\n+pool2d(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+pool2d = nn.MaxPool2d((2, 3), padding=(1, 1), stride=(2, 3))\\n+pool2d(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+pool2d = tf.keras.layers.MaxPool2D(pool_size=[2, 3], padding=\\'same\\',\\n+                                   strides=(2, 3))\\n+pool2d(X)\\n+```\\n+\\n+## 多渠道\\n+\\n+在处理多通道输入数据时，汇聚层分别将每个输入通道汇集在一起，而不是像在卷积层中那样将通道上的输入相加。这意味着用于池化层的输出通道的数量与输入通道的数量相同。下面，我们将连接通道维度上的张量`X`和`X + 1`，以构建具有2个通道的输入。\\n+\\n+```{.python .input}\\n+#@tab mxnet, pytorch\\n+X = d2l.concat((X, X + 1), 1)\\n+X\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.reshape(tf.stack([X, X+1], 0), (1, 2, 4, 4))\\n+```\\n+\\n+如我们所见，合并后的输出通道数仍为2个。\\n+\\n+```{.python .input}\\n+pool2d = nn.MaxPool2D(3, padding=1, strides=2)\\n+pool2d(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+pool2d = nn.MaxPool2d(3, padding=1, stride=2)\\n+pool2d(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+pool2d = tf.keras.layers.MaxPool2D(3, padding=\\'same\\', strides=2)\\n+pool2d(X)\\n+```\\n+\\n+## 摘要\\n+\\n+* 对于池化窗口中的输入元素，最大池化操作将最大值指定为输出，而平均池化操作将平均值指定为输出。\\n+* 汇集层的主要优点之一是减轻卷积层对位置的过度敏感性。\\n+* 我们可以指定池化层的填充和跨度。\\n+* 最大合并与大于1的跨度相结合可用于减小空间维度(例如，宽度和高度)。\\n+* 池层的输出通道数与输入通道数相同。\\n+\\n+## 练习\\n+\\n+1. 您能否将平均池作为卷积层的特例来实现？如果是这样，那就去做吧。\\n+1. 您能实现最大池作为卷积层的特例吗？如果是这样，那就去做吧。\\n+1. 池化层的计算成本是多少？假设汇聚层的输入大小为$c\\\\times h\\\\times w$，则汇聚窗口的形状为$p_h\\\\times p_w$，填充为$(p_h, p_w)$，跨度为$(s_h, s_w)$。\\n+1. 为什么您期望最大池化和平均池化的工作方式不同？\\n+1. 我们是否需要单独的最小池层？你能换成另一台手术吗？\\n+1. 在平均池和最大池之间是否有其他您可以考虑的操作(提示：回想一下Softmax)？为什么它可能不那么受欢迎呢？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/71)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/72)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/274)\\n+:end_tab:\\ndiff --git a/chapter_convolutional-neural-networks/why-conv_baidu.md b/chapter_convolutional-neural-networks/why-conv_baidu.md\\nnew file mode 100644\\nindex 00000000..bd9f8ec0\\n--- /dev/null\\n+++ b/chapter_convolutional-neural-networks/why-conv_baidu.md\\n@@ -0,0 +1,115 @@\\n+# 从完全连接的层到卷积\\n+:label:`sec_why-conv`\\n+\\n+时至今日，当我们处理表格数据时，我们迄今为止讨论的模型仍然是适当的选择。通过表格，我们的意思是数据由对应于示例的行和对应于特征的列组成。对于表格数据，我们可以预期我们所寻求的模式可能涉及到特征之间的交互，但是我们没有假设任何关于特征如何交互的结构。\\n+\\n+有时候，我们真的缺乏知识来指导建造更精巧的建筑。在这种情况下，MLP可能是我们能做的最好的。然而，对于高维感知数据，这种无结构的网络可能会变得笨拙。\\n+\\n+例如，让我们回到区分猫和狗的例子。假设我们在数据收集方面做了彻底的工作，收集了一个一百万像素照片的注释数据集。这意味着网络的每个输入都有一百万个维度。即使是积极地减少到一千个隐藏维度，也需要一个以$10^6 \\\\times 10^3 = 10^9$个参数为特征的完全连接层。除非我们有大量的gpu，一个分布式优化的天才，以及非常多的耐心，否则学习这个网络的参数可能是不可行的。\\n+\\n+细心的读者可能会反对这一论点，理由是可能不需要一百万像素的分辨率。然而，虽然我们可能可以逃脱十万像素，我们的隐藏层大小1000严重低估了隐藏单元的数量，它需要学习良好的图像表现，所以一个实际的系统仍然需要数十亿个参数。此外，通过拟合这么多参数来学习分类器可能需要收集大量的数据集。然而今天，人类和计算机都能很好地区分猫和狗，似乎与这些直觉相矛盾。这是因为图像具有丰富的结构，可以被人类和机器学习模型利用。卷积神经网络（CNNs）是一种创造性的机器学习方法，它利用自然图像中的某些已知结构。\\n+\\n+## 不变性\\n+\\n+假设您想检测图像中的对象。不管我们用什么方法来识别物体，都不应该过分关注物体在图像中的精确位置，这似乎是合理的。理想情况下，我们的系统应该利用这些知识。猪通常不会飞，飞机通常不会游泳。尽管如此，我们仍然应该认识到一头猪出现在图像的顶部。我们可以从儿童游戏《瓦尔多在哪里》（:numref:`img_waldo`中描述）中获得一些灵感。这个游戏由一系列充满活动的混乱场景组成。瓦尔多出现在每一个地方，通常潜伏在一些不太可能的地方。读者的目标是找到他。尽管他的服装很有特色，但这可能是令人惊讶的困难，因为大量的分心。然而，*瓦尔多的样子*并不取决于*瓦尔多在哪里*。我们可以用Waldo检测器扫描图像，它可以给每个补丁分配一个分数，表明该补丁包含Waldo的可能性。CNNs系统化了空间不变性的概念，利用它来学习用较少参数表示的有用表示。\\n+\\n+![An image of the \"Where\\'s Waldo\" game.](../img/where-wally-walker-books.jpg)\\n+:width:`400px`\\n+:label:`img_waldo`\\n+\\n+现在，我们可以通过列举一些需要帮助我们设计适用于计算机视觉的神经网络体系结构，使这些直觉更加具体：\\n+\\n+1. 在最早的层中，我们的网络应该对相同的补丁做出类似的响应，而不管它出现在图像中的哪个位置。这个原理叫做“平移不变性”。\\n+1. 网络最早的层次应该集中在局部区域，而不考虑遥远地区的图像内容。这就是“局部性”原则。最终，这些局部表示可以聚合起来，在整个图像级别上做出预测。\\n+\\n+让我们看看这是如何转化为数学的。\\n+\\n+## 限制MLP\\n+\\n+首先，我们可以考虑以二维图像$\\\\mathbf{X}$作为输入，其直接隐藏表示$\\\\mathbf{H}$在数学上类似地表示为矩阵，在代码中表示为二维张量，其中$\\\\mathbf{X}$和$\\\\mathbf{H}$具有相同的形状。让它沉下去。我们现在不仅认为输入，而且还认为隐藏的表示具有空间结构。\\n+\\n+让$[\\\\mathbf{X}]_{i, j}$和$[\\\\mathbf{H}]_{i, j}$分别表示输入图像和隐藏表示中的位置（$i$、$j$）处的像素。因此，为了让每个隐藏单元接收来自每个输入像素的输入，我们将从使用权重矩阵（如我们先前在MLPs中所做的那样）切换到将我们的参数表示为四阶权重张量$\\\\mathsf{W}$。假设$\\\\mathbf{U}$包含偏差，我们可以将全连通层正式表示为\\n+\\n+$$\\\\begin{aligned} \\\\left[\\\\mathbf{H}\\\\right]_{i, j} &= [\\\\mathbf{U}]_{i, j} + \\\\sum_k \\\\sum_l[\\\\mathsf{W}]_{i, j, k, l}  [\\\\mathbf{X}]_{k, l}\\\\\\\\ &=  [\\\\mathbf{U}]_{i, j} +\\n+\\\\sum_a \\\\sum_b [\\\\mathsf{V}]_{i, j, a, b}  [\\\\mathbf{X}]_{i+a, j+b}.\\\\end{aligned},$$\\n+\\n+其中，从$\\\\mathsf{W}$到$\\\\mathsf{V}$的转换现在完全是化妆品，因为在两个四阶张量中，系数之间存在一对一的对应关系。我们只需重新索引下标$(k, l)$，使$k = i+a$和$l = j+b$。换句话说，我们设置了$[\\\\mathsf{V}]_{i, j, a, b} = [\\\\mathsf{W}]_{i, j, i+a, j+b}$。索引$a$和$b$覆盖了正偏移和负偏移，覆盖了整个图像。对于隐藏表示$[\\\\mathbf{H}]_{i, j}$中的任何给定位置（$i$、$j$），我们通过对$x$中以$(i, j)$为中心并按$[\\\\mathsf{V}]_{i, j, a, b}$加权的像素求和来计算其值。\\n+\\n+### 平移不变性\\n+\\n+现在让我们引用上面建立的第一个原则：翻译不变性。这意味着输入$\\\\mathbf{X}$中的移位应该仅仅导致隐藏表示$\\\\mathbf{H}$中的移位。只有当$\\\\mathsf{V}$和$\\\\mathbf{U}$实际上不依赖于$(i, j)$时，这才有可能，也就是说，我们有$[\\\\mathsf{V}]_{i, j, a, b} = [\\\\mathbf{V}]_{a, b}$，$\\\\mathbf{U}$是一个常数，比如$u$。因此，我们可以简化$\\\\mathbf{H}$的定义：\\n+\\n+$$[\\\\mathbf{H}]_{i, j} = u + \\\\sum_a\\\\sum_b [\\\\mathbf{V}]_{a, b}  [\\\\mathbf{X}]_{i+a, j+b}.$$\\n+\\n+这是个卷积！我们使用系数$(i+a, j+b)$有效地加权位置$(i, j)$附近的像素以获得值$[\\\\mathbf{H}]_{i, j}$。注意，$[\\\\mathbf{V}]_{a, b}$需要的系数比$[\\\\mathsf{V}]_{i, j, a, b}$少很多，因为它不再依赖于图像中的位置。我们取得了重大进展！\\n+\\n+###  地点\\n+\\n+现在让我们引用第二个原则：局部性。如上所述，我们认为，为了收集相关信息，以评估$[\\\\mathbf{H}]_{i, j}$发生的情况，我们不应将目光投向离$(i, j)$很远的地方。这意味着在$|a|> \\\\Delta$或$|b| > \\\\Delta$的范围之外，我们应该设置$[\\\\mathbf{V}]_{a, b} = 0$。等效地，我们可以将$[\\\\mathbf{H}]_{i, j}$重写为\\n+\\n+$$[\\\\mathbf{H}]_{i, j} = u + \\\\sum_{a = -\\\\Delta}^{\\\\Delta} \\\\sum_{b = -\\\\Delta}^{\\\\Delta} [\\\\mathbf{V}]_{a, b}  [\\\\mathbf{X}]_{i+a, j+b}.$$\\n+:eqlabel:`eq_conv-layer`\\n+\\n+注意，:eqref:`eq_conv-layer`，简而言之，是一个*卷积层*。\\n+*卷积神经网络*（CNNs）\\n+是包含卷积层的神经网络的一个特殊家族。在深度学习研究社区中，$\\\\mathbf{V}$被称为卷积核*、过滤器*，或者简单地说，层的*权重*通常是可学习的参数。当局部区域很小时，与完全连接的网络相比，差异可能是巨大的。以前，我们可能需要数十亿个参数来表示图像处理网络中的一个层，而现在通常只需要几百个参数，而不改变输入或隐藏表示的维数。参数的急剧减少所付出的代价是，我们的特征现在是平移不变的，并且我们的层在确定每个隐藏激活的值时只能包含局部信息。所有的学习都依赖于施加归纳偏差。当这种偏差与实际情况相符时，我们得到了有效的样本模型，这些模型能很好地推广到不可见的数据中。但当然，如果这些偏差与实际情况不符，例如，如果图像不是平移不变的，我们的模型甚至可能难以适应我们的训练数据。\\n+\\n+## 卷积\\n+\\n+在进一步讨论之前，我们应该简要回顾一下为什么上面的操作被称为卷积。在数学中，两个函数（比如$f, g: \\\\mathbb{R}^d \\\\to \\\\mathbb{R}$）之间的卷积被定义为\\n+\\n+$$(f * g)(\\\\mathbf{x}) = \\\\int f(\\\\mathbf{z}) g(\\\\mathbf{x}-\\\\mathbf{z}) d\\\\mathbf{z}.$$\\n+\\n+也就是说，当一个函数被“翻转”并移位$\\\\mathbf{x}$时，我们测量$f$和$g$之间的重叠。当我们有离散对象时，积分就变成和。例如，对于索引超过$\\\\mathbb{Z}$的可平方和无限维向量集合中的向量，我们得到以下定义：\\n+\\n+$$(f * g)(i) = \\\\sum_a f(a) g(i-a).$$\\n+\\n+对于二维张量，对于$f$和$g$，我们分别具有$(a, b)$和$(i-a, j-b)$的对应和：\\n+\\n+$$(f * g)(i, j) = \\\\sum_a\\\\sum_b f(a, b) g(i-a, j-b).$$\\n+:eqlabel:`eq_2d-conv-discrete`\\n+\\n+这看起来与:eqref:`eq_conv-layer`相似，但有一个主要区别。我们不是使用$(i+a, j+b)$，而是使用差异。但是请注意，这种区别主要是表面上的，因为我们总是可以匹配:eqref:`eq_conv-layer`和:eqref:`eq_2d-conv-discrete`之间的符号。我们在:eqref:`eq_conv-layer`中的原始定义更恰当地描述了*互相关*。我们将在下一节讨论这个问题。\\n+\\n+## “瓦尔多在哪儿”又被重温了一遍\\n+\\n+回到瓦尔多探测器，让我们看看这是什么样子。卷积层根据滤波器$\\\\mathsf{V}$拾取给定尺寸的窗口并加权强度，如:numref:`fig_waldo_mask`中所示。我们的目标可能是学习一个模型，以便在“waldoness”最高的地方，我们应该在隐藏层表示中找到一个峰值。\\n+\\n+![Detect Waldo.](../img/waldo-mask.jpg)\\n+:width:`400px`\\n+:label:`fig_waldo_mask`\\n+\\n+### 渠道\\n+:label:`subsec_why-conv-channels`\\n+\\n+这种方法只有一个问题。到目前为止，我们很高兴地忽略了图像由3个通道组成：红色、绿色和蓝色。实际上，图像不是二维对象，而是三阶张量，其特征是高度、宽度和通道，例如，具有$1024 \\\\times 1024 \\\\times 3$像素的形状。虽然这些轴的前两个与空间关系有关，但第三个轴可被视为为为每个像素位置分配多维表示。因此，我们将$\\\\mathsf{X}$索引为$[\\\\mathsf{X}]_{i, j, k}$。卷积滤波器必须相应地进行调整。而不是$[\\\\mathbf{V}]_{a,b}$，我们现在有$[\\\\mathsf{V}]_{a,b,c}$。\\n+\\n+此外，正如我们的输入是由三阶张量组成的，将我们的隐藏表示类似地表示为三阶张量$\\\\mathsf{H}$是一个好主意。换言之，我们需要一个与每个空间位置对应的隐藏表示的完整向量，而不仅仅是与每个空间位置对应的隐藏表示。我们可以把隐藏的表示看作是由一系列相互叠加的二维网格组成的。在输入中，这些有时称为*通道*。它们有时也被称为*特征映射*，因为每一个都向后续层提供一组空间化的学习特征。直观地说，您可以想象，在靠近输入的较低层，一些通道可以专门识别边，而其他通道可以识别纹理。\\n+\\n+为了支持输入（$\\\\mathsf{X}$）和隐藏表示（$\\\\mathsf{H}$）中的多个通道，我们可以在$\\\\mathsf{V}$:$[\\\\mathsf{V}]_{a, b, c, d}$中添加第四个坐标。把我们所有的东西放在一起：\\n+\\n+$$[\\\\mathsf{H}]_{i,j,d} = \\\\sum_{a = -\\\\Delta}^{\\\\Delta} \\\\sum_{b = -\\\\Delta}^{\\\\Delta} \\\\sum_c [\\\\mathsf{V}]_{a, b, c, d} [\\\\mathsf{X}]_{i+a, j+b, c},$$\\n+:eqlabel:`eq_conv-layer-channels`\\n+\\n+其中$d$索引隐藏表示$\\\\mathsf{H}$中的输出信道。随后的卷积层将继续以三阶张量$\\\\mathsf{H}$作为输入。更一般地说，:eqref:`eq_conv-layer-channels`是用于多个信道的卷积层的定义，其中$\\\\mathsf{V}$是该层的内核或滤波器。\\n+\\n+我们仍有许多行动需要解决。例如，我们需要弄清楚如何将所有隐藏的表示组合到一个输出中，例如，图像中是否存在Waldo*anywhere*。我们还需要决定如何有效地计算事物，如何组合多个层次，适当的激活函数，以及如何做出合理的设计选择，以产生实际有效的网络。我们将在本章的其余部分讨论这些问题。\\n+\\n+## 摘要\\n+\\n+* 图像的平移不变性意味着图像的所有面片都将以相同的方式进行处理。\\n+* 局部性意味着只有一小部分像素邻域将用于计算相应的隐藏表示。\\n+* 在图像处理中，卷积层通常比完全连接的层需要更少的参数。\\n+* cnn是一类特殊的神经网络，包含卷积层。\\n+* 输入和输出通道允许我们的模型在每个空间位置捕捉图像的多个方面。\\n+\\n+## 练习\\n+\\n+1. 假设卷积核的大小是$\\\\Delta = 0$。说明在这种情况下，卷积内核为每组信道独立地实现了MLP。\\n+1. 为什么翻译不变性不是一个好主意呢？\\n+1. 在决定如何处理与图像边界像素位置对应的隐藏表示时，我们必须处理哪些问题？\\n+1. 描述一个类似的音频卷积层。\\n+1. 你认为卷积层也适用于文本数据吗？为什么或者为什么不呢？\\n+1. 证明$f * g = g * f$。\\n+\\n+[Discussions](https://discuss.d2l.ai/t/64)\\ndiff --git a/chapter_convolutional-neural-networks/why-conv_tencent.md b/chapter_convolutional-neural-networks/why-conv_tencent.md\\nnew file mode 100644\\nindex 00000000..959bb622\\n--- /dev/null\\n+++ b/chapter_convolutional-neural-networks/why-conv_tencent.md\\n@@ -0,0 +1,115 @@\\n+# 从全连通层到卷积\\n+:label:`sec_why-conv`\\n+\\n+直到今天，当我们处理表格数据时，我们到目前为止已经讨论过的模型仍然是合适的选择。通过表格，我们的意思是数据由对应于示例的行和对应于特性的列组成。对于表格数据，我们可能预计我们寻找的模式可能涉及特征之间的交互，但我们不假定关于特征如何交互的任何结构*先验*。\\n+\\n+有时候，我们确实缺乏知识来指导更巧妙的架构的构建。在这些情况下，MLP可能是我们能做的最好的选择。然而，对于高维感知数据，这样的无结构网络可能会变得笨拙。\\n+\\n+例如，让我们回到我们正在运行的区分猫和狗的例子上。假设我们在数据收集方面做了一次彻底的工作，收集了一百万像素照片的带注释的数据集。这意味着网络的每一次输入都有一百万个维度。即使大幅减少到1,000个隐藏维度，也需要一个以$10^6 \\\\times 10^3 = 10^9$个参数为特征的完全连接层。除非我们有大量的GPU，在分布式优化方面的天赋，以及非凡的耐心，否则学习这个网络的参数可能被证明是不可行的。\\n+\\n+细心的读者可能会反对这一论点，因为1百万像素的分辨率可能不是必要的。然而，虽然我们可能可以用10万像素逃脱惩罚，但我们1000大小的隐藏层严重低估了学习图像良好表示所需的隐藏单元的数量，因此一个实用的系统仍然需要数十亿个参数。此外，通过拟合如此多的参数来学习分类器可能需要收集大量的数据集。然而，今天人类和计算机都能很好地区分猫和狗，似乎与这些直觉相矛盾。这是因为图像显示了丰富的结构，人类和机器学习模型都可以利用这些结构。卷积神经网络(CNNs)是机器学习在利用自然图像中的一些已知结构时采用的一种创造性的方法。\\n+\\n+## 不变性\\n+\\n+假设您想要检测图像中的一个对象。这似乎是合理的，无论我们使用什么方法来识别对象，都不应该过分关注对象在图像中的精确位置。理想情况下，我们的系统应该利用这些知识。猪通常不会飞翔，飞机通常不会游泳。尽管如此，如果一头猪出现在图像的顶部，我们仍然应该认出它。我们可以从儿童游戏“沃尔多在哪里”(:numref:`img_waldo`)中得到一些启发。这个游戏由一些混乱的场景组成，充斥着各种活动。沃尔多出现在每一个地方，通常潜伏在一些不太可能的地方。读者的目标是找到他。尽管他的着装很有特色，但由于有大量的分心事情，这可能会出人意料地困难。然而，*Waldo是什么样子*并不取决于*Waldo位于*哪里*。我们可以用沃尔多探测器扫描图像，它可以给每个补丁分配一个分数，表明补丁包含沃尔多的可能性。CNNs将这一“空间不变性”的概念系统化，利用它来学习参数较少的有用表示。\\n+\\n+![An image of the \"Where\\'s Waldo\" game.](../img/where-wally-walker-books.jpg)\\n+:width:`400px`\\n+:label:`img_waldo`\\n+\\n+现在，我们可以通过列举一些理想的数据来指导我们设计适用于计算机视觉的神经网络体系结构，从而使这些直觉变得更加具体：\\n+\\n+1. 在最早的层中，我们的网络应该对相同的补丁做出类似的响应，而不管它出现在图像中的哪个位置。这一原理称为“平移不变性”。\\n+1. 网络的最早层应该集中在本地区域，而不考虑远程区域的图像内容。这就是“地方性”原则。最终，这些局部表示可以被聚合以在整个图像级别上做出预测。\\n+\\n+让我们看看这是如何转化为数学的。\\n+\\n+## 约束MLP\\n+\\n+首先，我们可以考虑具有二维图像$\\\\mathbf{X}$作为输入并且它们的直接隐藏表示$\\\\mathbf{H}$类似地被表示为数学上的矩阵和代码中的二维张量的多维图像处理，其中$\\\\mathbf{X}$和$\\\\mathbf{H}$具有相同的形状。让这一点深入人心。我们现在认为，不仅输入，而且隐藏的表征都具有空间结构。\\n+\\n+设$[\\\\mathbf{X}]_{i, j}$和$[\\\\mathbf{H}]_{i, j}$分别表示输入图像和隐藏表示中位置($i$,$j$)处的像素。因此，为了使每个隐藏单元接收来自每个输入像素的输入，我们将从使用权重矩阵(如我们先前在MLP中所做的那样)切换到将我们的参数表示为四阶权重张量$\\\\mathsf{W}$。假设$\\\\mathbf{U}$包含偏差，我们可以将完全连接层正式表示为\\n+\\n+$$\\\\begin{aligned} \\\\left[\\\\mathbf{H}\\\\right]_{i, j} &= [\\\\mathbf{U}]_{i, j} + \\\\sum_k \\\\sum_l[\\\\mathsf{W}]_{i, j, k, l}  [\\\\mathbf{X}]_{k, l}\\\\\\\\ &=  [\\\\mathbf{U}]_{i, j} +\\n+\\\\sum_a \\\\sum_b [\\\\mathsf{V}]_{i, j, a, b}  [\\\\mathbf{X}]_{i+a, j+b}.\\\\end{aligned},$$\\n+\\n+其中，由于在两个四阶张量中的系数之间存在一对一的对应关系，因此从$\\\\mathsf{W}$到$\\\\mathsf{V}$的切换目前完全是表面上的。我们简单地重新索引下标$(k, l)$，使得$k = i+a$和$l = j+b$。换句话说，我们设定了$[\\\\mathsf{V}]_{i, j, a, b} = [\\\\mathsf{W}]_{i, j, i+a, j+b}$。索引$a$和$b$在正偏移量和负偏移量两者上运行，覆盖整个图像。对于隐藏表示$[\\\\mathbf{H}]_{i, j}$中的任何给定位置($i$,$j$)，我们通过对$x$中的像素求和，以$(i, j)$为中心并按$[\\\\mathsf{V}]_{i, j, a, b}$加权来计算其值。\\n+\\n+### 平移不变性\\n+\\n+现在让我们引用上面建立的第一个原则：平移不变性。这意味着输入$\\\\mathbf{X}$中的移位应该简单地导致隐藏表示$\\\\mathbf{H}$中的移位。这只有在$\\\\mathsf{V}$和$\\\\mathbf{U}$实际上不依赖于$(i, j)$的情况下才有可能，也就是说，我们有$[\\\\mathsf{V}]_{i, j, a, b} = [\\\\mathbf{V}]_{a, b}$，而$\\\\mathbf{U}$是一个常数，比如说$u$。因此，我们可以简化$\\\\mathbf{H}$的定义：\\n+\\n+$$[\\\\mathbf{H}]_{i, j} = u + \\\\sum_a\\\\sum_b [\\\\mathbf{V}]_{a, b}  [\\\\mathbf{X}]_{i+a, j+b}.$$\\n+\\n+这是一个*卷积*！我们在$(i+a, j+b)$用系数$[\\\\mathbf{V}]_{a, b}$对位置$(i, j)$附近的像素进行有效加权，以获得值$[\\\\mathbf{H}]_{i, j}$。请注意，$[\\\\mathbf{V}]_{a, b}$需要的系数比$[\\\\mathsf{V}]_{i, j, a, b}$少得多，因为它不再依赖于图像中的位置。我们已经取得了重大进展！\\n+\\n+###  地方性\\n+\\n+现在让我们引用第二个原则：局部性。如上所述，我们认为，我们不应该为了收集相关信息来评估$(i, j)$地点正在发生的事情，就必须在距离地点$[\\\\mathbf{H}]_{i, j}$很远的地方寻找。这意味着在某个范围$|a|> \\\\Delta$或$|b| > \\\\Delta$之外，我们应该设置为$[\\\\mathbf{V}]_{a, b} = 0$。等效地，我们可以将$[\\\\mathbf{H}]_{i, j}$重写为\\n+\\n+$$[\\\\mathbf{H}]_{i, j} = u + \\\\sum_{a = -\\\\Delta}^{\\\\Delta} \\\\sum_{b = -\\\\Delta}^{\\\\Delta} [\\\\mathbf{V}]_{a, b}  [\\\\mathbf{X}]_{i+a, j+b}.$$\\n+:eqlabel:`eq_conv-layer`\\n+\\n+注意，简而言之，:eqref:`eq_conv-layer`是*卷积层*。\\n+*卷积神经网络(CNNs)\\n+是包含卷积层的一类特殊的神经网络。在深度学习研究社区中，$\\\\mathbf{V}$被称为“卷积内核”、“过滤”，或者仅仅是层的“权重”，它们通常是可学习的参数。当本地区域较小时，与完全连接的网络相比，差异可能会很大。以前，我们可能需要数十亿个参数来表示图像处理网络中的单个层，而现在我们通常只需要几百个参数，而不改变输入或隐藏表示的维度。这种参数的急剧减少所付出的代价是，我们的功能现在是平移不变的，并且在确定每个隐藏激活的值时，我们的层只能合并本地信息。所有的学习都依赖于强加归纳偏见。当这种偏见与现实相符时，我们就得到了样本效率高的模型，它很好地推广到了看不见的数据。但当然，如果这些偏见与现实不符，例如，如果图像被证明不是平移不变的，我们的模型甚至可能难以拟合我们的训练数据。\\n+\\n+## 卷积\\n+\\n+在进一步讨论之前，我们应该简单回顾一下为什么上述运算被称为卷积。在数学中，两个函数(比方说$f, g: \\\\mathbb{R}^d \\\\to \\\\mathbb{R}$)之间的“卷积”定义为\\n+\\n+$$(f * g)(\\\\mathbf{x}) = \\\\int f(\\\\mathbf{z}) g(\\\\mathbf{x}-\\\\mathbf{z}) d\\\\mathbf{z}.$$\\n+\\n+也就是说，当一个函数被“翻转”并移位$f$时，我们测量$f$和$g$之间的重叠。只要我们有离散的物体，积分就会变成和。例如，对于指数超过$\\\\mathbb{Z}$的平方可和无限维向量集合中的向量，我们获得以下定义：\\n+\\n+$$(f * g)(i) = \\\\sum_a f(a) g(i-a).$$\\n+\\n+对于二维张量，我们有一个相应的总和，指数分别为$(a, b)$($f$)和$(i-a, j-b)$($g$)：\\n+\\n+$$(f * g)(i, j) = \\\\sum_a\\\\sum_b f(a, b) g(i-a, j-b).$$\\n+:eqlabel:`eq_2d-conv-discrete`\\n+\\n+这看起来与:eqref:`eq_conv-layer`类似，但有一个主要区别。我们使用的不是$(i+a, j+b)$，而是差值。不过请注意，这种区别主要是表面性的，因为我们总是可以匹配:eqref:`eq_conv-layer`和:eqref:`eq_2d-conv-discrete`之间的记号。我们在:eqref:`eq_conv-layer`中的原始定义更恰当地描述了*交叉相关*。我们将在下一节回到这一点。\\n+\\n+## 重温“沃尔多在哪里”\\n+\\n+回到我们的沃尔多探测器，让我们看看这是什么样子。卷积层根据过滤$\\\\mathsf{V}$挑选给定大小的窗口并加权强度，如:numref:`fig_waldo_mask`中所示。我们的目标可能是学习一个模型，这样无论在哪里“瓦朗度”最高，我们都应该在隐藏层表示中找到一个峰值。\\n+\\n+![Detect Waldo.](../img/waldo-mask.jpg)\\n+:width:`400px`\\n+:label:`fig_waldo_mask`\\n+\\n+### 频道\\n+:label:`subsec_why-conv-channels`\\n+\\n+这种方法只有一个问题。到目前为止，我们幸福地忽略了图像由3个通道组成：红色、绿色和蓝色。实际上，图像不是二维对象，而是三阶张量，其特征在于高度、宽度和通道，例如形状为$1024 \\\\times 1024 \\\\times 3$像素。虽然这些轴中的前两个涉及空间关系，但第三个轴可以被视为为每个像素位置分配多维表示。因此，我们将$\\\\mathsf{X}$索引为$[\\\\mathsf{X}]_{i, j, k}$。错综复杂的过滤不得不做出相应的调整。我们现在有$[\\\\mathsf{V}]_{a,b,c}$人，而不是$[\\\\mathbf{V}]_{a,b}$人。\\n+\\n+此外，正如我们的输入由三阶张量组成一样，事实证明，将我们的隐藏表示类似地表示为三阶张量$\\\\mathsf{H}$也是一个好主意。换句话说，我们想要对应于每个空间位置的隐藏表示的整个向量，而不是只有一个与每个空间位置相对应的隐藏表示。我们可以认为隐藏的表示是由若干个堆叠在一起的二维网格组成的。与输入一样，这些有时也称为*通道*。它们有时也称为“要素地图”，因为每个地图都向后续图层提供了一组空间化的学习要素。直观地说，您可能会想象在更靠近输入的较低层，某些通道可能会专门用于识别边，而其他通道可能会识别纹理。\\n+\\n+要同时支持输入($\\\\mathsf{X}$)和隐藏表示($\\\\mathsf{H}$)中的多个通道，我们可以将第四个坐标添加到$\\\\mathsf{V}$：$[\\\\mathsf{V}]_{a, b, c, d}$。把我们拥有的一切都放在一起：\\n+\\n+$$[\\\\mathsf{H}]_{i,j,d} = \\\\sum_{a = -\\\\Delta}^{\\\\Delta} \\\\sum_{b = -\\\\Delta}^{\\\\Delta} \\\\sum_c [\\\\mathsf{V}]_{a, b, c, d} [\\\\mathsf{X}]_{i+a, j+b, c},$$\\n+:eqlabel:`eq_conv-layer-channels`\\n+\\n+其中$d$索引隐藏表示$\\\\mathsf{H}$中的输出通道。随后的卷积层将继续接受三阶张量$\\\\mathsf{H}$作为输入。更一般地，:eqref:`eq_conv-layer-channels`是多个信道的卷积层的定义，其中$\\\\mathsf{V}$是该层的核心或过滤。\\n+\\n+我们仍有许多行动需要解决。例如，我们需要弄清楚如何将所有隐藏的表示组合到单个输出中，例如，图像中是否有Waldo“Anywhere”。我们还需要决定如何有效地计算事物，如何组合多层、适当的激活函数，以及如何做出合理的设计选择，以产生在实践中有效的网络。我们将在本章的剩余部分讨论这些问题。\\n+\\n+## 摘要\\n+\\n+* 图像中的平移不变性意味着图像的所有块都将以相同的方式处理。\\n+* 局部性意味着只有一小部分像素将用于计算相应的隐藏表示。\\n+* 在图像处理中，卷积层通常比完全连通的层需要的参数少得多。\\n+* CNN是一类特殊的包含卷积层的神经网络。\\n+* 输入和输出上的通道允许我们的模型在每个空间位置捕捉图像的多个方面。\\n+\\n+## 练习\\n+\\n+1. 假设卷积核的大小为$\\\\Delta = 0$。显示在这种情况下，卷积内核为每组信道独立地实现MLP。\\n+1. 为什么翻译不变毕竟不是一个好主意呢？\\n+1. 在决定如何处理与图像边界上的像素位置相对应的隐藏表示时，我们必须处理哪些问题？\\n+1. 描述音频的类似卷积层。\\n+1. 您认为卷积层是否也适用于文本数据？为什么或者为什么不？\\n+1. 证明那$f * g = g * f$。\\n+\\n+[Discussions](https://discuss.d2l.ai/t/64)\\ndiff --git a/chapter_deep-learning-computation/custom-layer_baidu.md b/chapter_deep-learning-computation/custom-layer_baidu.md\\nnew file mode 100644\\nindex 00000000..c29e2305\\n--- /dev/null\\n+++ b/chapter_deep-learning-computation/custom-layer_baidu.md\\n@@ -0,0 +1,234 @@\\n+# 自定义图层\\n+\\n+深度学习成功背后的一个因素是，可以用创造性的方式组成一个广泛的层次来设计适合各种任务的架构。例如，研究人员发明了专门用于处理图像、文本、在连续数据上循环以及执行动态编程的层。迟早，你会遇到或发明一个在深度学习框架中还不存在的层。在这些情况下，必须构建自定义层。在本节中，我们将向您展示如何。\\n+\\n+## 无参数图层\\n+\\n+首先，我们构建一个没有任何参数的自定义层。在我们的介绍中，如果你能回忆起我们介绍的22934街区。下面的`CenteredLayer`类只需从输入中减去平均值。要构建它，我们只需要从基层类继承并实现前向传播函数。\\n+\\n+```{.python .input}\\n+from mxnet import gluon, np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+class CenteredLayer(nn.Block):\\n+    def __init__(self, **kwargs):\\n+        super().__init__(**kwargs)\\n+\\n+    def forward(self, X):\\n+        return X - X.mean()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+import torch\\n+from torch import nn\\n+import torch.nn.functional as F\\n+\\n+class CenteredLayer(nn.Module):\\n+    def __init__(self):\\n+        super().__init__()\\n+\\n+    def forward(self, X):\\n+        return X - X.mean()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+import tensorflow as tf\\n+\\n+class CenteredLayer(tf.keras.Model):\\n+    def __init__(self):\\n+        super().__init__()\\n+\\n+    def call(self, inputs):\\n+        return inputs - tf.reduce_mean(inputs)\\n+```\\n+\\n+让我们通过输入一些数据来验证我们的层是否按预期工作。\\n+\\n+```{.python .input}\\n+layer = CenteredLayer()\\n+layer(np.array([1, 2, 3, 4, 5]))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+layer = CenteredLayer()\\n+layer(torch.FloatTensor([1, 2, 3, 4, 5]))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+layer = CenteredLayer()\\n+layer(tf.constant([1, 2, 3, 4, 5]))\\n+```\\n+\\n+我们现在可以将我们的层作为一个组件来构建更复杂的模型。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nn.Dense(128), CenteredLayer())\\n+net.initialize()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = tf.keras.Sequential([tf.keras.layers.Dense(128), CenteredLayer()])\\n+```\\n+\\n+作为额外的健全性检查，我们可以通过网络发送随机数据，并检查平均值是否实际上为0。因为我们处理的是浮点数，我们可能仍然会看到一个非常小的非零数，这是由于量化。\\n+\\n+```{.python .input}\\n+Y = net(np.random.uniform(size=(4, 8)))\\n+Y.mean()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+Y = net(torch.rand(4, 8))\\n+Y.mean()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+Y = net(tf.random.uniform((4, 8)))\\n+tf.reduce_mean(Y)\\n+```\\n+\\n+## 带参数的图层\\n+\\n+既然我们知道了如何定义简单的层，那么让我们继续定义具有参数的层，这些参数可以通过训练进行调整。我们可以使用内置函数来创建参数，这些参数提供一些基本的管理功能。特别是，它们管理访问、初始化、共享、保存和加载模型参数。这样，除了其他好处外，我们不需要为每个自定义层编写自定义序列化例程。\\n+\\n+现在让我们实现我们自己版本的完全连接层。回想一下，这个层需要两个参数，一个表示权重，另一个表示偏差。在这个实现中，我们将ReLU激活作为默认值。该层需要输入参数：`in_units`和`units`，分别表示输入和输出的数量。\\n+\\n+```{.python .input}\\n+class MyDense(nn.Block):\\n+    def __init__(self, units, in_units, **kwargs):\\n+        super().__init__(**kwargs)\\n+        self.weight = self.params.get(\\'weight\\', shape=(in_units, units))\\n+        self.bias = self.params.get(\\'bias\\', shape=(units,))\\n+\\n+    def forward(self, x):\\n+        linear = np.dot(x, self.weight.data(ctx=x.ctx)) + self.bias.data(\\n+            ctx=x.ctx)\\n+        return npx.relu(linear)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class MyLinear(nn.Module):\\n+    def __init__(self, in_units, units):\\n+        super().__init__()\\n+        self.weight = nn.Parameter(torch.randn(in_units, units))\\n+        self.bias = nn.Parameter(torch.randn(units,))\\n+    def forward(self, X):\\n+        linear = torch.matmul(X, self.weight.data) + self.bias.data\\n+        return F.relu(linear)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class MyDense(tf.keras.Model):\\n+    def __init__(self, units):\\n+        super().__init__()\\n+        self.units = units\\n+\\n+    def build(self, X_shape):\\n+        self.weight = self.add_weight(name=\\'weight\\',\\n+            shape=[X_shape[-1], self.units],\\n+            initializer=tf.random_normal_initializer())\\n+        self.bias = self.add_weight(\\n+            name=\\'bias\\', shape=[self.units],\\n+            initializer=tf.zeros_initializer())\\n+\\n+    def call(self, X):\\n+        return tf.matmul(X, self.weight) + self.bias\\n+```\\n+\\n+接下来，我们实例化`MyDense`类并访问其模型参数。\\n+\\n+```{.python .input}\\n+dense = MyDense(units=3, in_units=5)\\n+dense.params\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+dense = MyLinear(5, 3)\\n+dense.weight\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+dense = MyDense(3)\\n+dense(tf.random.uniform((2, 5)))\\n+dense.get_weights()\\n+```\\n+\\n+我们可以使用自定义层直接进行前向传播计算。\\n+\\n+```{.python .input}\\n+dense.initialize()\\n+dense(np.random.uniform(size=(2, 5)))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+dense(torch.rand(2, 5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+dense(tf.random.uniform((2, 5)))\\n+```\\n+\\n+我们也可以使用自定义层构造模型。一旦我们有了它，我们就可以像内置的完全连接层一样使用它。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(MyDense(8, in_units=64),\\n+        MyDense(1, in_units=8))\\n+net.initialize()\\n+net(np.random.uniform(size=(2, 64)))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\\n+net(torch.rand(2, 64))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = tf.keras.models.Sequential([MyDense(8), MyDense(1)])\\n+net(tf.random.uniform((2, 64)))\\n+```\\n+\\n+## 摘要\\n+\\n+* 我们可以通过基本层类设计自定义层。这允许我们定义灵活的新层，其行为与库中的任何现有层不同。\\n+* 一旦定义好，就可以在任意上下文和体系结构中调用自定义层。\\n+* 图层可以有局部参数，可以通过内置函数创建。\\n+\\n+## 练习\\n+\\n+1. 设计一个接受输入并计算张量缩减的层，即返回$y_k = \\\\sum_{i, j} W_{ijk} x_i x_j$。\\n+1. 设计一个返回数据傅立叶系数前半部分的层。\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/58)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/59)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/279)\\n+:end_tab:\\ndiff --git a/chapter_deep-learning-computation/custom-layer_tencent.md b/chapter_deep-learning-computation/custom-layer_tencent.md\\nnew file mode 100644\\nindex 00000000..2091395b\\n--- /dev/null\\n+++ b/chapter_deep-learning-computation/custom-layer_tencent.md\\n@@ -0,0 +1,234 @@\\n+# 自定义图层\\n+\\n+深度学习成功背后的一个因素是，可以用创造性的方式组合广泛的层，以设计适合于各种任务的体系结构。例如，研究人员发明了专门用于处理图像、文本、循环顺序数据和执行动态编程的层。迟早，你会遇到或发明一个在深度学习框架中还不存在的层。在这些情况下，您必须构建自定义层。在本节中，我们将向您展示如何操作。\\n+\\n+## 不带参数的图层\\n+\\n+首先，我们构造一个自己没有任何参数的自定义层。如果你还记得我们在:numref:`sec_model_construction`对挡路的介绍，这应该看起来很眼熟。下面的`CenteredLayer`个类只需从其输入中减去平均值。要构建它，我们只需继承基础层类并实现前向传播功能。\\n+\\n+```{.python .input}\\n+from mxnet import gluon, np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+class CenteredLayer(nn.Block):\\n+    def __init__(self, **kwargs):\\n+        super().__init__(**kwargs)\\n+\\n+    def forward(self, X):\\n+        return X - X.mean()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+import torch\\n+from torch import nn\\n+import torch.nn.functional as F\\n+\\n+class CenteredLayer(nn.Module):\\n+    def __init__(self):\\n+        super().__init__()\\n+\\n+    def forward(self, X):\\n+        return X - X.mean()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+import tensorflow as tf\\n+\\n+class CenteredLayer(tf.keras.Model):\\n+    def __init__(self):\\n+        super().__init__()\\n+\\n+    def call(self, inputs):\\n+        return inputs - tf.reduce_mean(inputs)\\n+```\\n+\\n+让我们通过向其提供一些数据来验证我们的层是否按预期工作。\\n+\\n+```{.python .input}\\n+layer = CenteredLayer()\\n+layer(np.array([1, 2, 3, 4, 5]))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+layer = CenteredLayer()\\n+layer(torch.FloatTensor([1, 2, 3, 4, 5]))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+layer = CenteredLayer()\\n+layer(tf.constant([1, 2, 3, 4, 5]))\\n+```\\n+\\n+现在，我们可以将层作为组件合并到构建更复杂的模型中。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nn.Dense(128), CenteredLayer())\\n+net.initialize()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = tf.keras.Sequential([tf.keras.layers.Dense(128), CenteredLayer()])\\n+```\\n+\\n+作为额外的健全性检查，我们可以通过网络发送随机数据，并检查平均值实际上是否为0。因为我们处理的是浮点数，所以由于量化，我们可能仍然会看到一个非常小的非零数。\\n+\\n+```{.python .input}\\n+Y = net(np.random.uniform(size=(4, 8)))\\n+Y.mean()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+Y = net(torch.rand(4, 8))\\n+Y.mean()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+Y = net(tf.random.uniform((4, 8)))\\n+tf.reduce_mean(Y)\\n+```\\n+\\n+## 带参数的图层\\n+\\n+现在我们知道了如何定义简单的层，让我们继续使用可以通过培训调整的参数来定义层。我们可以使用内置函数创建参数，这些参数提供一些基本的内务管理功能。具体地说，它们管理访问、初始化、共享、保存和加载模型参数。这样，除了其他好处外，我们将不需要为每个自定义层编写自定义序列化例程。\\n+\\n+现在，让我们实现我们自己版本的完全连接层。回想一下，该层需要两个参数，一个用于表示权重，另一个用于偏移。在此实现中，我们默认烘焙RELU激活。这一层需要输入参数：`in_units`和`units`，分别表示输入和输出的数量。\\n+\\n+```{.python .input}\\n+class MyDense(nn.Block):\\n+    def __init__(self, units, in_units, **kwargs):\\n+        super().__init__(**kwargs)\\n+        self.weight = self.params.get(\\'weight\\', shape=(in_units, units))\\n+        self.bias = self.params.get(\\'bias\\', shape=(units,))\\n+\\n+    def forward(self, x):\\n+        linear = np.dot(x, self.weight.data(ctx=x.ctx)) + self.bias.data(\\n+            ctx=x.ctx)\\n+        return npx.relu(linear)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class MyLinear(nn.Module):\\n+    def __init__(self, in_units, units):\\n+        super().__init__()\\n+        self.weight = nn.Parameter(torch.randn(in_units, units))\\n+        self.bias = nn.Parameter(torch.randn(units,))\\n+    def forward(self, X):\\n+        linear = torch.matmul(X, self.weight.data) + self.bias.data\\n+        return F.relu(linear)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class MyDense(tf.keras.Model):\\n+    def __init__(self, units):\\n+        super().__init__()\\n+        self.units = units\\n+\\n+    def build(self, X_shape):\\n+        self.weight = self.add_weight(name=\\'weight\\',\\n+            shape=[X_shape[-1], self.units],\\n+            initializer=tf.random_normal_initializer())\\n+        self.bias = self.add_weight(\\n+            name=\\'bias\\', shape=[self.units],\\n+            initializer=tf.zeros_initializer())\\n+\\n+    def call(self, X):\\n+        return tf.matmul(X, self.weight) + self.bias\\n+```\\n+\\n+接下来，我们实例化`MyDense`类并访问其模型参数。\\n+\\n+```{.python .input}\\n+dense = MyDense(units=3, in_units=5)\\n+dense.params\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+dense = MyLinear(5, 3)\\n+dense.weight\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+dense = MyDense(3)\\n+dense(tf.random.uniform((2, 5)))\\n+dense.get_weights()\\n+```\\n+\\n+我们可以使用自定义层直接执行前向传播计算。\\n+\\n+```{.python .input}\\n+dense.initialize()\\n+dense(np.random.uniform(size=(2, 5)))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+dense(torch.rand(2, 5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+dense(tf.random.uniform((2, 5)))\\n+```\\n+\\n+我们还可以使用自定义层构建模型。一旦我们拥有了它，我们就可以像使用内置的完全连接层一样使用它。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(MyDense(8, in_units=64),\\n+        MyDense(1, in_units=8))\\n+net.initialize()\\n+net(np.random.uniform(size=(2, 64)))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\\n+net(torch.rand(2, 64))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = tf.keras.models.Sequential([MyDense(8), MyDense(1)])\\n+net(tf.random.uniform((2, 64)))\\n+```\\n+\\n+## 摘要\\n+\\n+* 我们可以通过Basic Layer类设计自定义层。这允许我们定义灵活的新层，其行为不同于库中的任何现有层。\\n+* 一旦定义，自定义层就可以在任意上下文和体系结构中调用。\\n+* 层可以有本地参数，这些参数可以通过内置函数创建。\\n+\\n+## 练习\\n+\\n+1. 设计一个接受输入并计算张量缩减的层，即，它返回$y_k = \\\\sum_{i, j} W_{ijk} x_i x_j$。\\n+1. 设计一个返回数据傅立叶系数前半部分的层。\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/58)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/59)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/279)\\n+:end_tab:\\ndiff --git a/chapter_deep-learning-computation/deferred-init_baidu.md b/chapter_deep-learning-computation/deferred-init_baidu.md\\nnew file mode 100644\\nindex 00000000..1d399a40\\n--- /dev/null\\n+++ b/chapter_deep-learning-computation/deferred-init_baidu.md\\n@@ -0,0 +1,106 @@\\n+# 延迟初始化\\n+:label:`sec_deferred_init`\\n+\\n+到目前为止，似乎我们在建立人际网络时表现得很马虎。具体来说，我们做了以下不直观的事情，这些事情似乎不应该奏效：\\n+\\n+* 我们定义了网络架构，但没有指定输入维度。\\n+* 我们添加层时没有指定前一层的输出维度。\\n+* 我们甚至在提供足够的信息来确定我们的模型应该包含多少参数之前“初始化”了这些参数。\\n+\\n+您可能会惊讶于我们的代码运行。毕竟，深度学习框架无法判断网络的输入维度是什么。这里的诀窍是框架*推迟初始化*，等到我们第一次通过模型传递数据时，才能动态地推断出每个层的大小。\\n+\\n+随后，当使用卷积神经网络时，由于输入维度（即图像的分辨率）将影响每个后续层的维数，因此该技术将变得更加方便。因此，在编写代码时无需知道维度是什么而设置参数的能力可以大大简化指定和随后修改模型的任务。接下来，我们将更深入地研究初始化机制。\\n+\\n+## 实例化网络\\n+\\n+首先，让我们实例化一个MLP。\\n+\\n+```{.python .input}\\n+from mxnet import init, np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+def get_net():\\n+    net = nn.Sequential()\\n+    net.add(nn.Dense(256, activation=\\'relu\\'))\\n+    net.add(nn.Dense(10))\\n+    return net\\n+\\n+net = get_net()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+import tensorflow as tf\\n+\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Dense(256, activation=tf.nn.relu),\\n+    tf.keras.layers.Dense(10),\\n+])\\n+```\\n+\\n+此时，网络不可能知道输入层权重的维数，因为输入维数仍然未知。因此，框架尚未初始化任何参数。我们通过尝试访问以下参数进行确认。\\n+\\n+```{.python .input}\\n+print(net.collect_params)\\n+print(net.collect_params())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+[net.layers[i].get_weights() for i in range(len(net.layers))]\\n+```\\n+\\n+:begin_tab:`mxnet`\\n+请注意，当参数对象存在时，每个层的输入维度列为-1。MXNet使用特殊值-1表示参数维度仍然未知。此时，尝试访问`net[0].weight.data()`将触发运行时错误，指出必须先初始化网络，然后才能访问参数。现在让我们看看当我们试图通过`initialize`函数初始化参数时会发生什么。\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+请注意，每个层对象都存在，但权重为空。使用`net.get_weights()`将抛出一个错误，因为权重尚未初始化。\\n+:end_tab:\\n+\\n+```{.python .input}\\n+net.initialize()\\n+net.collect_params()\\n+```\\n+\\n+:begin_tab:`mxnet`\\n+如我们所见，一切都没有改变。当输入维度未知时，调用initialize不会真正初始化参数。相反，这个调用注册到我们希望初始化参数的MXNet（可选，根据哪个分布）。\\n+:end_tab:\\n+\\n+接下来让我们通过网络传递数据，使框架最终初始化参数。\\n+\\n+```{.python .input}\\n+X = np.random.uniform(size=(2, 20))\\n+net(X)\\n+\\n+net.collect_params()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.random.uniform((2, 20))\\n+net(X)\\n+[w.shape for w in net.get_weights()]\\n+```\\n+\\n+一旦我们知道输入维数20，框架就可以通过插入值20来识别第一层权重矩阵的形状。识别出第一层的形状后，框架进入第二层，依此类推，直到所有形状都已知为止。注意，在这种情况下，只有第一层需要延迟初始化，但是框架按顺序初始化。一旦知道了所有参数形状，框架就可以最终初始化参数。\\n+\\n+## 摘要\\n+\\n+* 延迟初始化可以很方便，允许框架自动推断参数形状，使修改体系结构变得容易，并消除了一个常见的错误源。\\n+* 我们可以通过模型传递数据，使框架最终初始化参数。\\n+\\n+## 练习\\n+\\n+1. 如果将输入尺寸指定给第一个图层，而不是指定给后续图层，会发生什么情况？你能立即初始化吗？\\n+1. 如果指定不匹配的维度，会发生什么情况？\\n+1. 如果你有不同维度的输入，你需要做什么？提示：看看参数绑定。\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/280)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/281)\\n+:end_tab:\\ndiff --git a/chapter_deep-learning-computation/deferred-init_tencent.md b/chapter_deep-learning-computation/deferred-init_tencent.md\\nnew file mode 100644\\nindex 00000000..7dbb0a81\\n--- /dev/null\\n+++ b/chapter_deep-learning-computation/deferred-init_tencent.md\\n@@ -0,0 +1,106 @@\\n+# 延迟初始化\\n+:label:`sec_deferred_init`\\n+\\n+到目前为止，似乎我们在建立网络方面的草率行为没有受到影响。具体地说，我们做了以下不直观的事情，它们可能看起来不应该起作用：\\n+\\n+* 我们定义了网络体系结构，但没有指定输入维度。\\n+* 我们在没有指定上一层的输出尺寸的情况下添加层。\\n+* 我们甚至在提供足够的信息来确定我们的模型应该包含多少参数之前“初始化”了这些参数。\\n+\\n+您可能会对我们的代码运行感到惊讶。毕竟，深度学习框架无法判断网络的输入维度是多少。这里的诀窍是，框架*推迟初始化*，等到我们第一次通过模型传递数据时，才推断飞翔上每一层的大小。\\n+\\n+稍后，当与卷积神经网络一起工作时，该技术将变得更加方便，因为输入维数(即，图像的分辨率)将影响每个后续层的维数。因此，在编写代码时无需知道维度是什么而设置参数的能力可以极大地简化指定和随后修改模型的任务。接下来，我们将更深入地了解初始化机制。\\n+\\n+## 实例化网络\\n+\\n+首先，让我们实例化一个MLP。\\n+\\n+```{.python .input}\\n+from mxnet import init, np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+def get_net():\\n+    net = nn.Sequential()\\n+    net.add(nn.Dense(256, activation=\\'relu\\'))\\n+    net.add(nn.Dense(10))\\n+    return net\\n+\\n+net = get_net()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+import tensorflow as tf\\n+\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Dense(256, activation=tf.nn.relu),\\n+    tf.keras.layers.Dense(10),\\n+])\\n+```\\n+\\n+在这一点上，网络不可能知道输入层的权重的维度，因为输入维度仍然未知。因此，框架尚未初始化任何参数。我们通过尝试访问以下参数进行确认。\\n+\\n+```{.python .input}\\n+print(net.collect_params)\\n+print(net.collect_params())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+[net.layers[i].get_weights() for i in range(len(net.layers))]\\n+```\\n+\\n+:begin_tab:`mxnet`\\n+请注意，当参数对象存在时，每层的输入尺寸列为-1。MXNet使用特定值-1表示参数尺寸仍然未知。在这一点上，访问`net[0].weight.data()`的尝试将触发运行时错误，声明在可以访问参数之前必须初始化网络。现在让我们看看当我们试图通过`initialize`函数初始化参数时会发生什么。\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+请注意，每个层对象都存在，但权重为空。使用`net.get_weights()`会引发错误，因为权重尚未初始化。\\n+:end_tab:\\n+\\n+```{.python .input}\\n+net.initialize()\\n+net.collect_params()\\n+```\\n+\\n+:begin_tab:`mxnet`\\n+正如我们所见，一切都没有改变。当输入尺寸未知时，调用Initialize不会真正初始化参数。相反，此调用注册到MXNet，这是我们希望的(并且可选地，根据哪个分布)来初始化参数。\\n+:end_tab:\\n+\\n+接下来，让我们通过网络传递数据，使框架最终初始化参数。\\n+\\n+```{.python .input}\\n+X = np.random.uniform(size=(2, 20))\\n+net(X)\\n+\\n+net.collect_params()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.random.uniform((2, 20))\\n+net(X)\\n+[w.shape for w in net.get_weights()]\\n+```\\n+\\n+一旦我们知道输入维数20，框架就可以通过插入值20来识别第一层的权重矩阵的形状。在识别了第一层的形状之后，框架继续进行到第二层，依此类推，通过计算图直到知道所有的形状。请注意，在这种情况下，只有第一层需要延迟初始化，但框架会按顺序初始化。一旦所有参数形状都已知，框架最终可以初始化参数。\\n+\\n+## 摘要\\n+\\n+* 延迟初始化可能很方便，允许框架自动推断参数形状，使修改体系结构变得容易，并消除了一个常见的错误来源。\\n+* 我们可以通过模型传递数据，使框架最终初始化参数。\\n+\\n+## 练习\\n+\\n+1. 如果将输入尺寸指定给第一层而不指定给后续层，会发生什么情况？您可以立即进行初始化吗？\\n+1. 如果指定不匹配的尺寸，会发生什么情况？\\n+1. 如果您有不同维度的输入，您需要做什么？提示：请看参数绑定。\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/280)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/281)\\n+:end_tab:\\ndiff --git a/chapter_deep-learning-computation/index_baidu.md b/chapter_deep-learning-computation/index_baidu.md\\nnew file mode 100644\\nindex 00000000..c4947dc6\\n--- /dev/null\\n+++ b/chapter_deep-learning-computation/index_baidu.md\\n@@ -0,0 +1,17 @@\\n+# 深度学习计算\\n+:label:`chap_computation`\\n+\\n+除了巨大的数据集和强大的硬件之外，优秀的软件工具在深度学习的快速发展中扮演着不可或缺的角色。从2007年发布的开创性的Theano库开始，灵活的开源工具使研究人员能够快速原型化模型，避免在回收标准组件时重复工作，同时仍能保持进行低级修改的能力。随着时间的推移，深度学习的库已经发展到提供越来越粗糙的摘要。就像半导体设计师从指定晶体管到逻辑电路再到编写代码一样，神经网络研究人员也从考虑单个人工神经元的行为转变为从整个层的角度来构想网络，现在通常在设计结构时考虑的是更粗糙的块。\\n+\\n+到目前为止，我们已经介绍了一些基本的机器学习概念，逐步发展到功能齐全的深度学习模型。在最后一章中，我们从头开始实现了MLP的每个组件，甚至展示了如何利用高级api轻松地推出相同的模型。为了让你更快地走到这一步，我们*访问了*库，但跳过了关于*它们如何工作*的更高级的细节。在本章中，我们将揭开帷幕，深入挖掘深度学习计算的关键组成部分，即模型构建、参数访问和初始化、设计自定义层和块、将模型读写到磁盘以及利用gpu实现显著的加速。深度学习lib库可以让你从*终端用户到*Power用户，让你拥有一个成熟的深度学习库的好处所需的工具，同时保持灵活的实现更复杂的模型，包括你自己发明的模型。虽然本章不介绍任何新的模型或数据集，但后面的高级建模章节主要依赖于这些技术。\\n+\\n+```toc\\n+:maxdepth: 2\\n+\\n+model-construction\\n+parameters\\n+deferred-init\\n+custom-layer\\n+read-write\\n+use-gpu\\n+```\\ndiff --git a/chapter_deep-learning-computation/index_tencent.md b/chapter_deep-learning-computation/index_tencent.md\\nnew file mode 100644\\nindex 00000000..af8b6b96\\n--- /dev/null\\n+++ b/chapter_deep-learning-computation/index_tencent.md\\n@@ -0,0 +1,17 @@\\n+# 深度学习计算\\n+:label:`chap_computation`\\n+\\n+除了庞大的数据集和强大的硬件，伟大的软件工具在深度学习的快速发展中发挥了不可或缺的作用。从2007年发布的突破性的Theano库开始，灵活的开源工具使研究人员能够快速制作模型原型，避免了回收标准组件时的重复工作，同时仍然保持了进行低级修改的能力。随着时间的推移，深度学习的图书馆已经演变成提供越来越粗糙的抽象。就像半导体设计师从指定晶体管到逻辑电路再到编写代码一样，神经网络研究人员已经从考虑单个人工神经元的行为转移到从整层的角度构思网络，现在设计架构时往往考虑到要粗糙得多的*块*。\\n+\\n+到目前为止，我们已经介绍了一些基本的机器学习概念，逐步发展到功能齐全的深度学习模型。在上一章中，我们从头开始实现了MLP的每个组件，甚至展示了如何利用高级API毫不费力地推出相同的模型。为了让您走得那么快，我们“访问”了这些库，但是跳过了关于“它们是如何工作的”的更高级的细节。在本章中，我们将揭开帷幕，深入挖掘深度学习计算的关键组件，即模型构建、参数访问和初始化、设计自定义层和块、将模型读写到磁盘，以及利用GPU实现显著的加速。这些洞察力将使您从“最终用户”变为“高级用户”，为您提供所需的工具来获取成熟深度学习库的好处，同时保留实现更复杂模型(包括您自己发明的模型)的灵活性！虽然本章不介绍任何新的模型或数据集，但后面的高级建模章节在很大程度上依赖于这些技术。\\n+\\n+```toc\\n+:maxdepth: 2\\n+\\n+model-construction\\n+parameters\\n+deferred-init\\n+custom-layer\\n+read-write\\n+use-gpu\\n+```\\ndiff --git a/chapter_deep-learning-computation/model-construction_baidu.md b/chapter_deep-learning-computation/model-construction_baidu.md\\nnew file mode 100644\\nindex 00000000..316bfcba\\n--- /dev/null\\n+++ b/chapter_deep-learning-computation/model-construction_baidu.md\\n@@ -0,0 +1,452 @@\\n+# 层和块\\n+:label:`sec_model_construction`\\n+\\n+当我们第一次引入神经网络时，我们关注的是具有单一输出的线性模型。在这里，整个模型只由一个神经元组成。请注意，单个神经元（i）接受一些输入；（ii）生成相应的标量输出；（iii）具有一组相关参数，这些参数可以更新以优化某些感兴趣的目标函数。然后，一旦我们开始考虑具有多个输出的网络，我们就利用矢量化算法来描述整个神经元层。就像单个神经元一样，层（i）接受一组输入，（ii）生成相应的输出，（iii）由一组可调参数描述。当我们使用softmax回归时，一个单层本身就是模型。然而，即使我们随后引入了MLPs，我们仍然可以认为该模型保留了相同的基本结构。\\n+\\n+有趣的是，对于mlp，整个模型及其组成层都共享这种结构。整个模型接受原始输入（特征），生成输出（预测），并拥有参数（所有组成层的组合参数）。同样，每个单独的层接收输入（由前一层提供）生成输出（到下一层的输入），并且具有一组可调参数，这些参数根据从下一层向后流动的信号进行更新。\\n+\\n+虽然您可能认为神经元、层和模型为我们的业务提供了足够的抽象，但事实证明，我们经常发现谈论比单个层大但比整个模型小的组件更方便。例如，ResNet-152体系结构在计算机视觉中非常流行，它拥有数百层。这些层由*层*组*的重复模式组成。一次只实现一层这样的网络会变得很乏味。这种担心不仅仅是假设性的——这种设计模式在实践中很常见。上面提到的ResNet架构赢得了2015年ImageNet和COCO计算机视觉比赛的识别和检测:cite:`He.Zhang.Ren.ea.2016`，仍然是许多视觉任务的首选架构。在其他领域，包括自然语言处理和语音，类似的体系结构以不同的重复模式排列在一起。\\n+\\n+为了实现这些复杂的网络，我们引入了神经网络*块*的概念。块可以描述单个层、由多个层组成的组件或整个模型本身！使用块抽象的一个好处是可以将它们组合成更大的构件，通常是递归的。:numref:`fig_blocks`对此进行了说明。通过定义代码来按需生成任意复杂度的块，我们可以编写出奇紧凑的代码，同时仍然实现复杂的神经网络。\\n+\\n+![Multiple layers are combined into blocks, forming repeating patterns of larger models.](../img/blocks.svg)\\n+:label:`fig_blocks`\\n+\\n+从编程的角度来看，块由*类*表示。它的任何子类都必须定义一个将其输入转换为输出的前向传播函数，并且必须存储任何必需的参数。请注意，有些块根本不需要任何参数。最后，为了计算梯度，块必须具有反向传播函数。幸运的是，在定义我们自己的块时，由于自动微分（在:numref:`sec_autograd`中引入）提供了一些幕后魔术，我们只需要担心参数和前向传播函数。\\n+\\n+首先，我们回顾一下用于实现MLPs（:numref:`sec_mlp_concise`）的代码。下面的代码生成一个网络，其中一个完全连接的隐藏层有256个单元和ReLU激活，然后是一个具有10个单元的完全连接的输出层（没有激活功能）。\\n+\\n+```{.python .input}\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+net = nn.Sequential()\\n+net.add(nn.Dense(256, activation=\\'relu\\'))\\n+net.add(nn.Dense(10))\\n+net.initialize()\\n+\\n+X = np.random.uniform(size=(2, 20))\\n+net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+import torch\\n+from torch import nn\\n+from torch.nn import functional as F\\n+\\n+net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\\n+\\n+X = torch.rand(2, 20)\\n+net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+import tensorflow as tf\\n+\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Dense(256, activation=tf.nn.relu),\\n+    tf.keras.layers.Dense(10),\\n+])\\n+\\n+X = tf.random.uniform((2, 20))\\n+net(X)\\n+```\\n+\\n+:begin_tab:`mxnet`\\n+在这个例子中，我们通过实例化`nn.Sequential`来构建我们的模型，将返回的对象分配给`net`变量。接下来，我们反复调用它的`add`函数，按照应该执行的顺序添加层。简而言之，`nn.Sequential`定义了一种特殊类型的`Block`，这个类用胶子表示块。它维护一个有序的组成部分`Block`的列表，`add`函数只是方便将每个连续的`Block`添加到列表中。请注意，每个层都是`Dense`类的一个实例，该类本身就是`Block`的子类。前向传播（`forward`）函数也非常简单：它将列表中的每个`Block`链接在一起，将每个`Block`的输出作为下一个的输入传递给下一个。注意，到目前为止，我们一直在通过构造`net(X)`调用我们的模型来获得它们的输出。这实际上只是`net.forward(X)`的简写，这是通过`Block`类的`__call__`函数实现的一个巧妙的Python技巧。\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+在这个例子中，我们通过实例化一个`nn.Sequential`来构建我们的模型，层的执行顺序是作为参数传递的。简而言之，`nn.Sequential`定义了一种特殊类型的`Module`，该类在PyTorch中表示一个块。它维护了一个有序的组成部分`Module`的列表，注意两个完全连接的层中的每一个都是`Linear`类的一个实例，这个类本身就是`Module`的子类。前向传播（`forward`）函数也非常简单：它将列表中的每个块链接在一起，将每个块的输出作为下一个块的输入。注意，到目前为止，我们一直在通过构造`net(X)`调用我们的模型来获得它们的输出。这实际上只是`net.forward(X)`的简写，这是通过Block类的`__call__`函数实现的一个巧妙的Python技巧。\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+在这个例子中，我们通过实例化一个`keras.models.Sequential`来构建我们的模型，层的执行顺序是作为参数传递的。简而言之，`Sequential`定义了一种特殊类型的`keras.Model`，该类在Keras中表示一个块。它维护一个有序的组成部分`Model`s的列表，注意两个完全连接的层中的每一个都是`Model`类的一个实例，这个类本身就是`Model`的子类。前向传播（`call`）函数也非常简单：它将列表中的每个块链接在一起，将每个块的输出作为下一个块的输入。注意，到目前为止，我们一直在通过构造`net(X)`调用我们的模型来获得它们的输出。这实际上只是`net.call(X)`的简写，这是通过Block类的`__call__`函数实现的一个巧妙的Python技巧。\\n+:end_tab:\\n+\\n+## 自定义块\\n+\\n+也许发展关于块如何工作的直觉的最简单的方法是自己实现一个块。在实现我们自己的自定义块之前，我们简要总结一下每个块必须提供的基本功能：\\n+\\n+1. 将输入数据作为其前向传播函数的参数。\\n+1. 通过使前向传播函数返回值来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个完全连接的层接收任意维的输入，但是返回一个维度256的输出。\\n+1. 计算其输出相对于其输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。\\n+1. 存储并提供对执行前向传播计算所需的参数的访问。\\n+1. 根据需要初始化模型参数。\\n+\\n+在下面的代码片段中，我们从头开始编写一个块，对应于一个具有256个隐藏单元的隐藏层和一个10维输出层的MLP。注意，下面的`MLP`类继承了表示块的类。我们将严重依赖父类的函数，只提供我们自己的构造函数（Python中的`__init__`函数）和前向传播函数。\\n+\\n+```{.python .input}\\n+class MLP(nn.Block):\\n+    # Declare a layer with model parameters. Here, we declare two\\n+    # fully-connected layers\\n+    def __init__(self, **kwargs):\\n+        # Call the constructor of the `MLP` parent class `Block` to perform\\n+        # the necessary initialization. In this way, other function arguments\\n+        # can also be specified during class instantiation, such as the model\\n+        # parameters, `params` (to be described later)\\n+        super().__init__(**kwargs)\\n+        self.hidden = nn.Dense(256, activation=\\'relu\\')  # Hidden layer\\n+        self.out = nn.Dense(10)  # Output layer\\n+\\n+    # Define the forward propagation of the model, that is, how to return the\\n+    # required model output based on the input `X`\\n+    def forward(self, X):\\n+        return self.out(self.hidden(X))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class MLP(nn.Module):\\n+    # Declare a layer with model parameters. Here, we declare two fully\\n+    # connected layers\\n+    def __init__(self):\\n+        # Call the constructor of the `MLP` parent class `Block` to perform\\n+        # the necessary initialization. In this way, other function arguments\\n+        # can also be specified during class instantiation, such as the model\\n+        # parameters, `params` (to be described later)\\n+        super().__init__()\\n+        self.hidden = nn.Linear(20, 256)  # Hidden layer\\n+        self.out = nn.Linear(256, 10)  # Output layer\\n+\\n+    # Define the forward propagation of the model, that is, how to return the\\n+    # required model output based on the input `X`\\n+    def forward(self, X):\\n+        # Note here we use the funtional version of ReLU defined in the\\n+        # nn.functional module.\\n+        return self.out(F.relu(self.hidden(X)))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class MLP(tf.keras.Model):\\n+    # Declare a layer with model parameters. Here, we declare two fully\\n+    # connected layers\\n+    def __init__(self):\\n+        # Call the constructor of the `MLP` parent class `Block` to perform\\n+        # the necessary initialization. In this way, other function arguments\\n+        # can also be specified during class instantiation, such as the model\\n+        # parameters, `params` (to be described later)\\n+        super().__init__()\\n+        # Hidden layer\\n+        self.hidden = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\\n+        self.out = tf.keras.layers.Dense(units=10)  # Output layer\\n+\\n+    # Define the forward propagation of the model, that is, how to return the\\n+    # required model output based on the input `X`\\n+    def call(self, X):\\n+        return self.out(self.hidden((X)))\\n+```\\n+\\n+让我们首先关注前向传播函数。注意，它以`X`作为输入，计算应用激活函数的隐藏表示，并输出其逻辑。在这个`MLP`实现中，两个层都是实例变量。为了了解为什么这是合理的，设想实例化两个mlp，`net1`和`net2`，并在不同的数据上训练它们。当然，我们希望它们代表两种不同的学习模式。\\n+\\n+我们在构造函数中实例化MLP的层，然后在每次调用前向传播函数时调用这些层。注意一些关键细节。首先，我们定制的`__init__`函数通过`super().__init__()`调用父类的`__init__`函数，从而避免了重新编写适用于大多数块的样板代码的痛苦。然后我们实例化两个完全连接的层，将它们分配给`self.hidden`和`self.out`。注意，除非我们实现一个新的运算符，否则我们不必担心反向传播函数或参数初始化。系统将自动生成这些功能。让我们试试这个。\\n+\\n+```{.python .input}\\n+net = MLP()\\n+net.initialize()\\n+net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = MLP()\\n+net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = MLP()\\n+net(X)\\n+```\\n+\\n+块抽象的一个主要优点是它的多功能性。我们可以对块进行子类化以创建层（如完全连接的层类）、整个模型（如上面的`MLP`类）或具有中等复杂度的各种组件。我们在接下来的章节中充分利用了这种多功能性，比如在处理卷积神经网络时。\\n+\\n+## 顺序块\\n+\\n+现在我们可以更仔细地看看`Sequential`类是如何工作的。回想一下`Sequential`的设计是为了把其他模块串起来。为了构建我们自己的简化`MySequential`，我们只需要定义两个关键功能：\\n+1. 一种将块逐个追加到列表中的函数。\\n+2. 一种前向传播函数，用于将输入按与附加块相同的顺序传递给块链。\\n+\\n+下面的`MySequential`类提供了与默认`Sequential`类相同的功能。\\n+\\n+```{.python .input}\\n+class MySequential(nn.Block):\\n+    def add(self, block):\\n+        # Here, `block` is an instance of a `Block` subclass, and we assume \\n+        # that it has a unique name. We save it in the member variable\\n+        # `_children` of the `Block` class, and its type is OrderedDict. When\\n+        # the `MySequential` instance calls the `initialize` function, the\\n+        # system automatically initializes all members of `_children`\\n+        self._children[block.name] = block\\n+\\n+    def forward(self, X):\\n+        # OrderedDict guarantees that members will be traversed in the order\\n+        # they were added\\n+        for block in self._children.values():\\n+            X = block(X)\\n+        return X\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class MySequential(nn.Module):\\n+    def __init__(self, *args):\\n+        super().__init__()\\n+        for block in args:\\n+            # Here, `block` is an instance of a `Module` subclass. We save it\\n+            # in the member variable `_modules` of the `Module` class, and its\\n+            # type is OrderedDict\\n+            self._modules[block] = block\\n+\\n+    def forward(self, X):\\n+        # OrderedDict guarantees that members will be traversed in the order\\n+        # they were added\\n+        for block in self._modules.values():\\n+            X = block(X)\\n+        return X\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class MySequential(tf.keras.Model):\\n+    def __init__(self, *args):\\n+        super().__init__()\\n+        self.modules = []\\n+        for block in args:\\n+            # Here, `block` is an instance of a `tf.keras.layers.Layer`\\n+            # subclass\\n+            self.modules.append(block)\\n+\\n+    def call(self, X):\\n+        for module in self.modules:\\n+            X = module(X)\\n+        return X\\n+```\\n+\\n+:begin_tab:`mxnet`\\n+`add`函数向有序字典`_children`添加一个块。您可能会想知道为什么每个胶子`Block`都有一个`_children`属性，以及为什么我们使用它而不只是自己定义一个Python列表。简而言之，`_children`的主要优点是在我们的块的参数初始化过程中，Gluon知道在`_children`字典中查找参数也需要初始化的子块。\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+在`__init__`方法中，我们将每个块逐个添加到有序字典`_modules`中。您可能会想知道为什么每个`Module`都有一个`_modules`属性，以及为什么我们使用它而不只是自己定义一个Python列表。简而言之，`_modules`的主要优点是在我们的块参数初始化过程中，系统知道要在`_modules`字典中查找其参数也需要初始化的子块。\\n+:end_tab:\\n+\\n+当我们的`MySequential`的前向传播函数被调用时，每个添加的块都按照它们被添加的顺序执行。我们现在可以使用我们的`MySequential`类重新实现MLP。\\n+\\n+```{.python .input}\\n+net = MySequential()\\n+net.add(nn.Dense(256, activation=\\'relu\\'))\\n+net.add(nn.Dense(10))\\n+net.initialize()\\n+net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\\n+net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = MySequential(\\n+    tf.keras.layers.Dense(units=256, activation=tf.nn.relu),\\n+    tf.keras.layers.Dense(10))\\n+net(X)\\n+```\\n+\\n+注意，`MySequential`的这种用法与我们之前为`Sequential`类编写的代码相同（如:numref:`sec_mlp_concise`中所述）。\\n+\\n+## 在前向传播函数中执行代码\\n+\\n+`Sequential`类使模型构造变得简单，允许我们组装新的体系结构，而不必定义自己的类。然而，并不是所有的架构都是简单的菊花链。当需要更大的灵活性时，我们需要定义自己的块。例如，我们可能希望在前向传播函数中执行Python的控制流。此外，我们可能希望执行任意的数学运算，而不是简单地依赖预定义的神经网络层。\\n+\\n+您可能已经注意到，到目前为止，我们网络中的所有操作都对网络的激活及其参数起作用。然而，有时我们可能希望合并既不是前一层的结果也不是可更新参数的术语。我们称之为常数参数。例如，我们需要一个计算函数$f(\\\\mathbf{x},\\\\mathbf{w}) = c \\\\cdot \\\\mathbf{w}^\\\\top \\\\mathbf{x}$的层，其中$\\\\mathbf{x}$是输入，$\\\\mathbf{w}$是我们的参数，$c$是某个在优化过程中没有更新的指定常量。所以我们实现了一个`FixedHiddenMLP`类，如下所示。\\n+\\n+```{.python .input}\\n+class FixedHiddenMLP(nn.Block):\\n+    def __init__(self, **kwargs):\\n+        super().__init__(**kwargs)\\n+        # Random weight parameters created with the `get_constant` function\\n+        # are not updated during training (i.e., constant parameters)\\n+        self.rand_weight = self.params.get_constant(\\n+            \\'rand_weight\\', np.random.uniform(size=(20, 20)))\\n+        self.dense = nn.Dense(20, activation=\\'relu\\')\\n+\\n+    def forward(self, X):\\n+        X = self.dense(X)\\n+        # Use the created constant parameters, as well as the `relu` and `dot`\\n+        # functions\\n+        X = npx.relu(np.dot(X, self.rand_weight.data()) + 1)\\n+        # Reuse the fully-connected layer. This is equivalent to sharing\\n+        # parameters with two fully-connected layers\\n+        X = self.dense(X)\\n+        # Control flow\\n+        while np.abs(X).sum() > 1:\\n+            X /= 2\\n+        return X.sum()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class FixedHiddenMLP(nn.Module):\\n+    def __init__(self):\\n+        super().__init__()\\n+        # Random weight parameters that will not compute gradients and\\n+        # therefore keep constant during training\\n+        self.rand_weight = torch.rand((20, 20), requires_grad=False)\\n+        self.linear = nn.Linear(20, 20)\\n+\\n+    def forward(self, X):\\n+        X = self.linear(X)\\n+        # Use the created constant parameters, as well as the `relu` and `mm`\\n+        # functions\\n+        X = F.relu(torch.mm(X, self.rand_weight) + 1)\\n+        # Reuse the fully-connected layer. This is equivalent to sharing\\n+        # parameters with two fully-connected layers\\n+        X = self.linear(X)\\n+        # Control flow\\n+        while X.abs().sum() > 1:\\n+            X /= 2\\n+        return X.sum()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class FixedHiddenMLP(tf.keras.Model):\\n+    def __init__(self):\\n+        super().__init__()\\n+        self.flatten = tf.keras.layers.Flatten()\\n+        # Random weight parameters created with `tf.constant` are not updated\\n+        # during training (i.e., constant parameters)\\n+        self.rand_weight = tf.constant(tf.random.uniform((20, 20)))\\n+        self.dense = tf.keras.layers.Dense(20, activation=tf.nn.relu)\\n+\\n+    def call(self, inputs):\\n+        X = self.flatten(inputs)\\n+        # Use the created constant parameters, as well as the `relu` and\\n+        # `matmul` functions\\n+        X = tf.nn.relu(tf.matmul(X, self.rand_weight) + 1)\\n+        # Reuse the fully-connected layer. This is equivalent to sharing\\n+        # parameters with two fully-connected layers\\n+        X = self.dense(X)\\n+        # Control flow\\n+        while tf.reduce_sum(tf.math.abs(X)) > 1:\\n+            X /= 2\\n+        return tf.reduce_sum(X)\\n+```\\n+\\n+在这个`FixedHiddenMLP`模型中，我们实现了一个隐藏层，其权重（`self.rand_weight`）在实例化时被随机初始化，然后是常量。这个权重不是一个模型参数，因此它永远不会被反向传播更新。然后，网络将这个“固定”层的输出通过一个完全连接的层。\\n+\\n+注意，在返回输出之前，我们的模型做了一些不寻常的事情。我们运行了一个while循环，在$L_1$范数大于$1$的条件下进行测试，并将输出向量除以$2$，直到它满足条件为止。最后，我们在`X`中返回了条目的总和。据我们所知，没有标准的神经网络执行这种操作。请注意，此特定操作在任何实际任务中可能都没有用处。我们的重点只是向您展示如何将任意代码集成到神经网络计算的流程中。\\n+\\n+```{.python .input}\\n+net = FixedHiddenMLP()\\n+net.initialize()\\n+net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch, tensorflow\\n+net = FixedHiddenMLP()\\n+net(X)\\n+```\\n+\\n+我们可以混合和匹配各种组装块的方法。在下面的示例中，我们以一些创造性的方式嵌套块。\\n+\\n+```{.python .input}\\n+class NestMLP(nn.Block):\\n+    def __init__(self, **kwargs):\\n+        super().__init__(**kwargs)\\n+        self.net = nn.Sequential()\\n+        self.net.add(nn.Dense(64, activation=\\'relu\\'),\\n+                     nn.Dense(32, activation=\\'relu\\'))\\n+        self.dense = nn.Dense(16, activation=\\'relu\\')\\n+\\n+    def forward(self, X):\\n+        return self.dense(self.net(X))\\n+\\n+chimera = nn.Sequential()\\n+chimera.add(NestMLP(), nn.Dense(20), FixedHiddenMLP())\\n+chimera.initialize()\\n+chimera(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class NestMLP(nn.Module):\\n+    def __init__(self):\\n+        super().__init__()\\n+        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\\n+                                 nn.Linear(64, 32), nn.ReLU())\\n+        self.linear = nn.Linear(32, 16)\\n+\\n+    def forward(self, X):\\n+        return self.linear(self.net(X))\\n+\\n+chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\\n+chimera(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class NestMLP(tf.keras.Model):\\n+    def __init__(self):\\n+        super().__init__()\\n+        self.net = tf.keras.Sequential()\\n+        self.net.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\\n+        self.net.add(tf.keras.layers.Dense(32, activation=tf.nn.relu))\\n+        self.dense = tf.keras.layers.Dense(16, activation=tf.nn.relu)\\n+\\n+    def call(self, inputs):\\n+        return self.dense(self.net(inputs))\\n+\\n+chimera = tf.keras.Sequential()\\n+chimera.add(NestMLP())\\n+chimera.add(tf.keras.layers.Dense(20))\\n+chimera.add(FixedHiddenMLP())\\n+chimera(X)\\n+```\\n+\\n+## 汇编\\n+\\n+:begin_tab:`mxnet, tensorflow`\\n+热心的读者可能会开始担心其中一些操作的效率。毕竟，我们深度学习lib库，执行代码，还有很多其他的Python事情，它们都应该是一个高性能的深度学习库。Python的[global interpreter lock](https://wiki.python.org/moin/GlobalInterpreterLock)的问题是众所周知的。在深入学习的背景下，我们担心我们的极快的GPU可能要等到一个微不足道的CPU运行Python代码之后，才能运行另一个作业。加速Python的最好方法是完全避免它。\\n+:end_tab:\\n+\\n+:begin_tab:`mxnet`\\n+胶子这样做的一个方法是允许\\n+*杂交*，这将在后面描述。\\n+这里，Python解释器在第一次调用块时执行它。胶子运行时记录正在发生的事情，以及下一次它将对Python的调用短路。在某些情况下，这可以大大加快速度，但当控制流（如上所述）在不同的网络通道上引导不同的分支时，需要小心。我们建议感兴趣的读者在读完本章后，查看混合部分（:numref:`sec_hybridize`）来了解编译。\\n+:end_tab:\\n+\\n+## 摘要\\n+\\n+* 层就是块。\\n+* 许多层可以组成一个块。\\n+* 一个块可以由许多块组成。\\n+* 块可以包含代码。\\n+* 块负责大量的内务处理，包括参数初始化和反向传播。\\n+* 层和块的顺序连接由`Sequential`块处理。\\n+\\n+## 练习\\n+\\n+1. 如果将`MySequential`更改为在Python列表中存储块，会出现什么样的问题？\\n+1. 实现一个以两个块为参数的块，例如`net1`和`net2`，并返回前向传播中两个网络的级联输出。这也被称为并行块。\\n+1. 假设您想要连接同一网络的多个实例。实现一个工厂函数，该函数生成同一块的多个实例，并从中构建更大的网络。\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/54)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/55)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/264)\\n+:end_tab:\\ndiff --git a/chapter_deep-learning-computation/model-construction_tencent.md b/chapter_deep-learning-computation/model-construction_tencent.md\\nnew file mode 100644\\nindex 00000000..0a1bafb6\\n--- /dev/null\\n+++ b/chapter_deep-learning-computation/model-construction_tencent.md\\n@@ -0,0 +1,452 @@\\n+# 图层和块\\n+:label:`sec_model_construction`\\n+\\n+当我们第一次引入神经网络时，我们关注的是具有单一输出的线性模型。在这里，整个模型只由一个神经元组成。注意，单个神经元(I)接受某一组输入；(Ii)生成相应的标量输出；以及(Iii)具有一组可以更新以优化某些感兴趣的目标函数的相关参数。然后，一旦我们开始考虑具有多个输出的网络，我们就利用矢量化算法来表征整层神经元。就像单个神经元一样，层(I)接受一组输入，(Ii)产生相应的输出，(Iii)由一组可调参数描述。当我们使用SoftMax回归时，单层本身就是模型。然而，即使我们随后引入了MLP，我们仍然可以认为该模型保留了相同的基本结构。\\n+\\n+有趣的是，对于MLP，整个模型及其组成层都共享此结构。整个模型接受原始输入(特征)，生成输出(预测)，并拥有参数(来自所有组成层的组合参数)。同样，每个单独的层摄取输入(由前一层提供)生成输出(到后一层的输入)，并拥有一组根据从后一层向后流动的信号更新的可调参数。\\n+\\n+虽然您可能认为神经元、层和模型为我们的业务提供了足够的抽象，但事实证明，我们经常发现谈论比单个层大但比整个模型小的组件更方便。例如，在计算机视觉中广泛流行的ResNet-152体系结构就有数百层。这些层由*层组*的重复图案组成。一次实施一层这样的网络可能会变得单调乏味。这个问题不仅仅是假设的-这样的设计模式在实践中很常见。上述Resnet架构赢得了2015年ImageNet和COCO计算机视觉竞赛的识别和检测:cite:`He.Zhang.Ren.ea.2016`，并且仍然是许多视觉任务的首选架构。层以各种重复模式排列的类似体系结构现在在其他领域普遍存在，包括自然语言处理和语音。\\n+\\n+为了实现这些复杂的网络，我们引入了神经网络*挡路*的概念。挡路可以描述单层，可以描述由多层组成的组件，也可以描述整个模型本身！使用挡路抽象的一个好处是它们可以组合成更大的工件，通常是递归的。这一点在:numref:`fig_blocks`中进行了说明。通过定义代码来按需生成任意复杂度的块，我们可以编写出令人惊讶的紧凑代码，同时仍然可以实现复杂的神经网络。\\n+\\n+![Multiple layers are combined into blocks, forming repeating patterns of larger models.](../img/blocks.svg)\\n+:label:`fig_blocks`\\n+\\n+从编程的角度来看，挡路用*类*来表示。它的任何子类都必须定义将其输入转换为输出的前向传播函数，并且必须存储任何必要的参数。请注意，有些块根本不需要任何参数。最后，为了计算梯度，挡路必须具有反向传播函数。幸运的是，由于自动微分(:numref:`sec_autograd`引入)提供的一些幕后魔力，在定义我们自己的挡路时，我们只需要担心参数和前向传播函数。\\n+\\n+首先，我们回顾用于实现MLP的代码(:numref:`sec_mlp_concise`)。下面的代码生成一个具有256个单元的全连接隐藏层和REU激活的网络，然后是一个具有10个单元的全连接输出层(无激活功能)。\\n+\\n+```{.python .input}\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+net = nn.Sequential()\\n+net.add(nn.Dense(256, activation=\\'relu\\'))\\n+net.add(nn.Dense(10))\\n+net.initialize()\\n+\\n+X = np.random.uniform(size=(2, 20))\\n+net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+import torch\\n+from torch import nn\\n+from torch.nn import functional as F\\n+\\n+net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\\n+\\n+X = torch.rand(2, 20)\\n+net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+import tensorflow as tf\\n+\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Dense(256, activation=tf.nn.relu),\\n+    tf.keras.layers.Dense(10),\\n+])\\n+\\n+X = tf.random.uniform((2, 20))\\n+net(X)\\n+```\\n+\\n+:begin_tab:`mxnet`\\n+在本例中，我们通过实例化一个`nn.Sequential`，将返回的对象赋给`net`变量来构建我们的模型。接下来，我们重复调用它的`add`函数，按照应该执行的顺序附加层。简而言之，`nn.Sequential`定义了一种特殊的`Block`，即用胶子表示挡路的类。它维护成分`Block`s的有序列表。`add`功能简单地便于将每个连续的`Block`添加到列表。请注意，每一层都是`Dense`类的实例，而该类本身就是`Block`的子类。前向传播(`forward`)函数也非常简单：它将列表中的每个`Block`链接在一起，将每个的输出作为输入传递给下一个。注意，到目前为止，我们一直通过构造`net(X)`调用我们的模型以获得它们的输出。这实际上只是`net.forward(X)`的简写，这是通过`Block`类的`__call__`函数实现的一个巧妙的Python技巧。\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+在本例中，我们通过实例化一个`nn.Sequential`来构建我们的模型，这些层按照它们应该被执行的顺序作为参数传递。简而言之，`nn.Sequential`定义了一种特殊的`Module`，即在PyTorch中表示挡路的类。它维护成分`Module`的有序列表。请注意，两个完全连接的层中的每一个都是`Linear`类的实例，而该类本身就是`Module`的子类。前向传播(`forward`)函数也非常简单：它将列表中的每个挡路链接在一起，将每个挡路的输出作为输入传递给下一个。注意，到目前为止，我们一直通过构造`net(X)`调用我们的模型以获得它们的输出。这实际上只是`net.forward(X)`的简写，这是通过挡路类的`__call__`函数实现的一个巧妙的Python技巧。\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+在本例中，我们通过实例化一个`keras.models.Sequential`来构建我们的模型，这些层按照它们应该被执行的顺序作为参数传递。简而言之，`Sequential`定义了一种特殊的`keras.Model`，即在凯拉斯呈现挡路的类。它维护成分`Model`的有序列表。请注意，两个完全连接的层中的每一个都是`Dense`类的实例，而该类本身就是`Model`的子类。前向传播(`call`)函数也非常简单：它将列表中的每个挡路链接在一起，将每个挡路的输出作为输入传递给下一个。注意，到目前为止，我们一直通过构造`net(X)`调用我们的模型以获得它们的输出。这实际上只是`net.call(X)`的简写，这是通过挡路类的`__call__`函数实现的一个巧妙的Python技巧。\\n+:end_tab:\\n+\\n+## 自定义挡路\\n+\\n+要想直观地了解挡路是如何工作的，最简单的方法可能就是自己实现一个。在实施我们自己的自定义挡路之前，我们先简要总结一下每个挡路必须提供的基本功能：\\n+\\n+1. 将输入数据作为参数接收到其前向传播函数。\\n+1. 通过使前向传播函数返回值来生成输出。请注意，输出可能具有与输入不同的形状。例如，上面模型中的第一个完全连接层接受任意维度的输入，但返回维度256的输出。\\n+1. 计算其输出相对于其输入的梯度，该梯度可通过其反向传播功能访问。通常这是自动发生的。\\n+1. 存储并提供对执行前向传播计算所需的那些参数的访问。\\n+1. 根据需要初始化模型参数。\\n+\\n+在下面的代码片段中，我们从头开始编写一个挡路，对应于一个具有256万个隐藏单元的隐藏层和一个10维输出层。请注意，下面的`MLP`个类继承了表示挡路的类。我们将严重依赖父类的函数，只提供我们自己的构造函数(Python语言中的`__init__`函数)和前向传播函数。\\n+\\n+```{.python .input}\\n+class MLP(nn.Block):\\n+    # Declare a layer with model parameters. Here, we declare two\\n+    # fully-connected layers\\n+    def __init__(self, **kwargs):\\n+        # Call the constructor of the `MLP` parent class `Block` to perform\\n+        # the necessary initialization. In this way, other function arguments\\n+        # can also be specified during class instantiation, such as the model\\n+        # parameters, `params` (to be described later)\\n+        super().__init__(**kwargs)\\n+        self.hidden = nn.Dense(256, activation=\\'relu\\')  # Hidden layer\\n+        self.out = nn.Dense(10)  # Output layer\\n+\\n+    # Define the forward propagation of the model, that is, how to return the\\n+    # required model output based on the input `X`\\n+    def forward(self, X):\\n+        return self.out(self.hidden(X))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class MLP(nn.Module):\\n+    # Declare a layer with model parameters. Here, we declare two fully\\n+    # connected layers\\n+    def __init__(self):\\n+        # Call the constructor of the `MLP` parent class `Block` to perform\\n+        # the necessary initialization. In this way, other function arguments\\n+        # can also be specified during class instantiation, such as the model\\n+        # parameters, `params` (to be described later)\\n+        super().__init__()\\n+        self.hidden = nn.Linear(20, 256)  # Hidden layer\\n+        self.out = nn.Linear(256, 10)  # Output layer\\n+\\n+    # Define the forward propagation of the model, that is, how to return the\\n+    # required model output based on the input `X`\\n+    def forward(self, X):\\n+        # Note here we use the funtional version of ReLU defined in the\\n+        # nn.functional module.\\n+        return self.out(F.relu(self.hidden(X)))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class MLP(tf.keras.Model):\\n+    # Declare a layer with model parameters. Here, we declare two fully\\n+    # connected layers\\n+    def __init__(self):\\n+        # Call the constructor of the `MLP` parent class `Block` to perform\\n+        # the necessary initialization. In this way, other function arguments\\n+        # can also be specified during class instantiation, such as the model\\n+        # parameters, `params` (to be described later)\\n+        super().__init__()\\n+        # Hidden layer\\n+        self.hidden = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\\n+        self.out = tf.keras.layers.Dense(units=10)  # Output layer\\n+\\n+    # Define the forward propagation of the model, that is, how to return the\\n+    # required model output based on the input `X`\\n+    def call(self, X):\\n+        return self.out(self.hidden((X)))\\n+```\\n+\\n+让我们首先关注前向传播函数。请注意，它将`X`作为输入，计算应用了激活函数的隐藏表示，并输出其日志。在此`MLP`实现中，两个层都是实例变量。要了解这为什么是合理的，可以想象实例化两个MLP(`net1`和`net2`)，并根据不同的数据对它们进行训练。当然，我们希望它们代表两个不同的学习模型。\\n+\\n+我们在构造函数中实例化MLP的层，然后在每次调用转发传播函数时调用这些层。请注意几个关键细节。首先，我们定制的`__init__`函数通过`__init__`调用父类的`super().__init__()`函数，省去了重复适用于大多数块的样板代码的痛苦。然后，我们实例化两个完全连接的层，将它们分配给`self.hidden`和`self.out`。请注意，除非我们实现一个新操作符，否则我们不需要担心反向传播函数或参数初始化。系统将自动生成这些函数。让我们试试看吧。\\n+\\n+```{.python .input}\\n+net = MLP()\\n+net.initialize()\\n+net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = MLP()\\n+net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = MLP()\\n+net(X)\\n+```\\n+\\n+挡路抽象的一个关键优点是它的多功能性。我们可以子类化挡路来创建层(如全连通层类)、整个模型(如上面的`MLP`类)或各种中等复杂度的组件。我们将在接下来的章节中利用这种多功能性，例如在处理卷积神经网络时。\\n+\\n+## “挡路”系列丛书\\n+\\n+现在我们可以仔细看看`Sequential`级是如何工作的。回想一下，`Sequential`旨在将其他数据块以菊花链形式链接在一起。要构建我们自己的简化`MySequential`，我们只需要定义两个关键函数：\\n+1. 将块逐个附加到列表中的函数。\\n+2. 一种前向传播函数，用于以与附加输入的顺序相同的顺序，通过区块链传递输入。\\n+\\n+下面的`MySequential`类提供与默认`Sequential`类相同的功能。\\n+\\n+```{.python .input}\\n+class MySequential(nn.Block):\\n+    def add(self, block):\\n+        # Here, `block` is an instance of a `Block` subclass, and we assume \\n+        # that it has a unique name. We save it in the member variable\\n+        # `_children` of the `Block` class, and its type is OrderedDict. When\\n+        # the `MySequential` instance calls the `initialize` function, the\\n+        # system automatically initializes all members of `_children`\\n+        self._children[block.name] = block\\n+\\n+    def forward(self, X):\\n+        # OrderedDict guarantees that members will be traversed in the order\\n+        # they were added\\n+        for block in self._children.values():\\n+            X = block(X)\\n+        return X\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class MySequential(nn.Module):\\n+    def __init__(self, *args):\\n+        super().__init__()\\n+        for block in args:\\n+            # Here, `block` is an instance of a `Module` subclass. We save it\\n+            # in the member variable `_modules` of the `Module` class, and its\\n+            # type is OrderedDict\\n+            self._modules[block] = block\\n+\\n+    def forward(self, X):\\n+        # OrderedDict guarantees that members will be traversed in the order\\n+        # they were added\\n+        for block in self._modules.values():\\n+            X = block(X)\\n+        return X\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class MySequential(tf.keras.Model):\\n+    def __init__(self, *args):\\n+        super().__init__()\\n+        self.modules = []\\n+        for block in args:\\n+            # Here, `block` is an instance of a `tf.keras.layers.Layer`\\n+            # subclass\\n+            self.modules.append(block)\\n+\\n+    def call(self, X):\\n+        for module in self.modules:\\n+            X = module(X)\\n+        return X\\n+```\\n+\\n+:begin_tab:`mxnet`\\n+`add`函数将单个挡路添加到有序词典`_children`。您可能想知道为什么每个胶子`Block`都有一个`_children`属性，为什么我们要使用它，而不是自己定义一个Python列表。简而言之，`_children`的主要优势是，在我们挡路的参数初始化期间，胶子知道要在`_children`字典中查找参数也需要初始化的子块。\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+在`__init__`方法中，我们将每个挡路逐个添加到有序词典`_modules`中。您可能想知道为什么每个`Module`都有一个`_modules`属性，为什么我们要使用它，而不是自己定义一个Python列表。简而言之，`_modules`的主要优势在于，在我们挡路的参数初始化期间，系统知道要在`_modules`字典中查找参数也需要初始化的子块。\\n+:end_tab:\\n+\\n+当我们的`MySequential`的前向传播函数被调用时，每个添加的挡路都会按照它们被添加的顺序执行。我们现在可以使用`MySequential`类重新实现一个mlp。\\n+\\n+```{.python .input}\\n+net = MySequential()\\n+net.add(nn.Dense(256, activation=\\'relu\\'))\\n+net.add(nn.Dense(10))\\n+net.initialize()\\n+net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\\n+net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = MySequential(\\n+    tf.keras.layers.Dense(units=256, activation=tf.nn.relu),\\n+    tf.keras.layers.Dense(10))\\n+net(X)\\n+```\\n+\\n+请注意，`MySequential`的这种用法与我们之前为`Sequential`类编写的代码相同(如:numref:`sec_mlp_concise`中所述)。\\n+\\n+## 在前向传播函数中执行代码\\n+\\n+`Sequential`类使模型构建变得容易，允许我们组装新的体系结构，而不必定义我们自己的类。但是，并不是所有的架构都是简单的菊花链。当需要更大的灵活性时，我们会希望定义我们自己的块。例如，我们可能希望在前向传播函数中执行Python的控制流。此外，我们可能希望执行任意的数学运算，而不是简单地依赖预定义的神经网络层。\\n+\\n+您可能已经注意到，到目前为止，我们网络中的所有操作都是根据我们网络的激活及其参数进行操作的。但是，有时我们可能希望合并既不是先前层的结果也不是可更新参数的术语。我们称这些为“常数参数”。例如，假设我们想要一个计算函数$f(\\\\mathbf{x},\\\\mathbf{w}) = c \\\\cdot \\\\mathbf{w}^\\\\top \\\\mathbf{x}$的层，其中$\\\\mathbf{x}$是输入，$\\\\mathbf{w}$是我们的参数，$c$是在优化期间不更新的某个指定常量。因此，我们实现一个`FixedHiddenMLP`类，如下所示。\\n+\\n+```{.python .input}\\n+class FixedHiddenMLP(nn.Block):\\n+    def __init__(self, **kwargs):\\n+        super().__init__(**kwargs)\\n+        # Random weight parameters created with the `get_constant` function\\n+        # are not updated during training (i.e., constant parameters)\\n+        self.rand_weight = self.params.get_constant(\\n+            \\'rand_weight\\', np.random.uniform(size=(20, 20)))\\n+        self.dense = nn.Dense(20, activation=\\'relu\\')\\n+\\n+    def forward(self, X):\\n+        X = self.dense(X)\\n+        # Use the created constant parameters, as well as the `relu` and `dot`\\n+        # functions\\n+        X = npx.relu(np.dot(X, self.rand_weight.data()) + 1)\\n+        # Reuse the fully-connected layer. This is equivalent to sharing\\n+        # parameters with two fully-connected layers\\n+        X = self.dense(X)\\n+        # Control flow\\n+        while np.abs(X).sum() > 1:\\n+            X /= 2\\n+        return X.sum()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class FixedHiddenMLP(nn.Module):\\n+    def __init__(self):\\n+        super().__init__()\\n+        # Random weight parameters that will not compute gradients and\\n+        # therefore keep constant during training\\n+        self.rand_weight = torch.rand((20, 20), requires_grad=False)\\n+        self.linear = nn.Linear(20, 20)\\n+\\n+    def forward(self, X):\\n+        X = self.linear(X)\\n+        # Use the created constant parameters, as well as the `relu` and `mm`\\n+        # functions\\n+        X = F.relu(torch.mm(X, self.rand_weight) + 1)\\n+        # Reuse the fully-connected layer. This is equivalent to sharing\\n+        # parameters with two fully-connected layers\\n+        X = self.linear(X)\\n+        # Control flow\\n+        while X.abs().sum() > 1:\\n+            X /= 2\\n+        return X.sum()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class FixedHiddenMLP(tf.keras.Model):\\n+    def __init__(self):\\n+        super().__init__()\\n+        self.flatten = tf.keras.layers.Flatten()\\n+        # Random weight parameters created with `tf.constant` are not updated\\n+        # during training (i.e., constant parameters)\\n+        self.rand_weight = tf.constant(tf.random.uniform((20, 20)))\\n+        self.dense = tf.keras.layers.Dense(20, activation=tf.nn.relu)\\n+\\n+    def call(self, inputs):\\n+        X = self.flatten(inputs)\\n+        # Use the created constant parameters, as well as the `relu` and\\n+        # `matmul` functions\\n+        X = tf.nn.relu(tf.matmul(X, self.rand_weight) + 1)\\n+        # Reuse the fully-connected layer. This is equivalent to sharing\\n+        # parameters with two fully-connected layers\\n+        X = self.dense(X)\\n+        # Control flow\\n+        while tf.reduce_sum(tf.math.abs(X)) > 1:\\n+            X /= 2\\n+        return tf.reduce_sum(X)\\n+```\\n+\\n+在这个`FixedHiddenMLP`模型中，我们实现了一个隐藏层，其权重(`self.rand_weight`)在实例化时被随机初始化，并且此后是恒定的。该权重不是模型参数，因此不会通过反向传播进行更新。然后，网络将这个“固定”层的输出通过一个完全连接的层。\\n+\\n+请注意，在返回输出之前，我们的模型做了一些不寻常的事情。我们运行了WHILE循环，在其$L_1$范数大于$1$的条件下进行测试，并将输出向量除以$2$，直到它满足条件。最后，我们返回`X`中条目的总和。据我们所知，没有标准的神经网络执行这种操作。请注意，此特定操作可能在任何实际任务中都没有用处。我们的重点只是向您展示如何将任意代码集成到您的神经网络计算流程中。\\n+\\n+```{.python .input}\\n+net = FixedHiddenMLP()\\n+net.initialize()\\n+net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch, tensorflow\\n+net = FixedHiddenMLP()\\n+net(X)\\n+```\\n+\\n+我们可以混合搭配各种方式把积木组装在一起。在下面的示例中，我们以一些创造性的方式嵌套块。\\n+\\n+```{.python .input}\\n+class NestMLP(nn.Block):\\n+    def __init__(self, **kwargs):\\n+        super().__init__(**kwargs)\\n+        self.net = nn.Sequential()\\n+        self.net.add(nn.Dense(64, activation=\\'relu\\'),\\n+                     nn.Dense(32, activation=\\'relu\\'))\\n+        self.dense = nn.Dense(16, activation=\\'relu\\')\\n+\\n+    def forward(self, X):\\n+        return self.dense(self.net(X))\\n+\\n+chimera = nn.Sequential()\\n+chimera.add(NestMLP(), nn.Dense(20), FixedHiddenMLP())\\n+chimera.initialize()\\n+chimera(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class NestMLP(nn.Module):\\n+    def __init__(self):\\n+        super().__init__()\\n+        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\\n+                                 nn.Linear(64, 32), nn.ReLU())\\n+        self.linear = nn.Linear(32, 16)\\n+\\n+    def forward(self, X):\\n+        return self.linear(self.net(X))\\n+\\n+chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\\n+chimera(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class NestMLP(tf.keras.Model):\\n+    def __init__(self):\\n+        super().__init__()\\n+        self.net = tf.keras.Sequential()\\n+        self.net.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\\n+        self.net.add(tf.keras.layers.Dense(32, activation=tf.nn.relu))\\n+        self.dense = tf.keras.layers.Dense(16, activation=tf.nn.relu)\\n+\\n+    def call(self, inputs):\\n+        return self.dense(self.net(inputs))\\n+\\n+chimera = tf.keras.Sequential()\\n+chimera.add(NestMLP())\\n+chimera.add(tf.keras.layers.Dense(20))\\n+chimera.add(FixedHiddenMLP())\\n+chimera(X)\\n+```\\n+\\n+## 编译\\n+\\n+:begin_tab:`mxnet, tensorflow`\\n+狂热的读者可能会开始担心其中一些操作的效率。毕竟，我们在一个应该是高性能的深度学习库中进行了大量的字典查找、代码执行和许多其他Python式的事情。巨蟒[global interpreter lock](https://wiki.python.org/moin/GlobalInterpreterLock)的问题是众所周知的。在深度学习环境中，我们担心速度极快的GPU可能要等到微不足道的CPU运行Python代码后才能运行另一个作业。提高Python速度的最好方法是完全避免使用它。\\n+:end_tab:\\n+\\n+:begin_tab:`mxnet`\\n+胶子做到这一点的一种方式是允许\\n+*杂交*，这将在后面描述。\\n+在这里，Python解释器在第一次调用挡路时执行它。GLUON运行时记录正在发生的事情，并在下一次运行时缩短对Python的调用。在某些情况下，这可以大大加快速度，但是当控制流(如上所述)在不同通道上引导不同的分支通过网络时，需要注意这一点。我们建议感兴趣的读者在读完本章后查看杂交部分(:numref:`sec_hybridize`)来了解编译。\\n+:end_tab:\\n+\\n+## 摘要\\n+\\n+* 图层是块。\\n+* 许多层可以组成一个挡路。\\n+* 许多区块可以组成挡路。\\n+* 挡路可以包含代码。\\n+* 块负责大量内务工作，包括参数初始化和反向传播。\\n+* 层和块的顺序连接由`Sequential`挡路处理。\\n+\\n+## 练习\\n+\\n+1. 如果您将`MySequential`更改为在Python列表中存储块，会出现什么问题？\\n+1. 实现一个挡路，它以两个块作为参数，例如`net1`和`net2`，并在正向传播中返回两个网络的串联输出。这也叫平行挡路。\\n+1. 假设您要串联同一网络的多个实例。实现一个工厂函数，该函数可以生成同一挡路的多个实例，并在此基础上构建更大的网络。\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/54)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/55)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/264)\\n+:end_tab:\\ndiff --git a/chapter_deep-learning-computation/parameters_baidu.md b/chapter_deep-learning-computation/parameters_baidu.md\\nnew file mode 100644\\nindex 00000000..004db71d\\n--- /dev/null\\n+++ b/chapter_deep-learning-computation/parameters_baidu.md\\n@@ -0,0 +1,550 @@\\n+# 参数管理\\n+\\n+一旦我们选择了一个架构并设置了超参数，我们就进入了训练循环，我们的目标是找到使损失函数最小化的参数值。在训练之后，我们需要这些参数来进行未来的预测。此外，我们有时会希望提取参数，以便在其他上下文中重用它们，将我们的模型保存到磁盘以便可以在其他软件中执行，或者用于检查，以期获得科学的理解。\\n+\\n+大多数时候，我们将能够忽略参数如何声明和操作的基本细节，依靠深度学习框架来完成繁重的工作。然而，当我们离开带有标准层的层叠架构时，我们有时需要深入到声明和操作参数的麻烦中。在本节中，我们将介绍以下内容：\\n+\\n+* 访问用于调试、诊断和可视化的参数。\\n+* 参数初始化。\\n+* 在不同的模型构件之间共享参数。\\n+\\n+我们首先关注一个隐藏层的MLP。\\n+\\n+```{.python .input}\\n+from mxnet import init, np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+net = nn.Sequential()\\n+net.add(nn.Dense(8, activation=\\'relu\\'))\\n+net.add(nn.Dense(1))\\n+net.initialize()  # Use the default initialization method\\n+\\n+X = np.random.uniform(size=(2, 4))\\n+net(X)  # Forward computation\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+import torch\\n+from torch import nn\\n+\\n+net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\\n+X = torch.rand(size=(2, 4))\\n+net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+import tensorflow as tf\\n+import numpy as np\\n+\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Flatten(),\\n+    tf.keras.layers.Dense(4, activation=tf.nn.relu),\\n+    tf.keras.layers.Dense(1),\\n+])\\n+\\n+X = tf.random.uniform((2, 4))\\n+net(X)\\n+```\\n+\\n+## 参数访问\\n+\\n+让我们从如何从您已经知道的模型中访问参数开始。当一个模型通过`Sequential`类定义时，我们可以首先通过索引到模型中来访问任何层，就像它是一个列表一样。每个层的参数都可以方便地定位在其属性中。我们可以检查第二个完全连通层的参数，如下所示。\\n+\\n+```{.python .input}\\n+print(net[1].params)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+print(net[2].state_dict())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+print(net.layers[2].weights)\\n+```\\n+\\n+输出告诉我们一些重要的事情。首先，这个完全连接的层包含两个参数，分别对应于该层的权重和偏移。两者都存储为单精度浮点（float32）。请注意，参数的名称允许我们唯一地标识每个层的参数，即使在包含数百个层的网络中也是如此。\\n+\\n+### 目标参数\\n+\\n+请注意，每个参数都表示为参数类的一个实例。要对参数做任何有用的事情，我们首先需要访问底层的数值。有几种方法可以做到这一点。有些比较简单，有些则更一般。下面的代码从第二个神经网络层提取偏差，后者返回一个参数类实例，并进一步访问该参数的值。\\n+\\n+```{.python .input}\\n+print(type(net[1].bias))\\n+print(net[1].bias)\\n+print(net[1].bias.data())\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+print(type(net[2].bias))\\n+print(net[2].bias)\\n+print(net[2].bias.data)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+print(type(net.layers[2].weights[1]))\\n+print(net.layers[2].weights[1])\\n+print(tf.convert_to_tensor(net.layers[2].weights[1]))\\n+```\\n+\\n+:begin_tab:`mxnet,pytorch`\\n+参数是复杂的对象，包含值、渐变和其他信息。这就是为什么我们需要显式地请求值。\\n+\\n+除了值之外，每个参数还允许我们访问渐变。因为我们还没有为这个网络调用反向传播，所以它处于初始状态。\\n+:end_tab:\\n+\\n+```{.python .input}\\n+net[1].weight.grad()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net[2].weight.grad == None\\n+```\\n+\\n+### 一次所有参数\\n+\\n+当我们需要对所有参数执行操作时，一个接一个地访问它们可能会变得很乏味。当我们处理更复杂的块（例如，嵌套块）时，这种情况会变得特别棘手，因为我们需要递归整个树来提取每个子块的参数。下面我们将演示如何访问第一个完全连接层的参数与访问所有层的参数。\\n+\\n+```{.python .input}\\n+print(net[0].collect_params())\\n+print(net.collect_params())\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+print(*[(name, param.shape) for name, param in net[0].named_parameters()])\\n+print(*[(name, param.shape) for name, param in net.named_parameters()])\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+print(net.layers[1].weights)\\n+print(net.get_weights())\\n+```\\n+\\n+这为我们提供了另一种访问网络参数的方法，如下所示。\\n+\\n+```{.python .input}\\n+net.collect_params()[\\'dense1_bias\\'].data()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net.state_dict()[\\'2.bias\\'].data\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net.get_weights()[1]\\n+```\\n+\\n+### 从嵌套块收集参数\\n+\\n+让我们看看，如果我们在彼此内部嵌套多个块，参数命名约定是如何工作的。为此，我们首先定义一个生成块的函数（可以说是块工厂），然后在更大的块中组合这些块。\\n+\\n+```{.python .input}\\n+def block1():\\n+    net = nn.Sequential()\\n+    net.add(nn.Dense(32, activation=\\'relu\\'))\\n+    net.add(nn.Dense(16, activation=\\'relu\\'))\\n+    return net\\n+\\n+def block2():\\n+    net = nn.Sequential()\\n+    for _ in range(4):\\n+        # Nested here\\n+        net.add(block1())\\n+    return net\\n+\\n+rgnet = nn.Sequential()\\n+rgnet.add(block2())\\n+rgnet.add(nn.Dense(10))\\n+rgnet.initialize()\\n+rgnet(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def block1():\\n+    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\\n+                         nn.Linear(8, 4), nn.ReLU())\\n+\\n+def block2():\\n+    net = nn.Sequential()\\n+    for i in range(4):\\n+        # Nested here\\n+        net.add_module(f\\'block {i}\\', block1())\\n+    return net\\n+\\n+rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\\n+rgnet(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def block1(name):\\n+    return tf.keras.Sequential([\\n+        tf.keras.layers.Flatten(),\\n+        tf.keras.layers.Dense(4, activation=tf.nn.relu)],\\n+        name=name)\\n+\\n+def block2():\\n+    net = tf.keras.Sequential()\\n+    for i in range(4):\\n+        # Nested here\\n+        net.add(block1(name=f\\'block-{i}\\'))\\n+    return net\\n+\\n+rgnet = tf.keras.Sequential()\\n+rgnet.add(block2())\\n+rgnet.add(tf.keras.layers.Dense(1))\\n+rgnet(X)\\n+```\\n+\\n+现在我们已经设计了网络，让我们看看它是如何组织的。\\n+\\n+```{.python .input}\\n+print(rgnet.collect_params)\\n+print(rgnet.collect_params())\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+print(rgnet)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+print(rgnet.summary())\\n+```\\n+\\n+由于层是分层嵌套的，所以我们也可以访问它们，就像通过嵌套列表建立索引一样。例如，我们可以访问第一个主块，其中的第二个子块，以及第一层的偏移，如下所示。\\n+\\n+```{.python .input}\\n+rgnet[0][1][0].bias.data()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+rgnet[0][1][0].bias.data\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+rgnet.layers[0].layers[1].layers[1].weights[1]\\n+```\\n+\\n+## 参数初始化\\n+\\n+既然我们知道了如何访问参数，那么让我们看看如何正确初始化它们。我们在:numref:`sec_numerical_stability`中讨论了正确初始化的必要性。深度学习框架为其层提供默认的随机初始化。然而，我们通常希望根据各种其他协议初始化权重。该框架提供了最常用的协议，还允许创建自定义初始值设定项。\\n+\\n+:begin_tab:`mxnet`\\n+默认情况下，MXNet通过从均匀分布中随机提取$U(-0.07, 0.07)$初始化权重参数，将偏差参数清除为零。MXNet的`init`模块提供了各种预置的初始化方法。\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+默认情况下，PyTorch通过从根据输入和输出维度计算的范围中绘制来统一初始化权重和偏差矩阵。Pythorch的`nn.init`模块提供了各种预置的初始化方法。\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+默认情况下，Keras通过从根据输入和输出维度计算的范围中绘制来统一初始化权重矩阵，并且偏差参数都设置为零。TensorFlow在根模块和`keras.initializers`模块中提供了多种初始化方法。\\n+:end_tab:\\n+\\n+### 内置初始化\\n+\\n+让我们首先调用内置的初始化器。下面的代码将所有权重参数初始化为标准偏差为0.01的高斯随机变量，而偏差参数被清除为零。\\n+\\n+```{.python .input}\\n+# Here `force_reinit` ensures that parameters are freshly initialized even if\\n+# they were already initialized previously\\n+net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)\\n+net[0].weight.data()[0]\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def init_normal(m):\\n+    if type(m) == nn.Linear:\\n+        nn.init.normal_(m.weight, mean=0, std=0.01)\\n+        nn.init.zeros_(m.bias)\\n+net.apply(init_normal)\\n+net[0].weight.data[0], net[0].bias.data[0]\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Flatten(),\\n+    tf.keras.layers.Dense(\\n+        4, activation=tf.nn.relu,\\n+        kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.01),\\n+        bias_initializer=tf.zeros_initializer()),\\n+    tf.keras.layers.Dense(1)])\\n+\\n+net(X)\\n+net.weights[0], net.weights[1]\\n+```\\n+\\n+我们还可以将所有参数初始化为给定的常量值（比如1）。\\n+\\n+```{.python .input}\\n+net.initialize(init=init.Constant(1), force_reinit=True)\\n+net[0].weight.data()[0]\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def init_constant(m):\\n+    if type(m) == nn.Linear:\\n+        nn.init.constant_(m.weight, 1)\\n+        nn.init.zeros_(m.bias)\\n+net.apply(init_constant)\\n+net[0].weight.data[0], net[0].bias.data[0]\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Flatten(),\\n+    tf.keras.layers.Dense(\\n+        4, activation=tf.nn.relu,\\n+        kernel_initializer=tf.keras.initializers.Constant(1),\\n+        bias_initializer=tf.zeros_initializer()),\\n+    tf.keras.layers.Dense(1),\\n+])\\n+\\n+net(X)\\n+net.weights[0], net.weights[1]\\n+```\\n+\\n+我们还可以为某些块应用不同的初始值设定项。例如，下面我们用Xavier初始化器初始化第一层，并将第二层初始化为常量值42。\\n+\\n+```{.python .input}\\n+net[0].weight.initialize(init=init.Xavier(), force_reinit=True)\\n+net[1].initialize(init=init.Constant(42), force_reinit=True)\\n+print(net[0].weight.data()[0])\\n+print(net[1].weight.data())\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def xavier(m):\\n+    if type(m) == nn.Linear:\\n+        torch.nn.init.xavier_uniform_(m.weight)\\n+def init_42(m):\\n+    if type(m) == nn.Linear:\\n+        torch.nn.init.constant_(m.weight, 42)\\n+\\n+net[0].apply(xavier)\\n+net[2].apply(init_42)\\n+print(net[0].weight.data[0])\\n+print(net[2].weight.data)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Flatten(),\\n+    tf.keras.layers.Dense(\\n+        4,\\n+        activation=tf.nn.relu,\\n+        kernel_initializer=tf.keras.initializers.GlorotUniform()),\\n+    tf.keras.layers.Dense(\\n+        1, kernel_initializer=tf.keras.initializers.Constant(1)),\\n+])\\n+\\n+net(X)\\n+print(net.layers[1].weights[0])\\n+print(net.layers[2].weights[0])\\n+```\\n+\\n+### 自定义初始化\\n+\\n+有时，我们需要的初始化方法不是由深度学习框架提供的。在下面的示例中，我们使用以下奇怪分布为任何权重参数$w$定义初始值设定项：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+    w \\\\sim \\\\begin{cases}\\n+        U(5, 10) & \\\\text{ with probability } \\\\frac{1}{4} \\\\\\\\\\n+            0    & \\\\text{ with probability } \\\\frac{1}{2} \\\\\\\\\\n+        U(-10, -5) & \\\\text{ with probability } \\\\frac{1}{4}\\n+    \\\\end{cases}\\n+\\\\end{aligned}\\n+$$\\n+\\n+:begin_tab:`mxnet`\\n+这里我们定义了`Initializer`类的一个子类。通常，我们只需要实现`_init_weight`函数，该函数接受张量参数（`data`）并为其分配所需的初始化值。\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+同样，我们实现了一个`my_init`函数来应用于`net`。\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+这里我们定义了`Initializer`的一个子类，并实现了`__call__`函数，该函数返回给定形状和数据类型的期望张量。\\n+:end_tab:\\n+\\n+```{.python .input}\\n+class MyInit(init.Initializer):\\n+    def _init_weight(self, name, data):\\n+        print(\\'Init\\', name, data.shape)\\n+        data[:] = np.random.uniform(-10, 10, data.shape)\\n+        data *= np.abs(data) >= 5\\n+\\n+net.initialize(MyInit(), force_reinit=True)\\n+net[0].weight.data()[:2]\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def my_init(m):\\n+    if type(m) == nn.Linear:\\n+        print(\"Init\", *[(name, param.shape) \\n+                        for name, param in m.named_parameters()][0])\\n+        nn.init.uniform_(m.weight, -10, 10)\\n+        m.weight.data *= m.weight.data.abs() >= 5\\n+\\n+net.apply(my_init)\\n+net[0].weight[:2]\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class MyInit(tf.keras.initializers.Initializer):\\n+    def __call__(self, shape, dtype=None):\\n+        return tf.random.uniform(shape, dtype=dtype)\\n+\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Flatten(),\\n+    tf.keras.layers.Dense(\\n+        4,\\n+        activation=tf.nn.relu,\\n+        kernel_initializer=MyInit()),\\n+    tf.keras.layers.Dense(1),\\n+])\\n+\\n+net(X)\\n+print(net.layers[1].weights[0])\\n+```\\n+\\n+请注意，我们始终可以选择直接设置参数。\\n+\\n+```{.python .input}\\n+net[0].weight.data()[:] += 1\\n+net[0].weight.data()[0, 0] = 42\\n+net[0].weight.data()[0]\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net[0].weight.data[:] += 1\\n+net[0].weight.data[0, 0] = 42\\n+net[0].weight.data[0]\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net.layers[1].weights[0][:].assign(net.layers[1].weights[0] + 1)\\n+net.layers[1].weights[0][0, 0].assign(42)\\n+net.layers[1].weights[0]\\n+```\\n+\\n+:begin_tab:`mxnet`\\n+高级用户注意：如果要在`autograd`范围内调整参数，则需要使用`set_data`以避免混淆自动微分机制。\\n+:end_tab:\\n+\\n+## 约束参数\\n+\\n+通常，我们希望跨多个层共享参数。让我们看看如何优雅地做这件事。下面我们分配一个稠密层，然后使用它的参数来设置另一层的参数。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+# We need to give the shared layer a name so that we can refer to its\\n+# parameters\\n+shared = nn.Dense(8, activation=\\'relu\\')\\n+net.add(nn.Dense(8, activation=\\'relu\\'),\\n+        shared,\\n+        nn.Dense(8, activation=\\'relu\\', params=shared.params),\\n+        nn.Dense(10))\\n+net.initialize()\\n+\\n+X = np.random.uniform(size=(2, 20))\\n+net(X)\\n+\\n+# Check whether the parameters are the same\\n+print(net[1].weight.data()[0] == net[2].weight.data()[0])\\n+net[1].weight.data()[0, 0] = 100\\n+# Make sure that they are actually the same object rather than just having the\\n+# same value\\n+print(net[1].weight.data()[0] == net[2].weight.data()[0])\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+# We need to give the shared layer a name so that we can refer to its\\n+# parameters\\n+shared = nn.Linear(8, 8)\\n+net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\\n+                    shared, nn.ReLU(),\\n+                    shared, nn.ReLU(),\\n+                    nn.Linear(8, 1))\\n+net(X)\\n+# Check whether the parameters are the same\\n+print(net[2].weight.data[0] == net[4].weight.data[0])\\n+net[2].weight.data[0, 0] = 100\\n+# Make sure that they are actually the same object rather than just having the\\n+# same value\\n+print(net[2].weight.data[0] == net[4].weight.data[0])\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+# tf.keras behaves a bit differently. It removes the duplicate layer\\n+# automatically\\n+shared = tf.keras.layers.Dense(4, activation=tf.nn.relu)\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Flatten(),\\n+    shared,\\n+    shared,\\n+    tf.keras.layers.Dense(1),\\n+])\\n+\\n+net(X)\\n+# Check whether the parameters are different\\n+print(len(net.layers) == 3)\\n+```\\n+\\n+:begin_tab:`mxnet,pytorch`\\n+此示例显示第二层和第三层的参数是绑定的。它们不仅是相等的，它们是用相同的张量表示的。因此，如果我们改变其中一个参数，另一个参数也会改变。您可能会想知道，当参数绑定时，渐变会发生什么？由于模型参数包含梯度，所以在反向传播过程中，第二个隐藏层和第三个隐藏层的梯度被加在一起。\\n+:end_tab:\\n+\\n+## 摘要\\n+\\n+* 我们有几种方法来访问、初始化和绑定模型参数。\\n+* 我们可以使用自定义初始化。\\n+\\n+## 练习\\n+\\n+1. 使用:numref:`sec_model_construction`中定义的`FancyMLP`模型，访问各个层的参数。\\n+1. 查看初始化模块文档以了解不同的初始化器。\\n+1. 构造一个包含共享参数层的MLP并对其进行训练。在训练过程中，观察各层模型参数和梯度。\\n+1. 为什么共享参数是个好主意？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/56)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/57)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/269)\\n+:end_tab:\\ndiff --git a/chapter_deep-learning-computation/parameters_tencent.md b/chapter_deep-learning-computation/parameters_tencent.md\\nnew file mode 100644\\nindex 00000000..f16f36aa\\n--- /dev/null\\n+++ b/chapter_deep-learning-computation/parameters_tencent.md\\n@@ -0,0 +1,550 @@\\n+# 参数管理\\n+\\n+一旦我们选择了架构并设置了超参数，我们就进入训练循环，在那里我们的目标是找到使损失函数最小化的参数值。经过训练后，我们将需要这些参数来做出未来的预测。此外，我们有时希望提取参数以便在其他上下文中重用它们，将我们的模型保存到磁盘以便它可以在其他软件中执行，或者为了获得科学理解而进行检查。\\n+\\n+大多数情况下，我们可以忽略如何声明和操作参数的具体细节，而是依靠深度学习框架来完成繁重的工作。然而，当我们离开具有标准层的堆叠架构时，我们有时需要进入声明和操作参数的杂草中。在本节中，我们将介绍以下内容：\\n+\\n+* 访问用于调试、诊断和可视化的参数。\\n+* 参数初始化。\\n+* 在不同模型构件之间共享参数。\\n+\\n+我们首先关注具有一个隐藏层的MLP。\\n+\\n+```{.python .input}\\n+from mxnet import init, np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+net = nn.Sequential()\\n+net.add(nn.Dense(8, activation=\\'relu\\'))\\n+net.add(nn.Dense(1))\\n+net.initialize()  # Use the default initialization method\\n+\\n+X = np.random.uniform(size=(2, 4))\\n+net(X)  # Forward computation\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+import torch\\n+from torch import nn\\n+\\n+net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\\n+X = torch.rand(size=(2, 4))\\n+net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+import tensorflow as tf\\n+import numpy as np\\n+\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Flatten(),\\n+    tf.keras.layers.Dense(4, activation=tf.nn.relu),\\n+    tf.keras.layers.Dense(1),\\n+])\\n+\\n+X = tf.random.uniform((2, 4))\\n+net(X)\\n+```\\n+\\n+## 参数访问\\n+\\n+让我们从如何从您已经知道的模型中访问参数开始。当通过`Sequential`类定义模型时，我们可以首先通过索引模型来访问任何层，就像它是一个列表一样。每层的参数都方便地位于其属性中。我们可以检查第二个完全连接层的参数，如下所示。\\n+\\n+```{.python .input}\\n+print(net[1].params)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+print(net[2].state_dict())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+print(net.layers[2].weights)\\n+```\\n+\\n+输出告诉我们一些重要的事情。首先，这个完全连接的层包含两个参数，分别对应于该层的权重和偏移。两者都存储为单精度浮点数(Float32)。请注意，参数名称允许我们唯一地标识每个层的参数，即使在包含数百个层的网络中也是如此。\\n+\\n+### 目标参数\\n+\\n+请注意，每个参数都表示为PARAMETER类的一个实例。要对参数执行任何有用的操作，我们首先需要访问底层的数值。有几种方法可以做到这一点。有些比较简单，而另一些则比较笼统。下面的代码从返回参数类实例的第二个神经网络层提取偏差，并进一步访问该参数的值。\\n+\\n+```{.python .input}\\n+print(type(net[1].bias))\\n+print(net[1].bias)\\n+print(net[1].bias.data())\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+print(type(net[2].bias))\\n+print(net[2].bias)\\n+print(net[2].bias.data)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+print(type(net.layers[2].weights[1]))\\n+print(net.layers[2].weights[1])\\n+print(tf.convert_to_tensor(net.layers[2].weights[1]))\\n+```\\n+\\n+:begin_tab:`mxnet,pytorch`\\n+参数是复杂的对象，包含值、渐变和附加信息。这就是为什么我们需要显式请求值的原因。\\n+\\n+除了值之外，每个参数还允许我们访问渐变。由于我们尚未调用此网络的反向传播，因此它处于初始状态。\\n+:end_tab:\\n+\\n+```{.python .input}\\n+net[1].weight.grad()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net[2].weight.grad == None\\n+```\\n+\\n+### 一次使用所有参数\\n+\\n+当我们需要对所有参数执行操作时，逐个访问它们可能会变得单调乏味。当我们处理更复杂的块(例如，嵌套块)时，情况可能会变得特别笨拙，因为我们需要递归整个树来提取每个子挡路的参数。下面，我们将演示访问第一个完全连接层的参数与访问所有层的比较。\\n+\\n+```{.python .input}\\n+print(net[0].collect_params())\\n+print(net.collect_params())\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+print(*[(name, param.shape) for name, param in net[0].named_parameters()])\\n+print(*[(name, param.shape) for name, param in net.named_parameters()])\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+print(net.layers[1].weights)\\n+print(net.get_weights())\\n+```\\n+\\n+这为我们提供了另一种访问网络参数的方式，如下所示。\\n+\\n+```{.python .input}\\n+net.collect_params()[\\'dense1_bias\\'].data()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net.state_dict()[\\'2.bias\\'].data\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net.get_weights()[1]\\n+```\\n+\\n+### 从嵌套块收集参数\\n+\\n+让我们看看如果我们将多个块相互嵌套，参数命名约定是如何工作的。为此，我们首先定义一个生成块(可以说是挡路工厂)的函数，然后将这些块组合到更大的块中。\\n+\\n+```{.python .input}\\n+def block1():\\n+    net = nn.Sequential()\\n+    net.add(nn.Dense(32, activation=\\'relu\\'))\\n+    net.add(nn.Dense(16, activation=\\'relu\\'))\\n+    return net\\n+\\n+def block2():\\n+    net = nn.Sequential()\\n+    for _ in range(4):\\n+        # Nested here\\n+        net.add(block1())\\n+    return net\\n+\\n+rgnet = nn.Sequential()\\n+rgnet.add(block2())\\n+rgnet.add(nn.Dense(10))\\n+rgnet.initialize()\\n+rgnet(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def block1():\\n+    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\\n+                         nn.Linear(8, 4), nn.ReLU())\\n+\\n+def block2():\\n+    net = nn.Sequential()\\n+    for i in range(4):\\n+        # Nested here\\n+        net.add_module(f\\'block {i}\\', block1())\\n+    return net\\n+\\n+rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\\n+rgnet(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def block1(name):\\n+    return tf.keras.Sequential([\\n+        tf.keras.layers.Flatten(),\\n+        tf.keras.layers.Dense(4, activation=tf.nn.relu)],\\n+        name=name)\\n+\\n+def block2():\\n+    net = tf.keras.Sequential()\\n+    for i in range(4):\\n+        # Nested here\\n+        net.add(block1(name=f\\'block-{i}\\'))\\n+    return net\\n+\\n+rgnet = tf.keras.Sequential()\\n+rgnet.add(block2())\\n+rgnet.add(tf.keras.layers.Dense(1))\\n+rgnet(X)\\n+```\\n+\\n+现在我们已经设计了网络，让我们看看它是如何组织的。\\n+\\n+```{.python .input}\\n+print(rgnet.collect_params)\\n+print(rgnet.collect_params())\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+print(rgnet)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+print(rgnet.summary())\\n+```\\n+\\n+因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。例如，我们可以访问第一个主要的挡路，在它里面有第二个子挡路，在这个里面第一层的偏向如下。\\n+\\n+```{.python .input}\\n+rgnet[0][1][0].bias.data()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+rgnet[0][1][0].bias.data\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+rgnet.layers[0].layers[1].layers[1].weights[1]\\n+```\\n+\\n+## 参数初始化\\n+\\n+现在我们知道了如何访问参数，让我们看看如何正确地初始化它们。我们在:numref:`sec_numerical_stability`中讨论了正确初始化的必要性。深度学习框架向其层提供默认随机初始化。然而，我们经常希望根据各种其他协议初始化权重。该框架提供了最常用的协议，还允许创建自定义初始值设定项。\\n+\\n+:begin_tab:`mxnet`\\n+默认情况下，mxnet通过从均匀分布$U(-0.07, 0.07)$随机抽取，将偏差参数清除为零来初始化权重参数。MXnet的`init`模块提供了多种预置初始化方法。\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+默认情况下，PyTorch通过从根据输入和输出尺寸计算的范围进行绘制，统一初始化权重矩阵和偏移矩阵。PyTorch的`nn.init`模块提供了多种预置初始化方法。\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+默认情况下，KERAS通过从根据输入和输出尺寸计算的范围中绘制来统一初始化权重矩阵，并且偏移参数都设置为零。TensorFlow在根模块和`keras.initializers`模块中都提供了各种初始化方法。\\n+:end_tab:\\n+\\n+### 内置初始化\\n+\\n+让我们首先调用内置的初始值设定项。下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，而偏差参数清除为零。\\n+\\n+```{.python .input}\\n+# Here `force_reinit` ensures that parameters are freshly initialized even if\\n+# they were already initialized previously\\n+net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)\\n+net[0].weight.data()[0]\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def init_normal(m):\\n+    if type(m) == nn.Linear:\\n+        nn.init.normal_(m.weight, mean=0, std=0.01)\\n+        nn.init.zeros_(m.bias)\\n+net.apply(init_normal)\\n+net[0].weight.data[0], net[0].bias.data[0]\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Flatten(),\\n+    tf.keras.layers.Dense(\\n+        4, activation=tf.nn.relu,\\n+        kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.01),\\n+        bias_initializer=tf.zeros_initializer()),\\n+    tf.keras.layers.Dense(1)])\\n+\\n+net(X)\\n+net.weights[0], net.weights[1]\\n+```\\n+\\n+我们还可以将所有参数初始化为给定的常量值(比方说，1)。\\n+\\n+```{.python .input}\\n+net.initialize(init=init.Constant(1), force_reinit=True)\\n+net[0].weight.data()[0]\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def init_constant(m):\\n+    if type(m) == nn.Linear:\\n+        nn.init.constant_(m.weight, 1)\\n+        nn.init.zeros_(m.bias)\\n+net.apply(init_constant)\\n+net[0].weight.data[0], net[0].bias.data[0]\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Flatten(),\\n+    tf.keras.layers.Dense(\\n+        4, activation=tf.nn.relu,\\n+        kernel_initializer=tf.keras.initializers.Constant(1),\\n+        bias_initializer=tf.zeros_initializer()),\\n+    tf.keras.layers.Dense(1),\\n+])\\n+\\n+net(X)\\n+net.weights[0], net.weights[1]\\n+```\\n+\\n+我们还可以对某些块应用不同的初始值设定项。例如，下面我们使用Xavier初始化器初始化第一层，并将第二层初始化为常量值42。\\n+\\n+```{.python .input}\\n+net[0].weight.initialize(init=init.Xavier(), force_reinit=True)\\n+net[1].initialize(init=init.Constant(42), force_reinit=True)\\n+print(net[0].weight.data()[0])\\n+print(net[1].weight.data())\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def xavier(m):\\n+    if type(m) == nn.Linear:\\n+        torch.nn.init.xavier_uniform_(m.weight)\\n+def init_42(m):\\n+    if type(m) == nn.Linear:\\n+        torch.nn.init.constant_(m.weight, 42)\\n+\\n+net[0].apply(xavier)\\n+net[2].apply(init_42)\\n+print(net[0].weight.data[0])\\n+print(net[2].weight.data)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Flatten(),\\n+    tf.keras.layers.Dense(\\n+        4,\\n+        activation=tf.nn.relu,\\n+        kernel_initializer=tf.keras.initializers.GlorotUniform()),\\n+    tf.keras.layers.Dense(\\n+        1, kernel_initializer=tf.keras.initializers.Constant(1)),\\n+])\\n+\\n+net(X)\\n+print(net.layers[1].weights[0])\\n+print(net.layers[2].weights[0])\\n+```\\n+\\n+### 自定义初始化\\n+\\n+有时，深度学习框架没有提供我们需要的初始化方法。在下面的示例中，我们使用以下奇怪的分布为任意权重参数$w$定义初始值设定项：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+    w \\\\sim \\\\begin{cases}\\n+        U(5, 10) & \\\\text{ with probability } \\\\frac{1}{4} \\\\\\\\\\n+            0    & \\\\text{ with probability } \\\\frac{1}{2} \\\\\\\\\\n+        U(-10, -5) & \\\\text{ with probability } \\\\frac{1}{4}\\n+    \\\\end{cases}\\n+\\\\end{aligned}\\n+$$\\n+\\n+:begin_tab:`mxnet`\\n+在这里，我们定义了`Initializer`类的子类。通常，我们只需要实现`_init_weight`函数，该函数接受张量参数(`data`)并为其分配所需的初始化值。\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+同样，我们实现了一个`my_init`函数来应用到`net`。\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+在这里，我们定义了一个`Initializer`的子类，并实现了返回给定形状和数据类型所需张量的`__call__`函数。\\n+:end_tab:\\n+\\n+```{.python .input}\\n+class MyInit(init.Initializer):\\n+    def _init_weight(self, name, data):\\n+        print(\\'Init\\', name, data.shape)\\n+        data[:] = np.random.uniform(-10, 10, data.shape)\\n+        data *= np.abs(data) >= 5\\n+\\n+net.initialize(MyInit(), force_reinit=True)\\n+net[0].weight.data()[:2]\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def my_init(m):\\n+    if type(m) == nn.Linear:\\n+        print(\"Init\", *[(name, param.shape) \\n+                        for name, param in m.named_parameters()][0])\\n+        nn.init.uniform_(m.weight, -10, 10)\\n+        m.weight.data *= m.weight.data.abs() >= 5\\n+\\n+net.apply(my_init)\\n+net[0].weight[:2]\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class MyInit(tf.keras.initializers.Initializer):\\n+    def __call__(self, shape, dtype=None):\\n+        return tf.random.uniform(shape, dtype=dtype)\\n+\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Flatten(),\\n+    tf.keras.layers.Dense(\\n+        4,\\n+        activation=tf.nn.relu,\\n+        kernel_initializer=MyInit()),\\n+    tf.keras.layers.Dense(1),\\n+])\\n+\\n+net(X)\\n+print(net.layers[1].weights[0])\\n+```\\n+\\n+请注意，我们始终可以选择直接设置参数。\\n+\\n+```{.python .input}\\n+net[0].weight.data()[:] += 1\\n+net[0].weight.data()[0, 0] = 42\\n+net[0].weight.data()[0]\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net[0].weight.data[:] += 1\\n+net[0].weight.data[0, 0] = 42\\n+net[0].weight.data[0]\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net.layers[1].weights[0][:].assign(net.layers[1].weights[0] + 1)\\n+net.layers[1].weights[0][0, 0].assign(42)\\n+net.layers[1].weights[0]\\n+```\\n+\\n+:begin_tab:`mxnet`\\n+高级用户请注意：如果要在`autograd`范围内调整参数，则需要使用`set_data`，以避免混淆自动区分机制。\\n+:end_tab:\\n+\\n+## 捆绑参数\\n+\\n+通常，我们希望在多个层之间共享参数。让我们看看如何优雅地做这件事。在下面，我们分配一个致密层，然后专门使用它的参数来设置另一个层的参数。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+# We need to give the shared layer a name so that we can refer to its\\n+# parameters\\n+shared = nn.Dense(8, activation=\\'relu\\')\\n+net.add(nn.Dense(8, activation=\\'relu\\'),\\n+        shared,\\n+        nn.Dense(8, activation=\\'relu\\', params=shared.params),\\n+        nn.Dense(10))\\n+net.initialize()\\n+\\n+X = np.random.uniform(size=(2, 20))\\n+net(X)\\n+\\n+# Check whether the parameters are the same\\n+print(net[1].weight.data()[0] == net[2].weight.data()[0])\\n+net[1].weight.data()[0, 0] = 100\\n+# Make sure that they are actually the same object rather than just having the\\n+# same value\\n+print(net[1].weight.data()[0] == net[2].weight.data()[0])\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+# We need to give the shared layer a name so that we can refer to its\\n+# parameters\\n+shared = nn.Linear(8, 8)\\n+net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\\n+                    shared, nn.ReLU(),\\n+                    shared, nn.ReLU(),\\n+                    nn.Linear(8, 1))\\n+net(X)\\n+# Check whether the parameters are the same\\n+print(net[2].weight.data[0] == net[4].weight.data[0])\\n+net[2].weight.data[0, 0] = 100\\n+# Make sure that they are actually the same object rather than just having the\\n+# same value\\n+print(net[2].weight.data[0] == net[4].weight.data[0])\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+# tf.keras behaves a bit differently. It removes the duplicate layer\\n+# automatically\\n+shared = tf.keras.layers.Dense(4, activation=tf.nn.relu)\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Flatten(),\\n+    shared,\\n+    shared,\\n+    tf.keras.layers.Dense(1),\\n+])\\n+\\n+net(X)\\n+# Check whether the parameters are different\\n+print(len(net.layers) == 3)\\n+```\\n+\\n+:begin_tab:`mxnet,pytorch`\\n+此示例显示第二层和第三层的参数是绑定的。它们不仅相等，而且由相同的张量表示。因此，如果我们改变其中一个参数，另一个参数也会改变。您可能会想，当参数绑定时，渐变会发生什么情况？由于模型参数包含梯度，因此在反向传播期间将第二隐层和第三隐层的梯度相加在一起。\\n+:end_tab:\\n+\\n+## 摘要\\n+\\n+* 我们有几种方法可以访问、初始化和绑定模型参数。\\n+* 我们可以使用自定义初始化。\\n+\\n+## 练习\\n+\\n+1. 使用`FancyMLP`中定义的:numref:`sec_model_construction`模型并访问各个层的参数。\\n+1. 查看初始化模块文档以了解不同的初始化器。\\n+1. 构建包含共享参数层的MLP并对其进行训练。在训练过程中，观察各层的模型参数和梯度。\\n+1. 为什么共享参数是个好主意？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/56)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/57)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/269)\\n+:end_tab:\\ndiff --git a/chapter_deep-learning-computation/read-write_baidu.md b/chapter_deep-learning-computation/read-write_baidu.md\\nnew file mode 100644\\nindex 00000000..4c632f89\\n--- /dev/null\\n+++ b/chapter_deep-learning-computation/read-write_baidu.md\\n@@ -0,0 +1,238 @@\\n+# 文件I/O\\n+\\n+到目前为止，我们讨论了如何处理数据以及如何构建、培训和测试深度学习模型。然而，在某个时刻，我们希望对所学的模型足够满意，我们希望保存结果以备将来在各种环境中使用（甚至可能在部署中进行预测）。此外，当运行一个长时间的培训过程时，最佳实践是定期保存中间结果（检查点），以确保在服务器电源线被绊倒时不会损失几天的计算量。因此，现在是时候学习如何加载和存储单独的权重向量和整个模型。本节讨论这两个问题。\\n+\\n+## 加载和保存张量\\n+\\n+对于单个张量，我们可以直接调用`load`和`save`函数分别读写它们。这两个函数都要求我们提供一个名称，`save`要求将要保存的变量作为输入。\\n+\\n+```{.python .input}\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+x = np.arange(4)\\n+npx.save(\\'x-file\\', x)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+import torch\\n+from torch import nn\\n+from torch.nn import functional as F\\n+\\n+x = torch.arange(4)\\n+torch.save(x, \\'x-file\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+import tensorflow as tf\\n+import numpy as np\\n+\\n+x = tf.range(4)\\n+np.save(\"x-file.npy\", x)\\n+```\\n+\\n+我们现在可以将存储文件中的数据读回内存。\\n+\\n+```{.python .input}\\n+x2 = npx.load(\\'x-file\\')\\n+x2\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+x2 = torch.load(\"x-file\")\\n+x2\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+x2 = np.load(\\'x-file.npy\\', allow_pickle=True)\\n+x2\\n+```\\n+\\n+我们可以存储一个张量列表，然后把它们读回内存。\\n+\\n+```{.python .input}\\n+y = np.zeros(4)\\n+npx.save(\\'x-files\\', [x, y])\\n+x2, y2 = npx.load(\\'x-files\\')\\n+(x2, y2)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+y = torch.zeros(4)\\n+torch.save([x, y],\\'x-files\\')\\n+x2, y2 = torch.load(\\'x-files\\')\\n+(x2, y2)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+y = tf.zeros(4)\\n+np.save(\\'xy-files.npy\\', [x, y])\\n+x2, y2 = np.load(\\'xy-files.npy\\', allow_pickle=True)\\n+(x2, y2)\\n+```\\n+\\n+我们甚至可以编写和阅读从字符串映射到张量的字典。当我们要读取或写入模型中的所有权重时，这很方便。\\n+\\n+```{.python .input}\\n+mydict = {\\'x\\': x, \\'y\\': y}\\n+npx.save(\\'mydict\\', mydict)\\n+mydict2 = npx.load(\\'mydict\\')\\n+mydict2\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+mydict = {\\'x\\': x, \\'y\\': y}\\n+torch.save(mydict, \\'mydict\\')\\n+mydict2 = torch.load(\\'mydict\\')\\n+mydict2\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+mydict = {\\'x\\': x, \\'y\\': y}\\n+np.save(\\'mydict.npy\\', mydict)\\n+mydict2 = np.load(\\'mydict.npy\\', allow_pickle=True)\\n+mydict2\\n+```\\n+\\n+## 加载和保存模型参数\\n+\\n+保存单个权重向量（或其他张量）是有用的，但是如果我们想保存（并在以后加载）整个模型，则会变得非常乏味。毕竟，我们可能有数百个参数组散布在各处。因此，深度学习框架提供了内置功能来加载和保存整个网络。需要注意的一个重要细节是，这将保存模型*参数*而不是整个模型。例如，如果我们有一个3层MLP，我们需要单独指定体系结构。原因是模型本身可以包含任意代码，因此它们不能自然地序列化。因此，为了恢复模型，我们需要用代码生成体系结构，然后从磁盘加载参数。让我们从我们熟悉的MLP开始。\\n+\\n+```{.python .input}\\n+class MLP(nn.Block):\\n+    def __init__(self, **kwargs):\\n+        super(MLP, self).__init__(**kwargs)\\n+        self.hidden = nn.Dense(256, activation=\\'relu\\')\\n+        self.output = nn.Dense(10)\\n+\\n+    def forward(self, x):\\n+        return self.output(self.hidden(x))\\n+\\n+net = MLP()\\n+net.initialize()\\n+X = np.random.uniform(size=(2, 20))\\n+Y = net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class MLP(nn.Module):\\n+    def __init__(self):\\n+        super().__init__()\\n+        self.hidden = nn.Linear(20, 256)\\n+        self.output = nn.Linear(256, 10)\\n+\\n+    def forward(self, x):\\n+        return self.output(F.relu(self.hidden(x)))\\n+\\n+net = MLP()\\n+X = torch.randn(size=(2, 20))\\n+Y = net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class MLP(tf.keras.Model):\\n+    def __init__(self):\\n+        super().__init__()\\n+        self.flatten = tf.keras.layers.Flatten()\\n+        self.hidden = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\\n+        self.out = tf.keras.layers.Dense(units=10)\\n+\\n+    def call(self, inputs):\\n+        x = self.flatten(inputs)\\n+        x = self.hidden(x)\\n+        return self.out(x)\\n+\\n+net = MLP()\\n+X = tf.random.uniform((2, 20))\\n+Y = net(X)\\n+```\\n+\\n+接下来，我们将模型的参数存储为一个名为“mlp参数\".\\n+\\n+```{.python .input}\\n+net.save_parameters(\\'mlp.params\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+torch.save(net.state_dict(), \\'mlp.params\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net.save_weights(\\'mlp.params\\')\\n+```\\n+\\n+为了恢复模型，我们实例化了原始MLP模型的一个克隆。我们没有随机初始化模型参数，而是直接读取文件中存储的参数。\\n+\\n+```{.python .input}\\n+clone = MLP()\\n+clone.load_parameters(\\'mlp.params\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+clone = MLP()\\n+clone.load_state_dict(torch.load(\"mlp.params\"))\\n+clone.eval()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+clone = MLP()\\n+clone.load_weights(\"mlp.params\")\\n+```\\n+\\n+由于两个实例具有相同的模型参数，相同输入`X`的计算结果应该相同。让我们来验证一下。\\n+\\n+```{.python .input}\\n+Y_clone = clone(X)\\n+Y_clone == Y\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+Y_clone = clone(X)\\n+Y_clone == Y\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+Y_clone = clone(X)\\n+Y_clone == Y\\n+```\\n+\\n+## 摘要\\n+\\n+* `save`和`load`函数可用于执行张量对象的文件I/O。\\n+* 我们可以通过参数字典保存和加载网络的全部参数集。\\n+* 保存架构必须在代码中完成，而不是在参数中。\\n+\\n+## 练习\\n+\\n+1. 即使不需要将经过训练的模型部署到不同的设备上，存储模型参数的实际好处是什么？\\n+1. 假设我们只想重用网络的一部分，以将其合并到不同体系结构的网络中。你会如何使用，比如说在一个新的网络中使用前两层网络？\\n+1. 如何保存网络架构和参数？你会对架构施加什么限制？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/60)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/61)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/327)\\n+:end_tab:\\ndiff --git a/chapter_deep-learning-computation/read-write_tencent.md b/chapter_deep-learning-computation/read-write_tencent.md\\nnew file mode 100644\\nindex 00000000..5fec5b3c\\n--- /dev/null\\n+++ b/chapter_deep-learning-computation/read-write_tencent.md\\n@@ -0,0 +1,238 @@\\n+# 文件I/O\\n+\\n+到目前为止，我们讨论了如何处理数据以及如何构建、训练和测试深度学习模型。然而，在某一时刻，我们希望对学习到的模型足够满意，以便保存结果以供以后在各种上下文中使用(甚至可能在部署中进行预测)。此外，在运行较长的培训过程时，最佳实践是定期保存中间结果(检查点)，以确保如果我们在服务器的电源线上绊倒，我们不会损失几天的计算时间。因此，现在是学习如何加载和存储单个权重向量和整个模型的时候了。本节将解决这两个问题。\\n+\\n+## 加载和保存张量\\n+\\n+对于单个张量，我们可以直接调用`load`和`save`函数来分别读取和写入它们。这两个函数都要求我们提供一个名称，而`save`要求将变量作为输入保存。\\n+\\n+```{.python .input}\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+x = np.arange(4)\\n+npx.save(\\'x-file\\', x)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+import torch\\n+from torch import nn\\n+from torch.nn import functional as F\\n+\\n+x = torch.arange(4)\\n+torch.save(x, \\'x-file\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+import tensorflow as tf\\n+import numpy as np\\n+\\n+x = tf.range(4)\\n+np.save(\"x-file.npy\", x)\\n+```\\n+\\n+现在，我们可以将存储文件中的数据读回内存。\\n+\\n+```{.python .input}\\n+x2 = npx.load(\\'x-file\\')\\n+x2\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+x2 = torch.load(\"x-file\")\\n+x2\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+x2 = np.load(\\'x-file.npy\\', allow_pickle=True)\\n+x2\\n+```\\n+\\n+我们可以存储张量列表，并将它们读回内存。\\n+\\n+```{.python .input}\\n+y = np.zeros(4)\\n+npx.save(\\'x-files\\', [x, y])\\n+x2, y2 = npx.load(\\'x-files\\')\\n+(x2, y2)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+y = torch.zeros(4)\\n+torch.save([x, y],\\'x-files\\')\\n+x2, y2 = torch.load(\\'x-files\\')\\n+(x2, y2)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+y = tf.zeros(4)\\n+np.save(\\'xy-files.npy\\', [x, y])\\n+x2, y2 = np.load(\\'xy-files.npy\\', allow_pickle=True)\\n+(x2, y2)\\n+```\\n+\\n+我们甚至可以编写和阅读从字符串映射到张量的字典。当我们想要读取或写入模型中的所有权重时，这是很方便的。\\n+\\n+```{.python .input}\\n+mydict = {\\'x\\': x, \\'y\\': y}\\n+npx.save(\\'mydict\\', mydict)\\n+mydict2 = npx.load(\\'mydict\\')\\n+mydict2\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+mydict = {\\'x\\': x, \\'y\\': y}\\n+torch.save(mydict, \\'mydict\\')\\n+mydict2 = torch.load(\\'mydict\\')\\n+mydict2\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+mydict = {\\'x\\': x, \\'y\\': y}\\n+np.save(\\'mydict.npy\\', mydict)\\n+mydict2 = np.load(\\'mydict.npy\\', allow_pickle=True)\\n+mydict2\\n+```\\n+\\n+## 加载和保存模型参数\\n+\\n+保存单个权重向量(或其他张量)很有用，但如果我们要保存(并在以后加载)整个模型，则会变得非常单调乏味。毕竟，我们可能有数百个参数组散布在各处。为此，深度学习框架提供了加载和保存整个网络的内置功能。需要注意的一个重要细节是，这将保存模型*参数*，而不是整个模型。例如，如果我们有一个3层的MLP，我们需要单独指定架构。这是因为模型本身可以包含任意代码，因此它们不能自然地序列化。因此，为了恢复模型，我们需要在代码中生成体系结构，然后从磁盘加载参数。让我们从我们熟悉的MLP开始。\\n+\\n+```{.python .input}\\n+class MLP(nn.Block):\\n+    def __init__(self, **kwargs):\\n+        super(MLP, self).__init__(**kwargs)\\n+        self.hidden = nn.Dense(256, activation=\\'relu\\')\\n+        self.output = nn.Dense(10)\\n+\\n+    def forward(self, x):\\n+        return self.output(self.hidden(x))\\n+\\n+net = MLP()\\n+net.initialize()\\n+X = np.random.uniform(size=(2, 20))\\n+Y = net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+class MLP(nn.Module):\\n+    def __init__(self):\\n+        super().__init__()\\n+        self.hidden = nn.Linear(20, 256)\\n+        self.output = nn.Linear(256, 10)\\n+\\n+    def forward(self, x):\\n+        return self.output(F.relu(self.hidden(x)))\\n+\\n+net = MLP()\\n+X = torch.randn(size=(2, 20))\\n+Y = net(X)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+class MLP(tf.keras.Model):\\n+    def __init__(self):\\n+        super().__init__()\\n+        self.flatten = tf.keras.layers.Flatten()\\n+        self.hidden = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\\n+        self.out = tf.keras.layers.Dense(units=10)\\n+\\n+    def call(self, inputs):\\n+        x = self.flatten(inputs)\\n+        x = self.hidden(x)\\n+        return self.out(x)\\n+\\n+net = MLP()\\n+X = tf.random.uniform((2, 20))\\n+Y = net(X)\\n+```\\n+\\n+接下来，我们将模型的参数存储为名为“mlp.params”的文件。\\n+\\n+```{.python .input}\\n+net.save_parameters(\\'mlp.params\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+torch.save(net.state_dict(), \\'mlp.params\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net.save_weights(\\'mlp.params\\')\\n+```\\n+\\n+为了恢复模型，我们实例化了原始MLP模型的克隆。我们不是随机初始化模型参数，而是直接读取文件中存储的参数。\\n+\\n+```{.python .input}\\n+clone = MLP()\\n+clone.load_parameters(\\'mlp.params\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+clone = MLP()\\n+clone.load_state_dict(torch.load(\"mlp.params\"))\\n+clone.eval()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+clone = MLP()\\n+clone.load_weights(\"mlp.params\")\\n+```\\n+\\n+由于两个实例具有相同的模型参数，因此相同输入`X`的计算结果应该是相同的。让我们验证一下这一点。\\n+\\n+```{.python .input}\\n+Y_clone = clone(X)\\n+Y_clone == Y\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+Y_clone = clone(X)\\n+Y_clone == Y\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+Y_clone = clone(X)\\n+Y_clone == Y\\n+```\\n+\\n+## 摘要\\n+\\n+* `save`和`load`函数可用于执行张量对象的文件I/O。\\n+* 我们可以通过参数字典保存和加载网络的整个参数集。\\n+* 保存体系结构必须在代码中完成，而不是在参数中完成。\\n+\\n+## 练习\\n+\\n+1. 即使不需要将经过训练的模型部署到不同的设备，存储模型参数的实际好处是什么？\\n+1. 假设我们只想重用要合并到不同架构的网络中的网络的一部分。比如说，您将如何在新网络中使用以前网络的前两层？\\n+1. 您将如何着手保存网络架构和参数？您会对架构施加哪些限制？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/60)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/61)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/327)\\n+:end_tab:\\ndiff --git a/chapter_deep-learning-computation/use-gpu_baidu.md b/chapter_deep-learning-computation/use-gpu_baidu.md\\nnew file mode 100644\\nindex 00000000..eb5309c3\\n--- /dev/null\\n+++ b/chapter_deep-learning-computation/use-gpu_baidu.md\\n@@ -0,0 +1,343 @@\\n+# GPU\\n+:label:`sec_use_gpu`\\n+\\n+在:numref:`tab_intro_decade`中，我们讨论了过去20年中计算的快速增长。简而言之，自2000年以来，GPU性能每十年增长1000倍。这提供了巨大的机会，但也表明需要提供这样的业绩。\\n+\\n+在本节中，我们开始讨论如何利用这种计算性能进行研究。首先是使用单个gpu，然后是如何使用多个gpu和多个服务器（具有多个gpu）。\\n+\\n+具体来说，我们将讨论如何使用单个NVIDIA GPU进行计算。首先，确保至少安装了一个NVIDIA GPU。然后，下载[NVIDIA driver and CUDA](https://developer.nvidia.com/cuda-downloads)并按照提示设置适当的路径。一旦这些准备工作完成，就可以使用`nvidia-smi`命令来查看图形卡信息。\\n+\\n+```{.python .input}\\n+#@tab all\\n+!nvidia-smi\\n+```\\n+\\n+:begin_tab:`mxnet`\\n+您可能已经注意到MXNet张量看起来与numpy`ndarray`几乎相同。但有几个关键的区别。MXNet与NumPy的一个关键特性是它支持不同的硬件设备。\\n+\\n+在MXNet中，每个数组都有一个上下文。到目前为止，默认情况下，所有变量和相关的计算都分配给CPU。通常，其他上下文可能是各种gpu。当我们跨多个服务器部署作业时，事情会变得更加棘手。通过智能地将数组分配给上下文，我们可以最大限度地减少在设备之间传输数据的时间。例如，当在带有GPU的服务器上训练神经网络时，我们通常希望模型的参数在GPU上。\\n+\\n+接下来，我们需要确认是否安装了MXNet的GPU版本。如果已经安装了MXNet的CPU版本，我们需要先卸载它。例如，使用`pip uninstall mxnet`命令，然后根据您的CUDA版本安装相应的MXNet版本。假设您已经安装了CUDA10.0，您可以通过`pip install mxnet-cu100`安装支持CUDA10.0的MXNet版本。\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+在PyTorch中，每个数组都有一个设备，我们通常将其称为上下文。到目前为止，默认情况下，所有变量和相关的计算都分配给CPU。通常，其他上下文可能是各种gpu。当我们跨多个服务器部署作业时，事情会变得更加棘手。通过智能地将数组分配给上下文，我们可以最大限度地减少在设备之间传输数据的时间。例如，当在带有GPU的服务器上训练神经网络时，我们通常希望模型的参数在GPU上。\\n+\\n+接下来，我们需要确认安装了GPU版本的PyTorch。如果已经安装了Pythorch的CPU版本，我们需要先卸载它。例如，使用`pip uninstall torch`命令，然后根据您的CUDA版本安装相应的PyTorch版本。假设您安装了CUDA10.0，您可以通过`pip install torch-cu100`安装支持CUDA10.0的Pythorch版本。\\n+:end_tab:\\n+\\n+要运行此部分中的程序，至少需要两个GPU。请注意，对于大多数桌面计算机来说，这可能是奢侈的，但在云中很容易获得，例如，通过使用awsec2多GPU实例。几乎所有其他部分都不需要多个gpu。相反，这只是为了说明数据如何在不同的设备之间流动。\\n+\\n+## 计算设备\\n+\\n+我们可以指定用于存储和计算的设备，如cpu和gpu。默认情况下，张量是在主内存中创建的，然后使用CPU计算它。\\n+\\n+:begin_tab:`mxnet`\\n+在MXNet中，CPU和GPU可以用`cpu()`和`gpu()`表示。需要注意的是，`cpu()`（或括号中的任何整数）表示所有物理CPU和内存。这意味着MXNet的计算将尝试使用所有CPU核心。然而，`gpu()`只代表一个卡和相应的存储器。如果有多个GPU，我们使用`gpu(i)`表示$i^\\\\mathrm{th}$ GPU（$i$从0开始）。另外，`gpu(0)`和`gpu()`是等效的。\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+在Pythorch中，CPU和GPU可以用`torch.device(\\'cpu\\')`和`torch.cuda.device(\\'cuda\\')`表示。应该注意的是，`cpu`设备意味着所有物理CPU和内存。这意味着Pythorch的计算将尝试使用所有CPU核心。然而，`gpu`设备只代表一个卡和相应的存储器。如果有多个GPU，我们使用`torch.cuda.device(f\\'cuda:{i}\\')`来表示$i^\\\\mathrm{th}$ GPU（$i$从0开始）。此外，`gpu:0`和`gpu`是等效的。\\n+:end_tab:\\n+\\n+```{.python .input}\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+npx.cpu(), npx.gpu(), npx.gpu(1)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+import torch\\n+from torch import nn\\n+\\n+torch.device(\\'cpu\\'), torch.cuda.device(\\'cuda\\'), torch.cuda.device(\\'cuda:1\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+import tensorflow as tf\\n+\\n+tf.device(\\'/CPU:0\\'), tf.device(\\'/GPU:0\\'), tf.device(\\'/GPU:1\\')\\n+```\\n+\\n+我们可以查询可用gpu的数量。\\n+\\n+```{.python .input}\\n+npx.num_gpus()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+torch.cuda.device_count()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+len(tf.config.experimental.list_physical_devices(\\'GPU\\'))\\n+```\\n+\\n+现在我们定义了两个方便的函数，它们允许我们在请求的gpu不存在的情况下运行代码。\\n+\\n+```{.python .input}\\n+def try_gpu(i=0):  #@save\\n+    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\\n+    return npx.gpu(i) if npx.num_gpus() >= i + 1 else npx.cpu()\\n+\\n+def try_all_gpus():  #@save\\n+    \"\"\"Return all available GPUs, or [cpu()] if no GPU exists.\"\"\"\\n+    devices = [npx.gpu(i) for i in range(npx.num_gpus())]\\n+    return devices if devices else [npx.cpu()]\\n+\\n+try_gpu(), try_gpu(10), try_all_gpus()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def try_gpu(i=0):  #@save\\n+    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\\n+    if torch.cuda.device_count() >= i + 1:\\n+        return torch.device(f\\'cuda:{i}\\')\\n+    return torch.device(\\'cpu\\')\\n+\\n+def try_all_gpus():  #@save\\n+    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\\n+    devices = [torch.device(f\\'cuda:{i}\\')\\n+             for i in range(torch.cuda.device_count())]\\n+    return devices if devices else [torch.device(\\'cpu\\')]\\n+\\n+try_gpu(), try_gpu(10), try_all_gpus()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def try_gpu(i=0):  #@save\\n+    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\\n+    if len(tf.config.experimental.list_physical_devices(\\'GPU\\')) >= i + 1:\\n+        return tf.device(f\\'/GPU:{i}\\')\\n+    return tf.device(\\'/CPU:0\\')\\n+\\n+def try_all_gpus():  #@save\\n+    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\\n+    num_gpus = len(tf.config.experimental.list_physical_devices(\\'GPU\\'))\\n+    devices = [tf.device(f\\'/GPU:{i}\\') for i in range(num_gpus)]\\n+    return devices if devices else [tf.device(\\'/CPU:0\\')]\\n+\\n+try_gpu(), try_gpu(10), try_all_gpus()\\n+```\\n+\\n+## 张量与gpu\\n+\\n+默认情况下，张量是在CPU上创建的。我们可以查询张量所在的设备。\\n+\\n+```{.python .input}\\n+x = np.array([1, 2, 3])\\n+x.ctx\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+x = torch.tensor([1, 2, 3])\\n+x.device\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+x = tf.constant([1, 2, 3])\\n+x.device\\n+```\\n+\\n+需要注意的是，无论何时我们要对多个术语进行操作，它们都必须在同一个设备上。例如，如果我们对两个张量求和，我们需要确保两个参数都位于同一个设备上，否则框架将不知道在哪里存储结果，甚至不知道如何决定在哪里执行计算。\\n+\\n+### 存储在GPU上\\n+\\n+有几种方法可以在GPU上存储张量。例如，我们可以在创建张量时指定存储设备。接下来，我们在第一个`gpu`上创建张量变量`X`。在GPU上创建的张量只消耗这个GPU的内存。我们可以使用2293命令查看内存使用情况。一般来说，我们需要确保不创建超过GPU内存限制的数据。\\n+\\n+```{.python .input}\\n+X = np.ones((2, 3), ctx=try_gpu())\\n+X\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+X = torch.ones(2, 3, device=try_gpu())\\n+X\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+with try_gpu():\\n+    X = tf.ones((2, 3))\\n+X\\n+```\\n+\\n+假设您至少有两个GPU，下面的代码将在第二个GPU上创建一个随机张量。\\n+\\n+```{.python .input}\\n+Y = np.random.uniform(size=(2, 3), ctx=try_gpu(1))\\n+Y\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+Y = torch.rand(2, 3, device=try_gpu(1))\\n+Y\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+with try_gpu(1):\\n+    Y = tf.random.uniform((2, 3))\\n+Y\\n+```\\n+\\n+### 复制\\n+\\n+如果我们要计算`X + Y`，我们需要决定在哪里执行这个操作。例如，如:numref:`fig_copyto`所示，我们可以将`X`传输到第二个GPU并在那里执行操作。\\n+*不要*简单地加上`X`和`Y`，\\n+因为这会导致异常。运行时引擎不知道该怎么做：它在同一设备上找不到数据，结果失败了。由于`Y`位于第二个GPU上，所以我们需要将`X`移到那里，然后才能添加这两个GPU。\\n+\\n+![Copy data to perform an operation on the same device.](../img/copyto.svg)\\n+:label:`fig_copyto`\\n+\\n+```{.python .input}\\n+Z = X.copyto(try_gpu(1))\\n+print(X)\\n+print(Z)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+Z = X.cuda(1)\\n+print(X)\\n+print(Z)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+with try_gpu(1):\\n+    Z = X\\n+print(X)\\n+print(Z)\\n+```\\n+\\n+现在数据在同一个GPU上（`Z`和`Y`都在），我们可以将它们相加。\\n+\\n+```{.python .input}\\n+#@tab all\\n+Y + Z\\n+```\\n+\\n+:begin_tab:`mxnet`\\n+假设变量`Z`已经存在于第二个GPU上。如果我们还是打`Z.copyto(gpu(1))`怎么办？它将复制并分配新的内存，即使该变量已经存在于所需的设备上。有时，根据代码运行的环境不同，两个变量可能已经存在于同一设备上。因此，我们只想在变量当前存在于不同设备中时进行复制。在这种情况下，我们可以打`as_in_ctx`。如果变量已经存在于指定的设备中，则这是不可操作的。除非您特别想复制，否则选择`as_in_ctx`方法。\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+假设变量`Z`已经存在于第二个GPU上。如果我们还是打`Z.cuda(1)`怎么办？它将返回`Z`，而不是复制并分配新内存。\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+假设变量`Z`已经存在于第二个GPU上。如果我们仍然在同一个设备作用域下调用`Z2 = Z`会怎么样？它将返回`Z`，而不是复制并分配新内存。\\n+:end_tab:\\n+\\n+```{.python .input}\\n+Z.as_in_ctx(try_gpu(1)) is Z\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+Z.cuda(1) is Z\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+with try_gpu(1):\\n+    Z2 = Z\\n+Z2 is Z\\n+```\\n+\\n+### 旁注\\n+\\n+人们使用gpu来进行机器学习，因为他们希望它们速度快。但是在设备之间传输变量是缓慢的。所以在我们让你做之前，我们希望你百分之百确定你想做一些缓慢的事情。如果深度学习框架只是自动复制而没有崩溃，那么您可能不会意识到您编写了一些缓慢的代码。\\n+\\n+此外，在设备（CPU、GPU和其他机器）之间传输数据比计算慢得多。这也使得并行化变得更加困难，因为我们必须等待数据被发送（或者更确切地说是接收），然后才能继续进行更多的操作。这就是为什么拷贝操作要格外小心。根据经验，许多小手术比一个大手术差得多。此外，一次几个操作比代码中散布的许多单个操作要好得多，除非您知道自己在做什么。这种情况下，如果一个设备必须等待另一个设备才能执行其他操作，那么这样的操作可能会阻塞。这有点像排队订购咖啡，而不是通过电话预先订购，然后在你准备好的时候发现它已经准备好了。\\n+\\n+最后，当我们打印张量或将张量转换为NumPy格式时，如果数据不在主内存中，框架会首先将其复制到主内存中，从而导致额外的传输开销。更糟糕的是，它现在受制于可怕的全局解释器锁，这使得一切都等待Python完成。\\n+\\n+## 神经网络与gpu\\n+\\n+类似地，神经网络模型可以指定设备。下面的代码将模型参数放在GPU上。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nn.Dense(1))\\n+net.initialize(ctx=try_gpu())\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(nn.Linear(3, 1))\\n+net = net.to(device=try_gpu())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+strategy = tf.distribute.MirroredStrategy()\\n+with strategy.scope():\\n+    net = tf.keras.models.Sequential([\\n+        tf.keras.layers.Dense(1)])\\n+```\\n+\\n+在接下来的几章中，我们将看到更多关于如何在gpu上运行模型的例子，因为它们将变得更加计算密集。\\n+\\n+当输入为GPU上的张量时，模型将在同一GPU上计算结果。\\n+\\n+```{.python .input}\\n+#@tab all\\n+net(X)\\n+```\\n+\\n+让我们确认模型参数存储在同一个GPU上。\\n+\\n+```{.python .input}\\n+net[0].weight.data().ctx\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net[0].weight.data.device\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net.layers[0].weights[0].device, net.layers[0].weights[1].device\\n+```\\n+\\n+总之，只要所有的数据和参数都在同一个设备上，我们就可以有效地学习模型。在下面的章节中，我们将看到几个这样的例子。\\n+\\n+## 摘要\\n+\\n+* 我们可以指定用于存储和计算的设备，例如CPU或GPU。默认情况下，数据在主内存中创建，然后使用CPU进行计算。\\n+* 深度学习框架要求计算的所有输入数据都在同一设备上，无论是CPU还是GPU。\\n+* 如果不小心移动数据，您可能会失去显著的性能。一个典型的错误如下：计算GPU上每一个minibatch的丢失并在命令行上报告给用户（或将其记录在NumPy `ndarray`中）将触发一个全局解释器锁，从而暂停所有GPU。最好在GPU内部分配内存用于日志记录，并且只移动较大的日志。\\n+\\n+## 练习\\n+\\n+1. 尝试一个更大的计算任务，比如大矩阵的乘法，看看CPU和GPU之间的速度差异。一个计算量很小的任务呢？\\n+1. 我们应该如何在GPU上读写模型参数？\\n+1. 测量计算$100 \\\\times 100$个矩阵的1000个矩阵矩阵乘法所需的时间，并记录输出矩阵的Frobenius范数，一次一个结果，而不是在GPU上保持日志记录并仅传输最终结果。\\n+1. 测量同时在两个GPU上执行两个矩阵矩阵乘法与在一个GPU上按顺序执行两个矩阵矩阵乘法所需的时间。提示：你应该看到几乎是线性缩放。\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/62)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/63)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/270)\\n+:end_tab:\\ndiff --git a/chapter_deep-learning-computation/use-gpu_tencent.md b/chapter_deep-learning-computation/use-gpu_tencent.md\\nnew file mode 100644\\nindex 00000000..8ab80ddc\\n--- /dev/null\\n+++ b/chapter_deep-learning-computation/use-gpu_tencent.md\\n@@ -0,0 +1,343 @@\\n+# GPU\\n+:label:`sec_use_gpu`\\n+\\n+在:numref:`tab_intro_decade`，我们讨论了计算在过去二十年中的快速增长。简而言之，从2000年开始，GPU性能每十年提升1000倍。这提供了很好的机会，但也表明需要提供这样的性能。\\n+\\n+在本节中，我们将开始讨论如何为您的研究利用这种计算性能。首先使用单个GPU，稍后介绍如何使用多个GPU和多个服务器(具有多个GPU)。\\n+\\n+具体来说，我们将讨论如何使用单个NVIDIA GPU进行计算。首先，确保您至少安装了一个NVIDIA GPU。然后，下载[NVIDIA driver and CUDA](https://developer.nvidia.com/cuda-downloads)并按照提示设置适当的路径。一旦这些准备工作完成，就可以使用`nvidia-smi`命令查看图形卡信息。\\n+\\n+```{.python .input}\\n+#@tab all\\n+!nvidia-smi\\n+```\\n+\\n+:begin_tab:`mxnet`\\n+您可能已经注意到，MXnet张量看起来与Numpy `ndarray`几乎相同。但有几个关键的不同之处。MXNet与NumPy的主要区别之一是它支持不同的硬件设备。\\n+\\n+在MXNet中，每个数组都有一个上下文。到目前为止，默认情况下，所有变量和相关计算都已分配给CPU。通常，其他环境可能是各种GPU。当我们在多台服务器上部署作业时，情况可能会变得更加复杂。通过智能地将数组分配给上下文，我们可以最大限度地减少在设备之间传输数据所花费的时间。例如，当在使用GPU的服务器上训练神经网络时，我们通常希望模型的参数驻留在GPU上。\\n+\\n+接下来，我们需要确认是否安装了MXNet的GPU版本。如果已经安装了MXNet的CPU版本，我们需要先卸载它。例如，使用`pip uninstall mxnet`命令，然后根据您的CUDA版本安装相应的MXnet版本。假设您已经安装了CUDA10.0，您可以通过`pip install mxnet-cu100`安装支持CUDA10.0的MXnet版本。\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+在PyTorch中，每个数组都有一个设备，我们通常将其称为上下文。到目前为止，默认情况下，所有变量和相关计算都已分配给CPU。通常，其他环境可能是各种GPU。当我们在多台服务器上部署作业时，情况可能会变得更加复杂。通过智能地将数组分配给上下文，我们可以最大限度地减少在设备之间传输数据所花费的时间。例如，当在使用GPU的服务器上训练神经网络时，我们通常希望模型的参数驻留在GPU上。\\n+\\n+接下来，我们需要确认是否安装了PyTorch的GPU版本。如果已经安装了PyTorch的CPU版本，我们需要先卸载它。例如，使用`pip uninstall torch`命令，然后根据您的CUDA版本安装相应的PyTorch版本。假设您已经安装了CUDA10.0，您可以通过`pip install torch-cu100`安装支持CUDA10.0的PyTorch版本。\\n+:end_tab:\\n+\\n+要运行本节中的程序，您至少需要两个GPU。请注意，对于大多数台式计算机来说，这可能过于奢侈，但在云中很容易获得，例如，通过使用AWS EC2多GPU实例。几乎所有其他部分都不需要多个GPU。相反，这只是为了说明数据如何在不同设备之间流动。\\n+\\n+## 计算设备\\n+\\n+我们可以指定存储和计算的设备，如CPU和GPU。默认情况下，张量在主内存中创建，然后使用CPU进行计算。\\n+\\n+:begin_tab:`mxnet`\\n+在mxnet中，cpu和gpu可以用`cpu()`和`gpu()`表示。需要注意的是，`cpu()`(或括号中的任意整数)表示所有物理CPU和内存。这意味着MXNet的计算将尝试使用所有CPU核心。但是，`gpu()`仅代表一张卡和相应的内存。如果有多个GPU，我们使用`gpu(i)`表示$i^\\\\mathrm{th}$个GPU($i$从0开始)。另外，`gpu(0)`和`gpu()`是等效的。\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+在PyTorch中，cpu和gpu可以用`torch.device(\\'cpu\\')`和`torch.cuda.device(\\'cuda\\')`表示。需要注意的是，`cpu`设备指的是所有物理CPU和内存。这意味着PyTorch的计算将尝试使用所有CPU核心。但是，`gpu`设备仅代表一张卡和相应的内存。如果有多个GPU，我们使用`torch.cuda.device(f\\'cuda:{i}\\')`表示$i^\\\\mathrm{th}$个GPU($i$从0开始)。另外，`gpu:0`和`gpu`是等效的。\\n+:end_tab:\\n+\\n+```{.python .input}\\n+from mxnet import np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+npx.cpu(), npx.gpu(), npx.gpu(1)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+import torch\\n+from torch import nn\\n+\\n+torch.device(\\'cpu\\'), torch.cuda.device(\\'cuda\\'), torch.cuda.device(\\'cuda:1\\')\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+import tensorflow as tf\\n+\\n+tf.device(\\'/CPU:0\\'), tf.device(\\'/GPU:0\\'), tf.device(\\'/GPU:1\\')\\n+```\\n+\\n+我们可以查询可用GPU的数量。\\n+\\n+```{.python .input}\\n+npx.num_gpus()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+torch.cuda.device_count()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+len(tf.config.experimental.list_physical_devices(\\'GPU\\'))\\n+```\\n+\\n+现在，我们定义了两个方便的函数，这两个函数允许我们在请求的GPU不存在的情况下运行代码。\\n+\\n+```{.python .input}\\n+def try_gpu(i=0):  #@save\\n+    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\\n+    return npx.gpu(i) if npx.num_gpus() >= i + 1 else npx.cpu()\\n+\\n+def try_all_gpus():  #@save\\n+    \"\"\"Return all available GPUs, or [cpu()] if no GPU exists.\"\"\"\\n+    devices = [npx.gpu(i) for i in range(npx.num_gpus())]\\n+    return devices if devices else [npx.cpu()]\\n+\\n+try_gpu(), try_gpu(10), try_all_gpus()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def try_gpu(i=0):  #@save\\n+    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\\n+    if torch.cuda.device_count() >= i + 1:\\n+        return torch.device(f\\'cuda:{i}\\')\\n+    return torch.device(\\'cpu\\')\\n+\\n+def try_all_gpus():  #@save\\n+    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\\n+    devices = [torch.device(f\\'cuda:{i}\\')\\n+             for i in range(torch.cuda.device_count())]\\n+    return devices if devices else [torch.device(\\'cpu\\')]\\n+\\n+try_gpu(), try_gpu(10), try_all_gpus()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def try_gpu(i=0):  #@save\\n+    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\\n+    if len(tf.config.experimental.list_physical_devices(\\'GPU\\')) >= i + 1:\\n+        return tf.device(f\\'/GPU:{i}\\')\\n+    return tf.device(\\'/CPU:0\\')\\n+\\n+def try_all_gpus():  #@save\\n+    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\\n+    num_gpus = len(tf.config.experimental.list_physical_devices(\\'GPU\\'))\\n+    devices = [tf.device(f\\'/GPU:{i}\\') for i in range(num_gpus)]\\n+    return devices if devices else [tf.device(\\'/CPU:0\\')]\\n+\\n+try_gpu(), try_gpu(10), try_all_gpus()\\n+```\\n+\\n+## 张量和GPU\\n+\\n+默认情况下，在CPU上创建张量。我们可以查询张量所在的设备。\\n+\\n+```{.python .input}\\n+x = np.array([1, 2, 3])\\n+x.ctx\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+x = torch.tensor([1, 2, 3])\\n+x.device\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+x = tf.constant([1, 2, 3])\\n+x.device\\n+```\\n+\\n+需要注意的是，每当我们想要在多个终端上运行时，它们都需要在同一设备上运行。例如，如果我们将两个张量相加，我们需要确保这两个参数位于同一设备上-否则框架将不知道将结果存储在哪里，甚至不知道如何决定在哪里执行计算。\\n+\\n+### GPU上的存储\\n+\\n+有几种方法可以在GPU上存储张量。例如，我们可以在创建张量时指定存储设备。接下来，我们在第一个`X`上创建张量变量`gpu`。在GPU上创建的张量仅消耗此GPU的内存。我们可以使用`nvidia-smi`命令查看gpu内存使用情况。通常，我们需要确保不会创建超过GPU内存限制的数据。\\n+\\n+```{.python .input}\\n+X = np.ones((2, 3), ctx=try_gpu())\\n+X\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+X = torch.ones(2, 3, device=try_gpu())\\n+X\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+with try_gpu():\\n+    X = tf.ones((2, 3))\\n+X\\n+```\\n+\\n+假设您至少有两个GPU，下面的代码将在第二个GPU上创建一个随机张量。\\n+\\n+```{.python .input}\\n+Y = np.random.uniform(size=(2, 3), ctx=try_gpu(1))\\n+Y\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+Y = torch.rand(2, 3, device=try_gpu(1))\\n+Y\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+with try_gpu(1):\\n+    Y = tf.random.uniform((2, 3))\\n+Y\\n+```\\n+\\n+### 正在复制\\n+\\n+如果我们想要计算`X + Y`，我们需要决定在哪里执行此操作。例如，如:numref:`fig_copyto`所示，我们可以将`X`转移到第二个图形处理器并在那里执行操作。\\n+*不要*简单地将`X`和`Y`相加，\\n+因为这将导致异常。运行时引擎将不知道该做什么：它无法在同一设备上找到数据，因此会失败。由于`Y`在第二个图形处理器上运行，我们需要将`X`移到那里，然后才能将这两个GPU相加。\\n+\\n+![Copy data to perform an operation on the same device.](../img/copyto.svg)\\n+:label:`fig_copyto`\\n+\\n+```{.python .input}\\n+Z = X.copyto(try_gpu(1))\\n+print(X)\\n+print(Z)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+Z = X.cuda(1)\\n+print(X)\\n+print(Z)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+with try_gpu(1):\\n+    Z = X\\n+print(X)\\n+print(Z)\\n+```\\n+\\n+现在数据都在同一个图形处理器上(`Z`和`Y`都是)，我们可以把它们加起来了。\\n+\\n+```{.python .input}\\n+#@tab all\\n+Y + Z\\n+```\\n+\\n+:begin_tab:`mxnet`\\n+假设您的变量`Z`已经存在于您的第二个图形处理器上。如果我们还是打`Z.copyto(gpu(1))`怎么办？即使变量已经驻留在所需设备上，它也会复制并分配新内存。有时，根据我们的代码运行环境的不同，同一设备上可能已经存在两个变量。因此，我们只想在变量当前位于不同的设备中时进行复制。在这种情况下，我们可以拨打`as_in_ctx`。如果变量已经存在于指定的设备中，则这是一个无操作。除非您特别想复制一份，否则`as_in_ctx`是您的首选方法。\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+假设您的变量`Z`已经存在于您的第二个图形处理器上。如果我们还是打`Z.cuda(1)`怎么办？它将返回`Z`，而不是复制并分配新内存。\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+假设您的变量`Z`已经存在于您的第二个图形处理器上。如果我们仍然在同一设备范围内呼叫`Z2 = Z`，会发生什么情况？它将返回`Z`，而不是复制并分配新内存。\\n+:end_tab:\\n+\\n+```{.python .input}\\n+Z.as_in_ctx(try_gpu(1)) is Z\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+Z.cuda(1) is Z\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+with try_gpu(1):\\n+    Z2 = Z\\n+Z2 is Z\\n+```\\n+\\n+### 附注\\n+\\n+人们使用GPU进行机器学习，因为他们希望它们速度快。但在设备之间传输变量的速度很慢。所以在我们让你做之前，我们希望你百分之百确定你想做一些缓慢的事情。如果深度学习框架只是自动复制而不崩溃，那么您可能不会意识到您已经编写了一些速度较慢的代码。\\n+\\n+此外，在设备(CPU、GPU和其他计算机)之间传输数据比计算慢得多。这也使得并行化变得更加困难，因为我们必须等待数据被发送(或者更确切地说，是被接收)，然后才能继续进行更多的操作。这就是为什么复制操作应该非常小心的原因。根据经验，许多小手术比一次大手术要糟糕得多。此外，除非您知道自己在做什么，否则一次执行几个操作要比代码中散布的许多单个操作要好得多。情况就是这样，因为如果一个设备必须等待另一个设备才能做其他事情，这样的操作可以挡路。这有点像排队订购咖啡，而不是通过电话预购，然后发现它在你准备好的时候已经准备好了。\\n+\\n+最后，当我们打印张量或将张量转换为NumPy格式时，如果数据不在主内存中，框架将首先将其复制到主内存，从而导致额外的传输开销。更糟糕的是，它现在受到可怕的全局解释器锁的影响，这使得一切都要等待Python完成。\\n+\\n+## 神经网络和GPU\\n+\\n+同样，神经网络模型可以指定设备。下面的代码将模型参数放在GPU上。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nn.Dense(1))\\n+net.initialize(ctx=try_gpu())\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(nn.Linear(3, 1))\\n+net = net.to(device=try_gpu())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+strategy = tf.distribute.MirroredStrategy()\\n+with strategy.scope():\\n+    net = tf.keras.models.Sequential([\\n+        tf.keras.layers.Dense(1)])\\n+```\\n+\\n+在接下来的章节中，我们将看到更多如何在GPU上运行模型的示例，因为它们将变得更加计算密集。\\n+\\n+当输入是GPU上的张量时，模型将在同一GPU上计算结果。\\n+\\n+```{.python .input}\\n+#@tab all\\n+net(X)\\n+```\\n+\\n+让我们确认模型参数存储在同一GPU上。\\n+\\n+```{.python .input}\\n+net[0].weight.data().ctx\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net[0].weight.data.device\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net.layers[0].weights[0].device, net.layers[0].weights[1].device\\n+```\\n+\\n+简而言之，只要所有的数据和参数都在同一设备上，我们就可以有效地学习模型。在接下来的章节中，我们将看到几个这样的例子。\\n+\\n+## 摘要\\n+\\n+* 我们可以指定用于存储和计算的设备，例如CPU或GPU。默认情况下，数据在主内存中创建，然后使用CPU进行计算。\\n+* 深度学习框架要求所有用于计算的输入数据都在同一设备上，无论是CPU还是相同的GPU。\\n+* 不经意地移动数据可能会显著降低性能。一个典型的错误如下：计算GPU上每个小批量的损失，并在命令行上将其报告给用户(或将其记录在NumPy `ndarray`中)将触发全局解释器锁，从而使所有GPU停滞。最好是为GPU内部的日志记录分配内存，并且只移动较大的日志。\\n+\\n+## 练习\\n+\\n+1. 尝试执行更大的计算任务，例如大型矩阵的乘法，看看CPU和GPU之间的速度差异。如果是一个计算量很小的任务呢？\\n+1. 我们应该如何在GPU上读写模型参数？\\n+1. 测量计算$100 \\\\times 100$个矩阵的1,000个矩阵-矩阵乘法所需的时间，并记录输出矩阵的弗罗贝尼乌斯范数，一次记录一个结果，而不是在图形处理器上保存日志，只传输最终结果。\\n+1. 测量在两个GPU上同时执行两个矩阵-矩阵乘法运算所需的时间，而不是在一个GPU上按顺序执行两个矩阵-矩阵乘法运算所需的时间。提示：您应该看到近乎线性的缩放。\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/62)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/63)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/270)\\n+:end_tab:\\ndiff --git a/chapter_multilayer-perceptrons/backprop_baidu.md b/chapter_multilayer-perceptrons/backprop_baidu.md\\nnew file mode 100644\\nindex 00000000..32e00441\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/backprop_baidu.md\\n@@ -0,0 +1,133 @@\\n+# 前向传播、后向传播和计算图\\n+:label:`sec_backprop`\\n+\\n+到目前为止，我们已经用小批量随机梯度下降训练了我们的模型。然而，当我们实现该算法时，我们只担心通过模型的*前向传播*所涉及的计算。在计算梯度时，我们只调用了deep learning框架提供的反向传播函数。\\n+\\n+梯度的自动计算（自动微分）大大简化了深度学习算法的实现。在自动微分之前，即使是对复杂模型的微小变化也需要手工重新计算复杂的导数。令人惊讶的是，学术论文不得不分配大量页面来推导更新规则。虽然我们必须继续依赖于自动微分，这样我们就可以专注于有趣的部分，但是如果你想超越对深度学习的浅薄理解，你应该知道这些梯度是如何计算出来的。\\n+\\n+在本节中，我们将深入探讨*反向传播*的细节（通常称为*反向传播*）。为了传达对这些技术及其实现的一些见解，我们依赖一些基本的数学和计算图。首先，我们将我们的重点放在一个隐藏层MLP的重量衰减（$L_2$正则化）。\\n+\\n+## 前向传播\\n+\\n+*前向传播*（或*前向传递*）指的是计算和存储\\n+神经网络的中间变量（包括输出）按从输入层到输出层的顺序排列。我们现在一步一步地研究具有一个隐藏层的神经网络的机制。这看起来很乏味，但用滑稽大师詹姆斯布朗（jamesbrown）永恒的话来说，你必须“为当老板付出代价”。\\n+\\n+为了简单起见，我们假设输入示例是$\\\\mathbf{x}\\\\in \\\\mathbb{R}^d$，并且我们的隐藏层不包括偏移项。这里的中间变量是：\\n+\\n+$$\\\\mathbf{z}= \\\\mathbf{W}^{(1)} \\\\mathbf{x},$$\\n+\\n+其中$\\\\mathbf{W}^{(1)} \\\\in \\\\mathbb{R}^{h \\\\times d}$是隐藏层的权重参数。在通过激活函数$\\\\phi$运行中间变量$\\\\mathbf{z}\\\\in \\\\mathbb{R}^h$后，我们得到长度为$h$的隐藏激活向量，\\n+\\n+$$\\\\mathbf{h}= \\\\phi (\\\\mathbf{z}).$$\\n+\\n+隐藏变量$\\\\mathbf{h}$也是一个中间变量。假设输出层的参数只有$\\\\mathbf{W}^{(2)} \\\\in \\\\mathbb{R}^{q \\\\times h}$的权重，我们可以得到一个长度为$q$的向量的输出层变量：\\n+\\n+$$\\\\mathbf{o}= \\\\mathbf{W}^{(2)} \\\\mathbf{h}.$$\\n+\\n+假设损失函数为$l$，示例标签为$y$，我们可以计算单个数据示例的损失项，\\n+\\n+$$L = l(\\\\mathbf{o}, y).$$\\n+\\n+根据$L_2$正则化的定义，给定超参数$\\\\lambda$，正则化项为\\n+\\n+$$s = \\\\frac{\\\\lambda}{2} \\\\left(\\\\|\\\\mathbf{W}^{(1)}\\\\|_F^2 + \\\\|\\\\mathbf{W}^{(2)}\\\\|_F^2\\\\right),$$\\n+:eqlabel:`eq_forward-s`\\n+\\n+其中，矩阵的Frobenius范数是将矩阵展平为向量后应用的$L_2$范数。最后，模型在给定数据示例上的正则化损失为：\\n+\\n+$$J = L + s.$$\\n+\\n+在下面的讨论中，我们将$J$称为*目标函数*。\\n+\\n+## 前向传播计算图\\n+\\n+绘制*计算图*有助于我们可视化计算中运算符和变量的依赖关系。:numref:`fig_forward`包含与上述简单网络相关联的图，其中正方形表示变量，圆圈表示运算符。左下角表示输入，右上角表示输出。注意箭头的方向（显示数据流）主要是向右和向上的。\\n+\\n+![Computational graph of forward propagation.](../img/forward.svg)\\n+:label:`fig_forward`\\n+\\n+## 反向传播\\n+\\n+*反向传播*指的是计算方法\\n+神经网络参数的梯度。简言之，该方法根据微积分中的*链规则*按相反的顺序遍历网络，从输出层到输入层。该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。假设我们有函数$\\\\mathsf{Y}=f(\\\\mathsf{X})$和$\\\\mathsf{Z}=g(\\\\mathsf{Y})$，其中输入和输出$\\\\mathsf{X}, \\\\mathsf{Y}, \\\\mathsf{Z}$是任意形状的张量。利用链式法则，我们可以计算$\\\\mathsf{Z}$相对于$\\\\mathsf{X}$的导数\\n+\\n+$$\\\\frac{\\\\partial \\\\mathsf{Z}}{\\\\partial \\\\mathsf{X}} = \\\\text{prod}\\\\left(\\\\frac{\\\\partial \\\\mathsf{Z}}{\\\\partial \\\\mathsf{Y}}, \\\\frac{\\\\partial \\\\mathsf{Y}}{\\\\partial \\\\mathsf{X}}\\\\right).$$\\n+\\n+在这里，我们使用$\\\\text{prod}$运算符在执行必要的操作（如换位和交换输入位置）后将其参数相乘。对于向量，这很简单：它只是矩阵-矩阵乘法。对于高维张量，我们使用适当的对应项。运算符$\\\\text{prod}$隐藏了所有开销的符号。\\n+\\n+回想一下，计算图在:numref:`fig_forward`中的具有一个隐藏层的简单网络的参数是$\\\\mathbf{W}^{(1)}$和$\\\\mathbf{W}^{(2)}$。反向传播的目的是计算梯度$\\\\partial J/\\\\partial \\\\mathbf{W}^{(1)}$和$\\\\partial J/\\\\partial \\\\mathbf{W}^{(2)}$。为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。计算的顺序与前向传播中执行的顺序相反，因为我们需要从计算图的结果开始，并朝着参数的方向努力。第一步是计算目标函数$J=L+s$相对于损失项$L$和正则项$s$的梯度。\\n+\\n+$$\\\\frac{\\\\partial J}{\\\\partial L} = 1 \\\\; \\\\text{and} \\\\; \\\\frac{\\\\partial J}{\\\\partial s} = 1.$$\\n+\\n+接下来，我们根据链式法则计算目标函数相对于输出层$\\\\mathbf{o}$的变量的梯度：\\n+\\n+$$\\n+\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{o}}\\n+= \\\\text{prod}\\\\left(\\\\frac{\\\\partial J}{\\\\partial L}, \\\\frac{\\\\partial L}{\\\\partial \\\\mathbf{o}}\\\\right)\\n+= \\\\frac{\\\\partial L}{\\\\partial \\\\mathbf{o}}\\n+\\\\in \\\\mathbb{R}^q.\\n+$$\\n+\\n+接下来，我们计算正则化项相对于两个参数的梯度：\\n+\\n+$$\\\\frac{\\\\partial s}{\\\\partial \\\\mathbf{W}^{(1)}} = \\\\lambda \\\\mathbf{W}^{(1)}\\n+\\\\; \\\\text{and} \\\\;\\n+\\\\frac{\\\\partial s}{\\\\partial \\\\mathbf{W}^{(2)}} = \\\\lambda \\\\mathbf{W}^{(2)}.$$\\n+\\n+现在我们可以计算最接近输出层的模型参数的梯度$\\\\partial J/\\\\partial \\\\mathbf{W}^{(2)} \\\\in \\\\mathbb{R}^{q \\\\times h}$。使用链式法则得出：\\n+\\n+$$\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{W}^{(2)}}= \\\\text{prod}\\\\left(\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{o}}, \\\\frac{\\\\partial \\\\mathbf{o}}{\\\\partial \\\\mathbf{W}^{(2)}}\\\\right) + \\\\text{prod}\\\\left(\\\\frac{\\\\partial J}{\\\\partial s}, \\\\frac{\\\\partial s}{\\\\partial \\\\mathbf{W}^{(2)}}\\\\right)= \\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{o}} \\\\mathbf{h}^\\\\top + \\\\lambda \\\\mathbf{W}^{(2)}.$$\\n+:eqlabel:`eq_backprop-J-h`\\n+\\n+为了获得关于$\\\\mathbf{W}^{(1)}$的梯度，我们需要继续沿着输出层到隐藏层的反向传播。关于隐藏层输出$\\\\partial J/\\\\partial \\\\mathbf{h} \\\\in \\\\mathbb{R}^h$的梯度由下式给出\\n+\\n+$$\\n+\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{h}}\\n+= \\\\text{prod}\\\\left(\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{o}}, \\\\frac{\\\\partial \\\\mathbf{o}}{\\\\partial \\\\mathbf{h}}\\\\right)\\n+= {\\\\mathbf{W}^{(2)}}^\\\\top \\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{o}}.\\n+$$\\n+\\n+由于激活函数$\\\\phi$应用元素，计算中间变量$\\\\mathbf{z}$的梯度$\\\\partial J/\\\\partial \\\\mathbf{z} \\\\in \\\\mathbb{R}^h$需要使用元素乘法运算符，我们用$\\\\odot$表示：\\n+\\n+$$\\n+\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{z}}\\n+= \\\\text{prod}\\\\left(\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{h}}, \\\\frac{\\\\partial \\\\mathbf{h}}{\\\\partial \\\\mathbf{z}}\\\\right)\\n+= \\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{h}} \\\\odot \\\\phi\\'\\\\left(\\\\mathbf{z}\\\\right).\\n+$$\\n+\\n+最后，我们可以得到最接近输入层的模型参数梯度$\\\\partial J/\\\\partial \\\\mathbf{W}^{(1)} \\\\in \\\\mathbb{R}^{h \\\\times d}$。根据链式法则，我们得到\\n+\\n+$$\\n+\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{W}^{(1)}}\\n+= \\\\text{prod}\\\\left(\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{z}}, \\\\frac{\\\\partial \\\\mathbf{z}}{\\\\partial \\\\mathbf{W}^{(1)}}\\\\right) + \\\\text{prod}\\\\left(\\\\frac{\\\\partial J}{\\\\partial s}, \\\\frac{\\\\partial s}{\\\\partial \\\\mathbf{W}^{(1)}}\\\\right)\\n+= \\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{z}} \\\\mathbf{x}^\\\\top + \\\\lambda \\\\mathbf{W}^{(1)}.\\n+$$\\n+\\n+## 训练神经网络\\n+\\n+在训练神经网络时，前向传播和后向传播相互依赖。特别是对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。然后将它们用于反向传播，其中图上的计算顺序颠倒。\\n+\\n+以上述简单网络为例进行说明。一方面，在前向传播期间计算正则项:eqref:`eq_forward-s`取决于模型参数$\\\\mathbf{W}^{(1)}$和$\\\\mathbf{W}^{(2)}$的当前值。它们是由最新迭代的反向传播优化算法给出的。另一方面，反向传播期间参数`eq_backprop-J-h`的梯度计算取决于由前向传播给出的隐藏变量$\\\\mathbf{h}$的当前值。\\n+\\n+因此，在训练神经网络时，在初始化模型参数后，我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。注意，反向传播重用前向传播中存储的中间值，以避免重复计算。结果之一是我们需要保留中间值，直到反向传播完成。这也是为什么训练比单纯的预测需要更多的记忆的原因之一。此外，这些中间值的大小与网络层的数量和批处理的大小大致成正比。因此，使用更大的批处理来训练更深层次的网络更容易导致“内存不足”错误。\\n+\\n+## 摘要\\n+\\n+* 前向传播在神经网络定义的计算图中按顺序计算和存储中间变量。它从输入层进入输出层。\\n+* 反向传播按相反的顺序计算和存储神经网络中中间变量和参数的梯度。\\n+* 在训练深度学习模型时，正向传播和反向传播是相互依赖的。\\n+* 训练比预测需要更多的记忆。\\n+\\n+## 练习\\n+\\n+1. 假设一些标量函数$\\\\mathbf{X}$的输入$\\\\mathbf{X}$是$n \\\\times m$矩阵。$f$相对于$\\\\mathbf{X}$的梯度维数是多少？\\n+1. 向本节中描述的模型的隐藏层添加偏移（不需要在正则化项中包含偏移）。\\n+    1. 绘制相应的计算图。\\n+    1. 推导了前向和后向传播方程。\\n+1. 在本节描述的模型中计算用于训练和预测的内存占用。\\n+1. 假设你想计算二阶导数。计算图发生了什么？你预计计算需要多长时间？\\n+1. 假设计算图对于你的GPU来说太大了。\\n+    1. 你能把它划分到多个GPU上吗？\\n+    1. 与小批量培训相比，有哪些优点和缺点？\\n+\\n+[Discussions](https://discuss.d2l.ai/t/102)\\ndiff --git a/chapter_multilayer-perceptrons/backprop_tencent.md b/chapter_multilayer-perceptrons/backprop_tencent.md\\nnew file mode 100644\\nindex 00000000..e382674a\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/backprop_tencent.md\\n@@ -0,0 +1,133 @@\\n+# 前向传播、后向传播和计算图\\n+:label:`sec_backprop`\\n+\\n+到目前为止，我们已经用小批量随机梯度下降法训练了我们的模型。然而，当我们实现算法时，我们只担心通过模型的“前向传播”所涉及的计算。当需要计算梯度时，我们只需调用深度学习框架提供的反向传播函数。\\n+\\n+梯度的自动计算(自动微分)大大简化了深度学习算法的实现。在自动微分之前，即使是复杂模型的微小变化也需要手工重新计算复杂的导数。令人惊讶的是，学术论文常常不得不分配大量页面来派生更新规则。虽然我们必须继续依靠自动微分，这样我们才能专注于有趣的部分，但如果你想超越对深度学习的肤浅理解，你应该知道这些梯度是如何在幕后计算的。\\n+\\n+在本节中，我们将深入研究“反向传播”(通常称为“反向传播”)的细节。为了表达对技术及其实现的一些见解，我们依赖于一些基本的数学和计算图。首先，我们把重点放在一个权重衰减($L_2$正则化)的单隐层最大似然模型上。\\n+\\n+## 前向传播\\n+\\n+*前向传播*(或*前向传递*)是指计算和存储\\n+神经网络的中间变量(包括输出)从输入层到输出层的顺序。我们现在一步一步地研究具有一个隐藏层的神经网络的机制。这看起来可能很乏味，但用放克大师詹姆斯·布朗(James Brown)的永恒名言来说，你必须“付出成为老板的代价”。\\n+\\n+为简单起见，我们假设输入示例为$\\\\mathbf{x}\\\\in \\\\mathbb{R}^d$，并且我们的隐藏层不包括偏置项。这里的中间变量是：\\n+\\n+$$\\\\mathbf{z}= \\\\mathbf{W}^{(1)} \\\\mathbf{x},$$\\n+\\n+其中$\\\\mathbf{W}^{(1)} \\\\in \\\\mathbb{R}^{h \\\\times d}$是隐藏层的权重参数。在运行中间变量$\\\\mathbf{z}\\\\in \\\\mathbb{R}^h$通过激活函数$\\\\phi$之后，我们获得长度为$h$的隐藏激活向量，\\n+\\n+$$\\\\mathbf{h}= \\\\phi (\\\\mathbf{z}).$$\\n+\\n+隐藏变量$\\\\mathbf{h}$也是中间变量。假设输出层的参数只有$\\\\mathbf{W}^{(2)} \\\\in \\\\mathbb{R}^{q \\\\times h}$的权重，我们可以得到一个长度为$q$的输出层变量：\\n+\\n+$$\\\\mathbf{o}= \\\\mathbf{W}^{(2)} \\\\mathbf{h}.$$\\n+\\n+假设损失函数是$l$并且示例标签是$y$，然后我们可以计算单个数据示例的损失项，\\n+\\n+$$L = l(\\\\mathbf{o}, y).$$\\n+\\n+根据$L_2$正则化的定义，给定超参数$\\\\lambda$，正则化项为\\n+\\n+$$s = \\\\frac{\\\\lambda}{2} \\\\left(\\\\|\\\\mathbf{W}^{(1)}\\\\|_F^2 + \\\\|\\\\mathbf{W}^{(2)}\\\\|_F^2\\\\right),$$\\n+:eqlabel:`eq_forward-s`\\n+\\n+其中，矩阵的弗罗贝尼乌斯范数仅仅是在将矩阵展平为向量之后应用的$L_2$范数。最后，模型在给定数据实例上的正则化损失为：\\n+\\n+$$J = L + s.$$\\n+\\n+在下面的讨论中，我们将$J$称为“目标函数”。\\n+\\n+## 前向传播的计算图\\n+\\n+绘制*计算图*帮助我们可视化计算中运算符和变量的依赖关系。:numref:`fig_forward`包含与上述简单网络相关联的图，其中正方形表示变量，圆圈表示运算符。左下角表示输入，右上角表示输出。请注意，箭头(说明数据流)的方向主要是向右和向上。\\n+\\n+![Computational graph of forward propagation.](../img/forward.svg)\\n+:label:`fig_forward`\\n+\\n+## 反向传播\\n+\\n+*反向传播*指的是计算方法\\n+神经网络参数的梯度。简而言之，根据微积分中的“链规则”，该方法以相反的顺序遍历网络，从输出层到输入层。该算法存储计算关于某些参数的梯度时所需的任何中间变量(偏导数)。假设我们具有函数$\\\\mathsf{Y}=f(\\\\mathsf{X})$和$\\\\mathsf{Z}=g(\\\\mathsf{Y})$，其中输入和输出$\\\\mathsf{X}, \\\\mathsf{Y}, \\\\mathsf{Z}$是任意形状的张量。通过使用链式规则，我们可以计算$\\\\mathsf{Z}$相对于$\\\\mathsf{X}$ VIA的导数\\n+\\n+$$\\\\frac{\\\\partial \\\\mathsf{Z}}{\\\\partial \\\\mathsf{X}} = \\\\text{prod}\\\\left(\\\\frac{\\\\partial \\\\mathsf{Z}}{\\\\partial \\\\mathsf{Y}}, \\\\frac{\\\\partial \\\\mathsf{Y}}{\\\\partial \\\\mathsf{X}}\\\\right).$$\\n+\\n+在这里，在执行了必要的操作(如转置和交换输入位置)之后，我们使用$\\\\text{prod}$运算符将其参数相乘。对于向量，这很简单：它只是矩阵-矩阵乘法。对于高维张量，我们使用适当的对应项。操作员$\\\\text{prod}$隐藏所有符号开销。\\n+\\n+回想一下，具有一个隐藏层的简单网络的参数为$\\\\mathbf{W}^{(1)}$和$\\\\mathbf{W}^{(2)}$，其计算图为:numref:`fig_forward`。反向传播的目标是计算梯度$\\\\partial J/\\\\partial \\\\mathbf{W}^{(1)}$和$\\\\partial J/\\\\partial \\\\mathbf{W}^{(2)}$。为此，我们应用链式规则，依次计算每个中间变量和参数的梯度。计算的顺序与在正向传播中执行的顺序相反，因为我们需要从计算图形的结果开始，并以我们的方式工作以获得参数。第一步是计算目标函数$J=L+s$相对于损失项$L$和正则化项$s$的梯度。\\n+\\n+$$\\\\frac{\\\\partial J}{\\\\partial L} = 1 \\\\; \\\\text{and} \\\\; \\\\frac{\\\\partial J}{\\\\partial s} = 1.$$\\n+\\n+接下来，我们根据链规则计算目标函数相对于输出层$\\\\mathbf{o}$的变量的梯度：\\n+\\n+$$\\n+\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{o}}\\n+= \\\\text{prod}\\\\left(\\\\frac{\\\\partial J}{\\\\partial L}, \\\\frac{\\\\partial L}{\\\\partial \\\\mathbf{o}}\\\\right)\\n+= \\\\frac{\\\\partial L}{\\\\partial \\\\mathbf{o}}\\n+\\\\in \\\\mathbb{R}^q.\\n+$$\\n+\\n+接下来，我们计算关于这两个参数的正则化项的梯度：\\n+\\n+$$\\\\frac{\\\\partial s}{\\\\partial \\\\mathbf{W}^{(1)}} = \\\\lambda \\\\mathbf{W}^{(1)}\\n+\\\\; \\\\text{and} \\\\;\\n+\\\\frac{\\\\partial s}{\\\\partial \\\\mathbf{W}^{(2)}} = \\\\lambda \\\\mathbf{W}^{(2)}.$$\\n+\\n+现在我们能够计算最接近输出层的模型参数的梯度$\\\\partial J/\\\\partial \\\\mathbf{W}^{(2)} \\\\in \\\\mathbb{R}^{q \\\\times h}$。使用链规则会产生以下结果：\\n+\\n+$$\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{W}^{(2)}}= \\\\text{prod}\\\\left(\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{o}}, \\\\frac{\\\\partial \\\\mathbf{o}}{\\\\partial \\\\mathbf{W}^{(2)}}\\\\right) + \\\\text{prod}\\\\left(\\\\frac{\\\\partial J}{\\\\partial s}, \\\\frac{\\\\partial s}{\\\\partial \\\\mathbf{W}^{(2)}}\\\\right)= \\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{o}} \\\\mathbf{h}^\\\\top + \\\\lambda \\\\mathbf{W}^{(2)}.$$\\n+:eqlabel:`eq_backprop-J-h`\\n+\\n+为了获得相对于$\\\\mathbf{W}^{(1)}$的梯度，我们需要沿着输出层继续向后传播到隐藏层。关于隐藏层的输出$\\\\partial J/\\\\partial \\\\mathbf{h} \\\\in \\\\mathbb{R}^h$的梯度由下式给出\\n+\\n+$$\\n+\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{h}}\\n+= \\\\text{prod}\\\\left(\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{o}}, \\\\frac{\\\\partial \\\\mathbf{o}}{\\\\partial \\\\mathbf{h}}\\\\right)\\n+= {\\\\mathbf{W}^{(2)}}^\\\\top \\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{o}}.\\n+$$\\n+\\n+由于激活函数$\\\\phi$按元素应用，所以计算中间变量$\\\\partial J/\\\\partial \\\\mathbf{z} \\\\in \\\\mathbb{R}^h$的梯度$\\\\mathbf{z}$需要使用按元素乘法运算符，我们用$\\\\odot$表示：\\n+\\n+$$\\n+\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{z}}\\n+= \\\\text{prod}\\\\left(\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{h}}, \\\\frac{\\\\partial \\\\mathbf{h}}{\\\\partial \\\\mathbf{z}}\\\\right)\\n+= \\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{h}} \\\\odot \\\\phi\\'\\\\left(\\\\mathbf{z}\\\\right).\\n+$$\\n+\\n+最后，我们可以得到最接近输入层的模型参数的梯度$\\\\partial J/\\\\partial \\\\mathbf{W}^{(1)} \\\\in \\\\mathbb{R}^{h \\\\times d}$。根据链式法则，我们得到\\n+\\n+$$\\n+\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{W}^{(1)}}\\n+= \\\\text{prod}\\\\left(\\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{z}}, \\\\frac{\\\\partial \\\\mathbf{z}}{\\\\partial \\\\mathbf{W}^{(1)}}\\\\right) + \\\\text{prod}\\\\left(\\\\frac{\\\\partial J}{\\\\partial s}, \\\\frac{\\\\partial s}{\\\\partial \\\\mathbf{W}^{(1)}}\\\\right)\\n+= \\\\frac{\\\\partial J}{\\\\partial \\\\mathbf{z}} \\\\mathbf{x}^\\\\top + \\\\lambda \\\\mathbf{W}^{(1)}.\\n+$$\\n+\\n+## 训练神经网络\\n+\\n+在训练神经网络时，前向传播和后向传播是相互依赖的。特别地，对于正向传播，我们沿依赖方向遍历计算图，并计算其路径上的所有变量。然后将它们用于反向传播，其中图形上的计算顺序是颠倒的。\\n+\\n+以上述简单网络为例进行说明。一方面，在前向传播期间计算正则化项:eqref:`eq_forward-s`取决于模型参数$\\\\mathbf{W}^{(1)}$和$\\\\mathbf{W}^{(2)}$的当前值。它们是根据最近一次迭代中的反向传播由优化算法给出的。另一方面，参数`eq_backprop-J-h`在反向传播期间的梯度计算取决于由前向传播给出的隐藏变量$\\\\mathbf{h}$的当前值。\\n+\\n+因此，在训练神经网络时，在模型参数初始化后，我们交替使用正向传播和反向传播，利用反向传播给出的梯度来更新模型参数。请注意，反向传播重复使用前向传播中存储的中间值，以避免重复计算。结果之一是我们需要保留中间值，直到反向传播完成。这也是为什么训练需要比普通预测多得多的记忆的原因之一。此外，这种中间值的大小与网络层数和批量大小大致成正比。因此，使用更大的批大小训练更深的网络更容易导致“内存不足”错误。\\n+\\n+## 摘要\\n+\\n+* 前向传播在由神经网络定义的计算图内顺序地计算和存储中间变量。它从输入到输出层。\\n+* 反向传播以相反的顺序顺序计算和存储神经网络内的中间变量和参数的梯度。\\n+* 在训练深度学习模型时，前向传播和后向传播是相互依赖的。\\n+* 训练比预测需要更多的记忆。\\n+\\n+## 练习\\n+\\n+1. 假设某个标量函数$\\\\mathbf{X}$的输入$f$是$n \\\\times m$个矩阵。$f$的梯度相对于$\\\\mathbf{X}$的维度是多少？\\n+1. 将偏移添加到本节中介绍的模型的隐藏层(不需要在正则化项中包含偏移)。\\n+    1. 画出相应的计算图。\\n+    1. 推导了前向和后向传播方程。\\n+1. 计算本节描述的模型中用于训练和预测的内存占用。\\n+1. 假设您想要计算二阶导数。计算图形会发生什么情况？您预计计算需要多长时间？\\n+1. 假设计算图对于您的GPU来说太大。\\n+    1. 您可以将其分区到多个GPU上吗？\\n+    1. 与小批量培训相比，优势和劣势是什么？\\n+\\n+[Discussions](https://discuss.d2l.ai/t/102)\\ndiff --git a/chapter_multilayer-perceptrons/dropout_baidu.md b/chapter_multilayer-perceptrons/dropout_baidu.md\\nnew file mode 100644\\nindex 00000000..ad993595\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/dropout_baidu.md\\n@@ -0,0 +1,382 @@\\n+# 辍学\\n+:label:`sec_dropout`\\n+\\n+在:numref:`sec_weight_decay`中，我们通过惩罚$L_2$权重范数引入了正则化统计模型的经典方法。从概率的角度来看，我们可以通过论证我们已经假设了一个先验的观点，即权重从均值为零的高斯分布中取值，从而证明这一技术的合理性。更直观地说，我们可能会认为，我们鼓励模型在许多特性之间分散权重，而不是过分依赖于少量潜在的虚假关联。\\n+\\n+## 重新审视过拟合\\n+\\n+面对更多的特征而不是示例，线性模型往往过于拟合。但是给出的例子比特性多，我们通常可以指望线性模型不会过度拟合。不幸的是，线性模型推广的可靠性是有代价的。简单地说，线性模型不考虑特性之间的交互作用。对于每个特征，线性模型必须指定正权重或负权重，忽略上下文。\\n+\\n+在传统文本中，归纳性和灵活性之间的这种基本张力被描述为“偏差-方差权衡”。线性模型有很高的偏差：它们只能代表一小类函数。然而，这些模型的方差很低：它们在不同的随机数据样本中给出了相似的结果。\\n+\\n+深层神经网络位于偏差方差谱的另一端。与线性模型不同，神经网络不局限于单独观察每个特征。他们可以学习功能组之间的交互。例如，他们可能会推断“尼日利亚”和“西联”一起出现在一封电子邮件中表示垃圾邮件，但分开来看则不是。\\n+\\n+即使当我们的例子比特征多得多的时候，深层神经网络也有过度拟合的能力。2017年，一组研究人员通过在随机标记的图像上训练深度网络，展示了神经网络的极端灵活性。尽管没有将输入与输出联系起来的真实模式，他们发现通过随机梯度下降优化的神经网络可以完美地标记训练集中的每一幅图像。想想这意味着什么。如果标签是均匀随机分配的，并且有10个类，那么没有一个分类器能对保留数据的准确率超过10%。这里的泛化差距高达90%。如果我们的模型表现力如此之强，以至于它们会严重过度拟合，那么我们什么时候才能期望它们不会过度拟合呢？\\n+\\n+深层网络令人费解的泛化特性的数学基础仍然是一个开放的研究问题，我们鼓励以理论为导向的读者深入研究这个主题。现在，我们转向实际工具的调查，这些工具往往在经验上改进深网的泛化。\\n+\\n+## 扰动鲁棒性\\n+\\n+让我们简单地想想我们对一个好的预测模型的期望。我们希望它能在看不见的数据上表现良好。经典的泛化理论认为，为了缩小列车性能和试验性能之间的差距，我们应该建立一个简单的模型。简单可以以少量维度的形式出现。我们在:numref:`sec_model_selection`中讨论线性模型的单项基函数时探讨了这一点。此外，正如我们在:numref:`sec_weight_decay`中讨论权重衰减（$L_2$正则化）时所看到的，参数的（逆）范数也代表了一种有用的简化度量。简单性的另一个有用的概念是平滑性，即函数对输入的微小变化不敏感。例如，当我们对图像进行分类时，我们希望在像素中添加一些随机噪声基本上是无害的。\\n+\\n+1995年，Christopher Bishop证明了带输入噪声的训练等价于Tikhonov正则化:cite:`Bishop.1995`。这项工作在要求函数平滑（因而简单）和要求它对输入中的扰动具有弹性之间建立了清晰的数学联系。\\n+\\n+2014年，Srivastava等人。:cite:`Srivastava.Hinton.Krizhevsky.ea.2014`开发了一个聪明的想法，如何将毕晓普的想法应用到网络的内部层。也就是说，他们建议在训练时先在网络的每一层中注入噪声，然后再计算下一层。他们意识到，当训练具有多层的深层网络时，注入噪声只会在输入输出映射上强制实现平滑。\\n+\\n+他们的想法叫做“dropout”，在前向传播过程中，在计算每个内部层的同时注入噪声，这已经成为训练神经网络的标准技术。这个方法叫做“dropout”，因为我们\\n+*在训练过程中去掉一些神经元。\\n+在整个训练过程中，在每次迭代中，标准的丢失包括在计算下一层之前将每层中的一些节点归零。\\n+\\n+明确地说，我们把自己的叙述与毕肖普联系起来。关于辍学的原始论文通过对有性生殖的惊人类比提供了直觉。作者认为，神经网络过度拟合的特征是每一层都依赖于前一层的特定激活模式，称之为“协同适应”。他们声称，辍学破坏了共适应，正如有性生殖被认为是破坏共适应基因一样。\\n+\\n+关键的挑战是如何注入这种噪音。一种想法是以一种“无偏”的方式注入噪声，这样每一层的期望值——在固定其他层时——等于它在没有噪声的情况下的期望值。\\n+\\n+在毕晓普的工作中，他在线性模型的输入中加入了高斯噪声。在每次训练迭代中，他将从均值为零的分布$\\\\epsilon \\\\sim \\\\mathcal{N}(0,\\\\sigma^2)$采样的噪声添加到输入$\\\\mathbf{x}$，得到一个扰动点$\\\\mathbf{x}\\' = \\\\mathbf{x} + \\\\epsilon$。预计$E[\\\\mathbf{x}\\'] = \\\\mathbf{x}$。\\n+\\n+在标准的辍学正则化中，每一层都通过被保留（而不是退出）的节点的分数标准化而去借方。换句话说，在*退出概率*$p$的情况下，每个中间激活$h$被随机变量$h\\'$替换，如下所示：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+h\\' =\\n+\\\\begin{cases}\\n+    0 & \\\\text{ with probability } p \\\\\\\\\\n+    \\\\frac{h}{1-p} & \\\\text{ otherwise}\\n+\\\\end{cases}\\n+\\\\end{aligned}\\n+$$\\n+\\n+根据设计，期望值保持不变，即$E[h\\'] = h$。\\n+\\n+## 实践中辍学\\n+\\n+回忆一下在:numref:`fig_mlp`中有一个隐藏层和5个隐藏单元的MLP。当我们将dropout应用于一个隐藏层，以$p$的概率将每个隐藏单元归零，结果可以看作是一个只包含原始神经元子集的网络。在:numref:`fig_dropout2`中，$h_2$和$h_5$被移除。因此，输出的计算不再依赖于$h_2$或$h_5$，并且在执行反向传播时，它们各自的梯度也消失了。这样，输出层的计算不能过度依赖于$h_1, \\\\ldots, h_5$的任何一个元素。\\n+\\n+![MLP before and after dropout.](../img/dropout2.svg)\\n+:label:`fig_dropout2`\\n+\\n+通常，我们在测试时禁用退出。给定一个经过训练的模型和一个新的例子，我们不需要删除任何节点，因此不需要规范化。然而，也有一些例外：一些研究人员将测试时的辍学作为一种启发式方法来估计神经网络预测的不确定性：如果预测在许多不同的辍学掩码上一致，那么我们可以说网络更自信。\\n+\\n+## 从头开始实施\\n+\\n+为了实现单个层的丢失函数，我们必须从Bernoulli（二进制）随机变量中提取与我们层的维数相同的样本，其中随机变量的值为$1$（保持），概率为$1-p$（下降），$p$（下降）。实现这一点的一个简单方法是首先从均匀分布$U[0, 1]$中提取样本。然后我们可以保留那些对应的样本大于$p$的节点，去掉其余的节点。\\n+\\n+在下面的代码中，我们实现了一个`dropout_layer`函数，该函数以概率`X`删除张量输入`X`中的元素，并按上述方式重新缩放余数：将幸存者除以`1.0-dropout`。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import autograd, gluon, init, np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+def dropout_layer(X, dropout):\\n+    assert 0 <= dropout <= 1\\n+    # In this case, all elements are dropped out\\n+    if dropout == 1:\\n+        return np.zeros_like(X)\\n+    # In this case, all elements are kept\\n+    if dropout == 0:\\n+        return X\\n+    mask = np.random.uniform(0, 1, X.shape) > dropout\\n+    return mask.astype(np.float32) * X / (1.0 - dropout)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+\\n+def dropout_layer(X, dropout):\\n+    assert 0 <= dropout <= 1\\n+    # In this case, all elements are dropped out\\n+    if dropout == 1:\\n+        return torch.zeros_like(X)\\n+    # In this case, all elements are kept\\n+    if dropout == 0:\\n+        return X\\n+    mask = (torch.Tensor(X.shape).uniform_(0, 1) > dropout).float()\\n+    return mask * X / (1.0 - dropout)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+def dropout_layer(X, dropout):\\n+    assert 0 <= dropout <= 1\\n+    # In this case, all elements are dropped out\\n+    if dropout == 1:\\n+        return tf.zeros_like(X)\\n+    # In this case, all elements are kept\\n+    if dropout == 0:\\n+        return X\\n+    mask = tf.random.uniform(\\n+        shape=tf.shape(X), minval=0, maxval=1) < 1 - dropout\\n+    return tf.cast(mask, dtype=tf.float32) * X / (1.0 - dropout)\\n+```\\n+\\n+我们可以用几个例子来测试`dropout_layer`函数。在下面的代码行中，我们通过dropout操作传递输入`X`，概率分别为0、0.5和1。\\n+\\n+```{.python .input}\\n+X = np.arange(16).reshape(2, 8)\\n+print(dropout_layer(X, 0))\\n+print(dropout_layer(X, 0.5))\\n+print(dropout_layer(X, 1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+X= torch.arange(16, dtype = torch.float32).reshape((2, 8))\\n+print(X)\\n+print(dropout_layer(X, 0.))\\n+print(dropout_layer(X, 0.5))\\n+print(dropout_layer(X, 1.))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.reshape(tf.range(16, dtype=tf.float32), (2, 8))\\n+print(X)\\n+print(dropout_layer(X, 0.))\\n+print(dropout_layer(X, 0.5))\\n+print(dropout_layer(X, 1.))\\n+```\\n+\\n+### 定义模型参数\\n+\\n+同样，我们使用:numref:`sec_fashion_mnist`中引入的时尚MNIST数据集。我们定义了一个包含两个隐藏层的MLP，每个层包含256个单元。\\n+\\n+```{.python .input}\\n+num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\\n+\\n+W1 = np.random.normal(scale=0.01, size=(num_inputs, num_hiddens1))\\n+b1 = np.zeros(num_hiddens1)\\n+W2 = np.random.normal(scale=0.01, size=(num_hiddens1, num_hiddens2))\\n+b2 = np.zeros(num_hiddens2)\\n+W3 = np.random.normal(scale=0.01, size=(num_hiddens2, num_outputs))\\n+b3 = np.zeros(num_outputs)\\n+\\n+params = [W1, b1, W2, b2, W3, b3]\\n+for param in params:\\n+    param.attach_grad()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+num_outputs, num_hiddens1, num_hiddens2 = 10, 256, 256\\n+```\\n+\\n+### 定义模型\\n+\\n+下面的模型将dropout应用于每个隐藏层的输出（遵循激活函数）。我们可以分别为每一层设置丢失概率。一个常见的趋势是将较低的辍学概率设置为靠近输入层。下面我们分别为第一层和第二层设置0.2和0.5。我们确保退学只在训练期间有效。\\n+\\n+```{.python .input}\\n+dropout1, dropout2 = 0.2, 0.5\\n+\\n+def net(X):\\n+    X = X.reshape(-1, num_inputs)\\n+    H1 = npx.relu(np.dot(X, W1) + b1)\\n+    # Use dropout only when training the model\\n+    if autograd.is_training():\\n+        # Add a dropout layer after the first fully connected layer\\n+        H1 = dropout_layer(H1, dropout1)\\n+    H2 = npx.relu(np.dot(H1, W2) + b2)\\n+    if autograd.is_training():\\n+        # Add a dropout layer after the second fully connected layer\\n+        H2 = dropout_layer(H2, dropout2)\\n+    return np.dot(H2, W3) + b3\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+dropout1, dropout2 = 0.2, 0.5\\n+\\n+class Net(nn.Module):\\n+    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,\\n+                 is_training = True):\\n+        super(Net, self).__init__()\\n+\\n+        self.num_inputs = num_inputs\\n+        self.training = is_training\\n+\\n+        self.lin1 = nn.Linear(num_inputs, num_hiddens1)\\n+        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)\\n+        self.lin3 = nn.Linear(num_hiddens2, num_outputs)\\n+\\n+        self.relu = nn.ReLU()\\n+\\n+    def forward(self, X):\\n+        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))\\n+        # Use dropout only when training the model\\n+        if self.training == True:\\n+            # Add a dropout layer after the first fully connected layer\\n+            H1 = dropout_layer(H1, dropout1)\\n+        H2 = self.relu(self.lin2(H1))\\n+        if self.training == True:\\n+            # Add a dropout layer after the second fully connected layer\\n+            H2 = dropout_layer(H2, dropout2)\\n+        out = self.lin3(H2)\\n+        return out\\n+\\n+\\n+net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+dropout1, dropout2 = 0.2, 0.5\\n+\\n+class Net(tf.keras.Model):\\n+    def __init__(self, num_outputs, num_hiddens1, num_hiddens2):\\n+        super().__init__()\\n+        self.input_layer = tf.keras.layers.Flatten()\\n+        self.hidden1 = tf.keras.layers.Dense(num_hiddens1, activation=\\'relu\\')\\n+        self.hidden2 = tf.keras.layers.Dense(num_hiddens2, activation=\\'relu\\')\\n+        self.output_layer = tf.keras.layers.Dense(num_outputs)\\n+\\n+    def call(self, inputs, training=None):\\n+        x = self.input_layer(inputs)\\n+        x = self.hidden1(x)\\n+        if training:\\n+            x = dropout_layer(x, dropout1)\\n+        x = self.hidden2(x)\\n+        if training:\\n+            x = dropout_layer(x, dropout2)\\n+        x = self.output_layer(x)\\n+        return x\\n+\\n+net = Net(num_outputs, num_hiddens1, num_hiddens2)\\n+```\\n+\\n+### 培训和测试\\n+\\n+这与前面描述的MLP培训和测试类似。\\n+\\n+```{.python .input}\\n+num_epochs, lr, batch_size = 10, 0.5, 256\\n+loss = gluon.loss.SoftmaxCrossEntropyLoss()\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs,\\n+              lambda batch_size: d2l.sgd(params, lr, batch_size))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+num_epochs, lr, batch_size = 10, 0.5, 256\\n+loss = nn.CrossEntropyLoss()\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\\n+trainer = torch.optim.SGD(net.parameters(), lr=lr)\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+num_epochs, lr, batch_size = 10, 0.5, 256\\n+loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\\n+trainer = tf.keras.optimizers.SGD(learning_rate=lr)\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\\n+```\\n+\\n+## 简明实施\\n+\\n+对于高级api，我们只需要在每个完全连接的层之后添加一个`Dropout`层，将丢失概率作为惟一的参数传递给它的构造函数。在训练过程中，`Dropout`层将根据指定的丢失概率随机丢弃前一层的输出（或相当于下一层的输入）。当不处于训练模式时，`Dropout`层只在测试期间传递数据。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nn.Dense(256, activation=\"relu\"),\\n+        # Add a dropout layer after the first fully connected layer\\n+        nn.Dropout(dropout1),\\n+        nn.Dense(256, activation=\"relu\"),\\n+        # Add a dropout layer after the second fully connected layer\\n+        nn.Dropout(dropout2),\\n+        nn.Dense(10))\\n+net.initialize(init.Normal(sigma=0.01))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(nn.Flatten(),\\n+        nn.Linear(784, 256),\\n+        nn.ReLU(),\\n+        # Add a dropout layer after the first fully connected layer\\n+        nn.Dropout(dropout1),\\n+        nn.Linear(256, 256),\\n+        nn.ReLU(),\\n+        # Add a dropout layer after the second fully connected layer\\n+        nn.Dropout(dropout2),\\n+        nn.Linear(256, 10))\\n+\\n+def init_weights(m):\\n+    if type(m) == nn.Linear:\\n+        torch.nn.init.normal_(m.weight, std=0.01)\\n+\\n+net.apply(init_weights)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Flatten(),\\n+    tf.keras.layers.Dense(256, activation=tf.nn.relu),\\n+    # Add a dropout layer after the first fully connected layer\\n+    tf.keras.layers.Dropout(dropout1),\\n+    tf.keras.layers.Dense(256, activation=tf.nn.relu),\\n+    # Add a dropout layer after the second fully connected layer\\n+    tf.keras.layers.Dropout(dropout2),\\n+    tf.keras.layers.Dense(10),\\n+])\\n+```\\n+\\n+接下来，我们对模型进行训练和测试。\\n+\\n+```{.python .input}\\n+trainer = gluon.Trainer(net.collect_params(), \\'sgd\\', {\\'learning_rate\\': lr})\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+trainer = torch.optim.SGD(net.parameters(), lr=lr)\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+trainer = tf.keras.optimizers.SGD(learning_rate=lr)\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\\n+```\\n+\\n+## 摘要\\n+\\n+* 除了控制维数和权重向量的大小之外，dropout是另一种避免过度拟合的工具。它们通常是联合使用的。\\n+* Dropout用期望值为$h$的随机变量替换激活$h$。\\n+* 退学只在训练期间使用。\\n+\\n+## 练习\\n+\\n+1. 如果改变第一层和第二层的退出概率会怎么样？特别是，如果切换两个层的值会怎么样？设计一个实验来回答这些问题，定量地描述你的结果，并总结出定性的结论。\\n+1. 增加epoch的数量，并将使用dropout和不使用dropout时获得的结果进行比较。\\n+1. 当应用和不应用dropout时，每个隐藏层中激活的变化是多少？绘制一个图来显示这两个模型的数量随时间的变化情况。\\n+1. 为什么在考试时通常不使用辍学？\\n+1. 以本节中的模型为例，比较使用dropout和weight decay的效果。当同时使用“辍学”和“体重衰减”时会发生什么情况？结果是相加的吗？回报是否减少（或更糟）？他们互相抵消了吗？\\n+1. 如果我们将dropout应用于权重矩阵的各个权重而不是激活，会发生什么？\\n+1. 发明另一种技术，在每层注入随机噪声，这与标准的丢失技术不同。你能开发出一种在时尚MNIST数据集上表现优于dropout的方法吗（对于一个固定的架构）？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/100)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/101)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/261)\\n+:end_tab:\\ndiff --git a/chapter_multilayer-perceptrons/dropout_tencent.md b/chapter_multilayer-perceptrons/dropout_tencent.md\\nnew file mode 100644\\nindex 00000000..96b4c94f\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/dropout_tencent.md\\n@@ -0,0 +1,382 @@\\n+# 辍学\\n+:label:`sec_dropout`\\n+\\n+在:numref:`sec_weight_decay`中，我们介绍了通过惩罚权重的$L_2$范数来正规化统计模型的经典方法。在概率术语中，我们可以通过以下论证来证明这一技术的合理性：我们已经假设了一个先验信念，即权重取自均值为零的高斯分布的值。更直观的是，我们可能会争辩说，我们鼓励了模型将其权重分散到许多特征中，而不是过于依赖于少数潜在的虚假关联。\\n+\\n+## 重新审视过装问题\\n+\\n+面对更多的功能而不是示例，线性模型往往会过度拟合。但是给出更多的例子而不是特征，我们通常可以指望线性模型不会过度拟合。不幸的是，线性模型泛化的可靠性是有代价的。简单地应用，线性模型没有考虑到特征之间的交互作用。对于每个特征，线性模型必须指定正的或负的权重，而忽略上下文。\\n+\\n+在传统文本中，概括性和灵活性之间的这种基本张力被描述为“偏差-方差权衡”。线性模型有很高的偏差：它们只能表示一小类函数。然而，这些模型的方差很低：它们在不同的随机数据样本上给出了相似的结果。\\n+\\n+深度神经网络位于偏差-方差谱的另一端。与线性模型不同，神经网络并不局限于单独查看每个特征。他们可以学习功能组之间的交互。例如，他们可能推断“尼日利亚”和“西联汇款”一起出现在电子邮件中表示垃圾邮件，但单独出现则不表示垃圾邮件。\\n+\\n+即使我们有比特征多得多的例子，深度神经网络也有能力过度拟合。2017年，一组研究人员通过在随机标记的图像上训练深度网络，展示了神经网络的极大灵活性。尽管没有任何真实的模式将输入和输出联系起来，但他们发现，通过随机梯度下降优化的神经网络可以完美地标记训练集中的每一幅图像。想一想这意味着什么。如果标签是随机均匀分配的，并且有10个类别，那么没有一个分类器可以对抗拒数据进行比10%更高的准确率。这里的泛化差距高达90%。如果我们的模型是如此富有表现力，以至于它们可以如此严重地超配，那么我们应该在什么时候期望它们不会超配呢？\\n+\\n+深层网络令人费解的泛化性质的数学基础仍然是悬而未决的研究问题，我们鼓励面向理论的读者在挖洞上更深入地研究这个主题。目前，我们转向对实际工具的调查，这些工具倾向于经验上改进深层网络的泛化。\\n+\\n+## 通过摄动获得稳健性\\n+\\n+让我们简单地思考一下我们对一个好的预测模型的期望。我们希望它能在看不见的数据上有很好的表现。经典泛化理论认为，为了缩小训练和测试性能之间的差距，我们应该以简单的模型为目标。简单性可以以少量维度的形式出现。我们在:numref:`sec_model_selection`讨论线性模型的单项式基函数时探讨了这一点。此外，正如我们在:numref:`sec_weight_decay`中讨论权重衰减($L_2$正则化)时看到的那样，参数的(逆)范数也代表了一种有用的简单性度量。简单性的另一个有用的概念是平滑性，即函数不应该对其输入的微小变化敏感。例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无害的。\\n+\\n+1995年，克里斯托弗·毕晓普证明了具有输入噪声的训练等价于Tikhonov正则化:cite:`Bishop.1995`，从而将这一观点形式化。这项工作在要求函数光滑(因而简单)和要求它对输入中的扰动具有弹性之间有了明确的数学联系。\\n+\\n+然后，在2014年，斯里瓦斯塔瓦等人。:cite:`Srivastava.Hinton.Krizhevsky.ea.2014`还就如何将毕晓普的想法应用于网络的内部层提出了一个聪明的想法。也就是说，在训练过程中，他们建议在计算后续层之前向网络的每一层注入噪声。他们意识到，当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。\\n+\\n+他们的想法被称为“丢弃”(Dropout)，涉及在前向传播过程中计算每一内部层的同时注入噪声，这已经成为训练神经网络的标准技术。这种方法被称为“Dropout”，因为我们从字面上看\\n+*在训练过程中脱落*一些神经元。\\n+在整个训练过程中，在每一次迭代中，标准丢弃包括在计算下一层之前将每一层中的一些节点置零。\\n+\\n+需要说明的是，我们将自己的叙述与毕晓普联系起来。关于辍学的原始论文通过一个出人意料的有性繁殖类比提供了直觉。作者认为，神经网络过拟合的特征是每一层都依赖于前一层激活的特定模式，称这种情况为“共同适应”。他们声称，辍学会破坏共同适应，就像有性生殖会破坏共同适应的基因一样。\\n+\\n+那么关键的挑战就是如何注入这种噪音。一种想法是以一种“无偏见”的方式注入噪音，这样每一层的期望值-同时固定其他层-等于没有噪音时的值。\\n+\\n+在毕晓普的工作中，他将高斯噪声添加到线性模型的输入中。在每次训练迭代中，他将从均值为零的分布$\\\\epsilon \\\\sim \\\\mathcal{N}(0,\\\\sigma^2)$采样的噪声添加到输入$\\\\mathbf{x}$，从而产生扰动点$\\\\mathbf{x}\\' = \\\\mathbf{x} + \\\\epsilon$。在预期中，是$E[\\\\mathbf{x}\\'] = \\\\mathbf{x}$。\\n+\\n+在标准丢弃正则化中，通过按保留(未丢弃)的节点的分数进行归一化来消除每一层的偏差。换言之，在*丢弃概率*$p$的情况下，每个中间激活$h$由随机变量$h\\'$替换，如下所示：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+h\\' =\\n+\\\\begin{cases}\\n+    0 & \\\\text{ with probability } p \\\\\\\\\\n+    \\\\frac{h}{1-p} & \\\\text{ otherwise}\\n+\\\\end{cases}\\n+\\\\end{aligned}\\n+$$\\n+\\n+根据设计，期望值保持不变，即$E[h\\'] = h$。\\n+\\n+## 实践中的辍学\\n+\\n+回想一下:numref:`fig_mlp`中带有一个隐藏层和5个隐藏单元的mlp。当我们将丢弃应用到隐层，以$p$的概率将每个隐藏单元归零时，结果可以看作是一个只包含原始神经元子集的网络。在:numref:`fig_dropout2`中，删除了$h_2$和$h_5$。因此，输出的计算不再依赖于$h_2$或$h_5$，并且它们各自的梯度在执行反向传播时也会消失。这样，输出层的计算不能过度依赖于$h_1, \\\\ldots, h_5$的任何一个元素。\\n+\\n+![MLP before and after dropout.](../img/dropout2.svg)\\n+:label:`fig_dropout2`\\n+\\n+通常，我们在测试时禁用辍学。给定一个训练好的模型和一个新的例子，我们不会遗漏任何节点，因此不需要规范化。然而，也有一些例外：一些研究人员使用测试时的辍学作为估计神经网络预测的“不确定性”的启发式方法：如果预测在许多不同的辍学掩码上都是一致的，那么我们可以说网络更有信心。\\n+\\n+## 从头开始实施\\n+\\n+要实现单层的丢弃函数，我们必须从伯努利(二进制)随机变量中提取与我们的层的维度一样多的样本，其中随机变量取值$1$(保持)，概率为$1-p$，值为$0$(丢弃)，概率为$p$。实现这一点的一种简单方式是首先从均匀分布$U[0, 1]$中抽取样本。那么我们可以保留那些对应样本大于$p$的节点，丢弃睡觉。\\n+\\n+在下面的代码中，我们实现了一个`dropout_layer`函数，该函数以`dropout`的概率丢弃张量输入`X`中的元素，如上所述重新缩放剩余部分：将幸存者除以`1.0-dropout`。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import autograd, gluon, init, np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+\\n+def dropout_layer(X, dropout):\\n+    assert 0 <= dropout <= 1\\n+    # In this case, all elements are dropped out\\n+    if dropout == 1:\\n+        return np.zeros_like(X)\\n+    # In this case, all elements are kept\\n+    if dropout == 0:\\n+        return X\\n+    mask = np.random.uniform(0, 1, X.shape) > dropout\\n+    return mask.astype(np.float32) * X / (1.0 - dropout)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+\\n+def dropout_layer(X, dropout):\\n+    assert 0 <= dropout <= 1\\n+    # In this case, all elements are dropped out\\n+    if dropout == 1:\\n+        return torch.zeros_like(X)\\n+    # In this case, all elements are kept\\n+    if dropout == 0:\\n+        return X\\n+    mask = (torch.Tensor(X.shape).uniform_(0, 1) > dropout).float()\\n+    return mask * X / (1.0 - dropout)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+def dropout_layer(X, dropout):\\n+    assert 0 <= dropout <= 1\\n+    # In this case, all elements are dropped out\\n+    if dropout == 1:\\n+        return tf.zeros_like(X)\\n+    # In this case, all elements are kept\\n+    if dropout == 0:\\n+        return X\\n+    mask = tf.random.uniform(\\n+        shape=tf.shape(X), minval=0, maxval=1) < 1 - dropout\\n+    return tf.cast(mask, dtype=tf.float32) * X / (1.0 - dropout)\\n+```\\n+\\n+我们可以通过几个示例来测试`dropout_layer`函数。在下面的代码行中，我们通过丢弃操作传递输入`X`，概率分别为0、0.5和1。\\n+\\n+```{.python .input}\\n+X = np.arange(16).reshape(2, 8)\\n+print(dropout_layer(X, 0))\\n+print(dropout_layer(X, 0.5))\\n+print(dropout_layer(X, 1))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+X= torch.arange(16, dtype = torch.float32).reshape((2, 8))\\n+print(X)\\n+print(dropout_layer(X, 0.))\\n+print(dropout_layer(X, 0.5))\\n+print(dropout_layer(X, 1.))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+X = tf.reshape(tf.range(16, dtype=tf.float32), (2, 8))\\n+print(X)\\n+print(dropout_layer(X, 0.))\\n+print(dropout_layer(X, 0.5))\\n+print(dropout_layer(X, 1.))\\n+```\\n+\\n+### 定义模型参数\\n+\\n+同样，我们使用:numref:`sec_fashion_mnist`中引入的Fashion-MNIST数据集。我们定义具有两个隐藏层的MLP，每个隐藏层包含256个单元。\\n+\\n+```{.python .input}\\n+num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\\n+\\n+W1 = np.random.normal(scale=0.01, size=(num_inputs, num_hiddens1))\\n+b1 = np.zeros(num_hiddens1)\\n+W2 = np.random.normal(scale=0.01, size=(num_hiddens1, num_hiddens2))\\n+b2 = np.zeros(num_hiddens2)\\n+W3 = np.random.normal(scale=0.01, size=(num_hiddens2, num_outputs))\\n+b3 = np.zeros(num_outputs)\\n+\\n+params = [W1, b1, W2, b2, W3, b3]\\n+for param in params:\\n+    param.attach_grad()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+num_outputs, num_hiddens1, num_hiddens2 = 10, 256, 256\\n+```\\n+\\n+### 定义模型\\n+\\n+下面的模型将Dropout应用于每个隐藏层的输出(遵循激活函数)。我们可以分别为每一层设置丢弃概率。一种常见的趋势是将丢弃概率设置得更接近输入层。下面，我们将第一个和第二个隐藏层的值分别设置为0.2和0.5。我们确保退学只在训练期间有效。\\n+\\n+```{.python .input}\\n+dropout1, dropout2 = 0.2, 0.5\\n+\\n+def net(X):\\n+    X = X.reshape(-1, num_inputs)\\n+    H1 = npx.relu(np.dot(X, W1) + b1)\\n+    # Use dropout only when training the model\\n+    if autograd.is_training():\\n+        # Add a dropout layer after the first fully connected layer\\n+        H1 = dropout_layer(H1, dropout1)\\n+    H2 = npx.relu(np.dot(H1, W2) + b2)\\n+    if autograd.is_training():\\n+        # Add a dropout layer after the second fully connected layer\\n+        H2 = dropout_layer(H2, dropout2)\\n+    return np.dot(H2, W3) + b3\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+dropout1, dropout2 = 0.2, 0.5\\n+\\n+class Net(nn.Module):\\n+    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,\\n+                 is_training = True):\\n+        super(Net, self).__init__()\\n+\\n+        self.num_inputs = num_inputs\\n+        self.training = is_training\\n+\\n+        self.lin1 = nn.Linear(num_inputs, num_hiddens1)\\n+        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)\\n+        self.lin3 = nn.Linear(num_hiddens2, num_outputs)\\n+\\n+        self.relu = nn.ReLU()\\n+\\n+    def forward(self, X):\\n+        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))\\n+        # Use dropout only when training the model\\n+        if self.training == True:\\n+            # Add a dropout layer after the first fully connected layer\\n+            H1 = dropout_layer(H1, dropout1)\\n+        H2 = self.relu(self.lin2(H1))\\n+        if self.training == True:\\n+            # Add a dropout layer after the second fully connected layer\\n+            H2 = dropout_layer(H2, dropout2)\\n+        out = self.lin3(H2)\\n+        return out\\n+\\n+\\n+net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+dropout1, dropout2 = 0.2, 0.5\\n+\\n+class Net(tf.keras.Model):\\n+    def __init__(self, num_outputs, num_hiddens1, num_hiddens2):\\n+        super().__init__()\\n+        self.input_layer = tf.keras.layers.Flatten()\\n+        self.hidden1 = tf.keras.layers.Dense(num_hiddens1, activation=\\'relu\\')\\n+        self.hidden2 = tf.keras.layers.Dense(num_hiddens2, activation=\\'relu\\')\\n+        self.output_layer = tf.keras.layers.Dense(num_outputs)\\n+\\n+    def call(self, inputs, training=None):\\n+        x = self.input_layer(inputs)\\n+        x = self.hidden1(x)\\n+        if training:\\n+            x = dropout_layer(x, dropout1)\\n+        x = self.hidden2(x)\\n+        if training:\\n+            x = dropout_layer(x, dropout2)\\n+        x = self.output_layer(x)\\n+        return x\\n+\\n+net = Net(num_outputs, num_hiddens1, num_hiddens2)\\n+```\\n+\\n+### 培训和测试\\n+\\n+这类似于前面描述的MLP的培训和测试。\\n+\\n+```{.python .input}\\n+num_epochs, lr, batch_size = 10, 0.5, 256\\n+loss = gluon.loss.SoftmaxCrossEntropyLoss()\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs,\\n+              lambda batch_size: d2l.sgd(params, lr, batch_size))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+num_epochs, lr, batch_size = 10, 0.5, 256\\n+loss = nn.CrossEntropyLoss()\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\\n+trainer = torch.optim.SGD(net.parameters(), lr=lr)\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+num_epochs, lr, batch_size = 10, 0.5, 256\\n+loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\\n+trainer = tf.keras.optimizers.SGD(learning_rate=lr)\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\\n+```\\n+\\n+## 简明实施\\n+\\n+对于高级API，我们所需要做的就是在每个完全连接的层之后添加一个`Dropout`层，将丢弃概率作为唯一的参数传递给它的构造函数。在训练过程中，`Dropout`层将根据指定的丢弃概率随机丢弃上一层的输出(或相当于对下一层的输入)。当不处于训练模式时，`Dropout`层仅在测试期间传递数据。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nn.Dense(256, activation=\"relu\"),\\n+        # Add a dropout layer after the first fully connected layer\\n+        nn.Dropout(dropout1),\\n+        nn.Dense(256, activation=\"relu\"),\\n+        # Add a dropout layer after the second fully connected layer\\n+        nn.Dropout(dropout2),\\n+        nn.Dense(10))\\n+net.initialize(init.Normal(sigma=0.01))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(nn.Flatten(),\\n+        nn.Linear(784, 256),\\n+        nn.ReLU(),\\n+        # Add a dropout layer after the first fully connected layer\\n+        nn.Dropout(dropout1),\\n+        nn.Linear(256, 256),\\n+        nn.ReLU(),\\n+        # Add a dropout layer after the second fully connected layer\\n+        nn.Dropout(dropout2),\\n+        nn.Linear(256, 10))\\n+\\n+def init_weights(m):\\n+    if type(m) == nn.Linear:\\n+        torch.nn.init.normal_(m.weight, std=0.01)\\n+\\n+net.apply(init_weights)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Flatten(),\\n+    tf.keras.layers.Dense(256, activation=tf.nn.relu),\\n+    # Add a dropout layer after the first fully connected layer\\n+    tf.keras.layers.Dropout(dropout1),\\n+    tf.keras.layers.Dense(256, activation=tf.nn.relu),\\n+    # Add a dropout layer after the second fully connected layer\\n+    tf.keras.layers.Dropout(dropout2),\\n+    tf.keras.layers.Dense(10),\\n+])\\n+```\\n+\\n+接下来，我们对模型进行训练和测试。\\n+\\n+```{.python .input}\\n+trainer = gluon.Trainer(net.collect_params(), \\'sgd\\', {\\'learning_rate\\': lr})\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+trainer = torch.optim.SGD(net.parameters(), lr=lr)\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+trainer = tf.keras.optimizers.SGD(learning_rate=lr)\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\\n+```\\n+\\n+## 摘要\\n+\\n+* 除了控制权重向量的维数和大小之外，丢弃也是避免过度拟合的另一种工具。它们通常是联合使用的。\\n+* 退出将激活$h$替换为具有期望值$h$的随机变量。\\n+* 退学仅在训练期间使用。\\n+\\n+## 练习\\n+\\n+1. 如果更改第一层和第二层的辍学概率，会发生什么情况？具体地说，如果为两个层都切换这两个层，会发生什么情况？设计一个实验来回答这些问题，定量描述你的结果，并总结定性的结论。\\n+1. 增加历元数，并将使用Dropout和不使用Dropout时获得的结果进行比较。\\n+1. 当应用和不应用Dropout时，每个隐藏层中的激活差异是什么？绘制一个曲线图，以显示这两个模型的此量是如何随时间演变的。\\n+1. 为什么在测试时通常不使用辍学？\\n+1. 以本节中的模型为例，比较使用丢弃和权重衰减的效果。如果同时使用辍学和体重衰减，会发生什么情况？结果是累加的吗？是否存在回报递减(或更糟)？它们会相互抵消吗？\\n+1. 如果我们将丢弃应用到权重矩阵的各个权重，而不是激活，会发生什么？\\n+1. 发明另一种用于在每一层注入随机噪声的技术，该技术不同于标准的丢弃技术。您能否开发一种在Fashion-MNIST数据集(对于固定体系结构)上性能优于Dropout的方法？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/100)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/101)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/261)\\n+:end_tab:\\ndiff --git a/chapter_multilayer-perceptrons/environment_baidu.md b/chapter_multilayer-perceptrons/environment_baidu.md\\nnew file mode 100644\\nindex 00000000..25a8a308\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/environment_baidu.md\\n@@ -0,0 +1,252 @@\\n+# 环境与分销转移\\n+\\n+在前面的部分中，我们学习了机器学习的许多实际应用，将模型拟合到各种数据集。然而，我们从来没有停下来思考数据从哪里来，或者我们计划最终如何处理模型的输出。通常情况下，拥有数据的机器学习开发人员急于开发模型，而不停下来考虑这些基本问题。\\n+\\n+许多失败的机器学习部署都可以追溯到这种模式。有时，根据测试集的准确度衡量，模型表现得非常出色，但是当数据分布突然改变时，模型在部署中会出现灾难性的失败。更阴险的是，有时模型的部署本身就是扰乱数据分布的催化剂。例如，我们训练了一个模型来预测谁将偿还贷款而不是违约，发现申请人选择的鞋子与违约风险相关（牛津鞋表示还款，运动鞋表示违约）。此后，我们可能倾向于向所有穿着牛津鞋的申请人发放贷款，并拒绝所有穿着运动鞋的申请人。\\n+\\n+在这种情况下，我们从模式识别到决策的未经深思熟虑的飞跃，以及我们未能对环境进行批判性考虑，可能会带来灾难性的后果。首先，一旦我们开始根据鞋类做出决定，顾客就会理解并改变他们的行为。不久，所有的申请者都会穿牛津鞋，而信用度却没有相应的提高。花点时间来理解这一点，因为机器学习的许多应用中都存在类似的问题：通过将基于模型的决策引入到环境中，我们可能会破坏模型。\\n+\\n+虽然我们不可能在一节中完整地讨论这些主题，但我们的目的是揭示一些常见的问题，并激发批判性思维，以便及早发现这些情况，减轻损害，负责任地使用机器学习。有些解决方案很简单（要求“正确”的数据），有些在技术上很困难（实施强化学习系统），还有一些解决方案要求我们完全跳出统计预测的领域，解决有关算法伦理应用的哲学难题。\\n+\\n+## 分配转移类型\\n+\\n+首先，我们坚持使用被动预测设置，考虑到数据分布可能发生变化的各种方式，以及为挽救模型性能可能采取的措施。在一个经典的设置中，我们假设我们的训练数据是从某个分布$p_S(\\\\mathbf{x},y)$中采样的，但是我们的测试数据将包含从不同分布$p_T(\\\\mathbf{x},y)$中提取的未标记示例。我们必须面对一个清醒的现实。如果没有任何关于$p_S$和$p_T$之间相互关系的假设，学习一个健壮的分类器是不可能的。\\n+\\n+考虑一个二元分类问题，我们希望区分狗和猫。如果分布可以以任意方式移动，那么我们的设置允许输入上的分布保持不变的病态情况：$p_S(\\\\mathbf{x}) = p_T(\\\\mathbf{x})$，但标签全部翻转：$p_S(y | \\\\mathbf{x}) = 1 - p_T(y | \\\\mathbf{x})$。换言之，如果上帝能突然决定，将来所有的“猫”现在都是狗，而我们以前所说的“狗”现在是猫——而输入的分布没有任何改变，那么我们就不可能将这种设置与分布完全没有变化的设置区分开。\\n+\\n+幸运的是，在对数据未来可能发生变化的某些限制性假设下，原则性算法可以检测到移位，有时甚至可以动态调整，从而提高了原始分类器的精度。\\n+\\n+### 协变量移位\\n+\\n+在分布转移的分类中，协变量转移可能是研究得最广泛的。这里，我们假设，虽然输入的分布可能随时间而改变，但标记函数，即条件分布$P(y \\\\mid \\\\mathbf{x})$没有改变。统计学家称之为“协变量转移”，因为这个问题是由于协变量（特征）分布的变化而产生的。虽然有时我们可以在不调用因果关系的情况下对分布转移进行推理，但我们注意到，在我们认为$\\\\mathbf{x}$导致$y$的情况下，协变量转移是一种自然假设。\\n+\\n+考虑一下区分猫和狗的挑战。我们的训练数据可能包括:numref:`fig_cat-dog-train`中的图像。\\n+\\n+![Training data for distinguishing cats and dogs.](../img/cat-dog-train.svg)\\n+:label:`fig_cat-dog-train`\\n+\\n+在测试时，我们被要求对:numref:`fig_cat-dog-test`中的图像进行分类。\\n+\\n+![Test data for distinguishing cats and dogs.](../img/cat-dog-test.svg)\\n+:label:`fig_cat-dog-test`\\n+\\n+训练集由照片组成，而测试集只包含卡通。在一个与测试集有着本质上不同特征的数据集上进行训练，如果没有一个一致的计划来适应新的领域，可能会带来麻烦。\\n+\\n+### 标签移位\\n+\\n+*标签shift*描述了逆向问题。\\n+这里，我们假设标签边距$P(y)$可以更改，但是类条件分布$P(\\\\mathbf{x} \\\\mid y)$在域之间保持不变。当我们认为$y$导致$\\\\mathbf{x}$时，标签偏移是一个合理的假设。例如，我们可能希望根据症状（或其他表现）来预测诊断，即使诊断的相对流行率随着时间的推移而变化。标签转移是恰当的假设，因为疾病会导致症状。在一些退化的情况下，标签移位和协变量移位假设可以同时成立。例如，当标签是确定性的，即使$y$导致$\\\\mathbf{x}$，协变量移位假设也会得到满足。有趣的是，在这些情况下，使用来自标签移位假设的方法通常是有利的。这是因为这些方法倾向于操作看起来像标签（通常是低维）的对象，而不是像输入的对象，后者在深度学习中往往是高维的。\\n+\\n+### 观念转变\\n+\\n+我们也可能会遇到“概念转移”的相关问题，当标签的定义发生变化时，就会出现这种问题。这听起来很奇怪——猫是猫，不是吗？但是，其他类别的使用会随着时间的推移而发生变化。精神疾病的诊断标准，被认为是时尚的，以及职称，都会受到相当数量的观念转变的影响。事实证明，如果我们环游美国，根据地理位置改变我们的数据来源，我们会发现关于*软饮料*名称分布的概念发生了相当大的转变，如:numref:`fig_popvssoda`所示。\\n+\\n+![Concept shift on soft drink names in the United States.](../img/popvssoda.png)\\n+:width:`400px`\\n+:label:`fig_popvssoda`\\n+\\n+如果我们要建立一个机器翻译系统，$P(y \\\\mid \\\\mathbf{x})$的分布可能会因我们的位置不同而有所不同。这个问题很难发现。我们可能希望利用这种知识，即转移只会在时间或地理意义上逐渐发生。\\n+\\n+## 分布转移示例\\n+\\n+在深入研究形式主义和算法之前，我们可以讨论一些具体情况，其中协变量或概念转移可能并不明显。\\n+\\n+### 医学诊断\\n+\\n+假设你想设计一个检测癌症的算法。你从健康人和病人那里收集数据，然后训练你的算法。它工作得很好，给你很高的准确性，你的结论是，你已经准备好在医疗诊断事业的成功。\\n+*别这么快*\\n+\\n+产生训练数据的分布和你在野外遇到的分布可能有很大的不同。这件事发生在一个不幸的创业公司身上，我们中的一些人（作者）几年前合作过。他们正在研究一种主要影响老年男性的疾病的血液检测方法，并希望利用他们从病人身上采集的血液样本进行研究。然而，从健康男性身上获取血样比系统中已有的病人要困难得多。作为补偿，这家初创公司向一所大学校园内的学生征集献血，作为开发测试的健康对照。然后他们问我们是否可以帮助他们建立一个用于检测疾病的分类器。\\n+\\n+正如我们向他们解释的那样，用近乎完美的准确度来区分健康和患病人群确实很容易。然而，这是因为受试者在年龄、激素水平、体力活动、饮食、饮酒以及其他许多与疾病无关的因素上存在差异。这不太可能是真正的病人。由于他们的抽样程序，我们可能会遇到极端的协变量变化。此外，这种情况不太可能通过常规方法加以纠正。简言之，他们浪费了一大笔钱。\\n+\\n+### 自动驾驶汽车\\n+\\n+比如一家公司想利用机器学习来开发自动驾驶汽车。这里的一个关键部件是路边探测器。由于真实的注释数据的获取成本很高，他们想出了一个（聪明而可疑的）想法，将游戏渲染引擎中的合成数据用作额外的训练数据。这对从渲染引擎中提取的“测试数据”非常有效。唉，在一辆真正的汽车里真是一场灾难。结果，路边被渲染成一种非常简单的纹理。更重要的是，所有的路边都被渲染成了相同的纹理，路边探测器很快就知道了这个“特征”。\\n+\\n+当美军第一次试图在森林中探测坦克时，也发生了类似的事情。他们在没有坦克的情况下拍摄了森林的航拍照片，然后把坦克开进森林，拍摄了另一组照片。分类器似乎工作得很好。不幸的是，它仅仅学会了如何区分有阴影的树和没有阴影的树——第一组照片是在清晨拍摄的，第二组是在中午拍摄的。\\n+\\n+### 非平稳分布\\n+\\n+当分布变化缓慢（也称为*非平稳分布*）并且模型没有得到充分更新时，就会出现更微妙的情况。以下是一些典型案例。\\n+\\n+* 我们训练了一个计算广告模型，但却没有经常更新（例如，我们忘记了一个叫iPad的不知名的新设备刚刚上市）。\\n+* 我们建立了一个垃圾邮件过滤器。它能很好地检测到我们目前看到的所有垃圾邮件。但是，垃圾邮件发送者们变得聪明起来，制造出新的信息，看起来不像我们以前见过的任何东西。\\n+* 我们建立了一个产品推荐系统。它在整个冬天都有效，但在圣诞节之后很长一段时间里，它仍然推荐圣诞帽。\\n+\\n+### 更多轶事\\n+\\n+* 我们建造了一个面部探测器。它在所有基准测试中都能很好地工作。不幸的是，它在测试数据上失败了——有问题的例子是面部填充整个图像的特写镜头（训练集中没有这样的数据）。\\n+* 我们为美国市场建立了一个网络搜索引擎，并希望将其部署到英国。\\n+* 我们通过编译一个大的数据集来训练图像分类器，其中一个大类集中的每一个都在数据集中平均表示，比如1000个类别，每个类别由1000个图像表示。然后我们将该系统部署到真实世界中，照片的实际标签分布显然是不均匀的。\\n+\\n+## 分布偏移校正\\n+\\n+正如我们所讨论的，在许多情况下，培训和测试分布$P(\\\\mathbf{x}, y)$是不同的。在某些情况下，我们很幸运，不管协变量、标签或概念发生变化，模型都能正常工作。在其他情况下，我们可以通过运用有原则的策略来应对这种转变而做得更好。本节的其余部分将变得更加技术化。不耐烦的读者可以继续下一节，因为这些材料不是后续概念的先决条件。\\n+\\n+### 经验风险与真实风险\\n+\\n+让我们首先思考一下在模型训练期间到底发生了什么：我们迭代训练数据$\\\\{(\\\\mathbf{x}_1, y_1), \\\\ldots, (\\\\mathbf{x}_n, y_n)\\\\}$的特性和相关的标签，并在每一个小批量之后更新$f$模型的参数。为了简单起见，我们不考虑正则化，因此我们在很大程度上减少了培训损失：\\n+\\n+$$\\\\mathop{\\\\mathrm{minimize}}_f \\\\frac{1}{n} \\\\sum_{i=1}^n l(f(\\\\mathbf{x}_i), y_i),$$\\n+:eqlabel:`eq_empirical-risk-min`\\n+\\n+其中$l$是测量“多坏”的损失函数，预测$f(\\\\mathbf{x}_i)$被赋予相关标签$y_i$。统计学家称之为:eqref:`eq_empirical-risk-min`*经验风险*。\\n+*经验风险*是训练数据的平均损失\\n+为了近似*真实风险*，即从其真实分布$p(\\\\mathbf{x},y)$中提取的整个数据总体损失的预期值：\\n+\\n+$$E_{p(\\\\mathbf{x}, y)} [l(f(\\\\mathbf{x}), y)] = \\\\int\\\\int l(f(\\\\mathbf{x}), y) p(\\\\mathbf{x}, y) \\\\;d\\\\mathbf{x}dy.$$\\n+:eqlabel:`eq_true-risk`\\n+\\n+然而，在实践中，我们通常无法获得全部数据。因此，在:eqref:`eq_empirical-risk-min`中，*经验风险最小化*是一种实用的机器学习策略，希望能近似最小化真实风险。\\n+\\n+### 协变量移位校正\\n+:label:`subsec_covariate-shift-correction`\\n+\\n+假设我们要估计一些依赖项$P(y \\\\mid \\\\mathbf{x})$，我们已经为其标记了数据$(\\\\mathbf{x}_i, y_i)$。不幸的是，观察值$\\\\mathbf{x}_i$是从某些*源分布*$q(\\\\mathbf{x})$中得出的，而不是*目标分布*$p(\\\\mathbf{x})$。幸运的是，依赖性假设意味着条件分布不变：$p(y \\\\mid \\\\mathbf{x}) = q(y \\\\mid \\\\mathbf{x})$。如果源分布$q(\\\\mathbf{x})$是“错误的”，我们可以通过在真实风险中使用以下简单标识来更正：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+\\\\int\\\\int l(f(\\\\mathbf{x}), y) p(y \\\\mid \\\\mathbf{x})p(\\\\mathbf{x}) \\\\;d\\\\mathbf{x}dy =\\n+\\\\int\\\\int l(f(\\\\mathbf{x}), y) q(y \\\\mid \\\\mathbf{x})q(\\\\mathbf{x})\\\\frac{p(\\\\mathbf{x})}{q(\\\\mathbf{x})} \\\\;d\\\\mathbf{x}dy.\\n+\\\\end{aligned}\\n+$$\\n+\\n+换言之，我们需要根据从正确分布中提取的概率与从错误分布中提取的概率比率来重新衡量每个数据示例：\\n+\\n+$$\\\\beta_i \\\\stackrel{\\\\mathrm{def}}{=} \\\\frac{p(\\\\mathbf{x}_i)}{q(\\\\mathbf{x}_i)}.$$\\n+\\n+插入每个数据示例$(\\\\mathbf{x}_i, y_i)$的权重$\\\\beta_i$，我们可以使用\\n+*加权经验风险最小化*：\\n+\\n+$$\\\\mathop{\\\\mathrm{minimize}}_f \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\beta_i l(f(\\\\mathbf{x}_i), y_i).$$\\n+:eqlabel:`eq_weighted-empirical-risk-min`\\n+\\n+唉，我们不知道这个比率，所以在我们可以做任何有用的事情之前，我们需要估计它。有许多方法是可行的，包括一些奇特的算子理论方法，试图直接使用最小范数或最大熵原理重新校准期望算子。注意，对于任何这样的方法，我们需要从两个分布中提取样本——“真实”的$p$，例如，通过访问测试数据，以及用于生成训练集$q$的样本（后者很容易获得）。但是请注意，我们只需要特性$\\\\mathbf{x} \\\\sim p(\\\\mathbf{x})$；我们不需要访问标签$y \\\\sim p(y)$。\\n+\\n+在这种情况下，有一种非常有效的方法可以得到几乎与原始方法一样好的结果：logistic回归，这是用于二元分类的softmax回归（见:numref:`sec_softmax`）的特例。这就是计算估计概率比所需的全部内容。我们学习一个分类器来区分$p(\\\\mathbf{x})$和$q(\\\\mathbf{x})$中的数据。如果无法区分这两个分布，则意味着关联实例同样可能来自两个分布中的任何一个。另一方面，任何能够被很好地区分的实例都应该相应地显著地过多或过轻。\\n+\\n+为了简单起见，假设我们分别从$p(\\\\mathbf{x})$和$q(\\\\mathbf{x})$两个发行版中拥有相同数量的实例。现在用$z$表示$1$标签，$p$中的数据为$-1$，$q$的数据为$-1$。然后，混合数据集中的概率由下式给出\\n+\\n+$$P(z=1 \\\\mid \\\\mathbf{x}) = \\\\frac{p(\\\\mathbf{x})}{p(\\\\mathbf{x})+q(\\\\mathbf{x})} \\\\text{ and hence } \\\\frac{P(z=1 \\\\mid \\\\mathbf{x})}{P(z=-1 \\\\mid \\\\mathbf{x})} = \\\\frac{p(\\\\mathbf{x})}{q(\\\\mathbf{x})}.$$\\n+\\n+因此，如果我们使用logistic回归方法，其中$P(z=1 \\\\mid \\\\mathbf{x})=\\\\frac{1}{1+\\\\exp(-h(\\\\mathbf{x}))}$（$h$是一个参数化函数），则如下所示\\n+\\n+$$\\n+\\\\beta_i = \\\\frac{1/(1 + \\\\exp(-h(\\\\mathbf{x}_i)))}{\\\\exp(-h(\\\\mathbf{x}_i))/(1 + \\\\exp(-h(\\\\mathbf{x}_i)))} = \\\\exp(h(\\\\mathbf{x}_i)).\\n+$$\\n+\\n+因此，我们需要解决两个问题：第一个问题是区分来自两个分布的数据，然后是:eqref:`eq_weighted-empirical-risk-min`中的加权经验风险最小化问题，在:eqref:`eq_weighted-empirical-risk-min`中我们用$\\\\beta_i$加权。\\n+\\n+现在我们准备描述一个校正算法。假设我们有一个训练集$\\\\{(\\\\mathbf{x}_1, y_1), \\\\ldots, (\\\\mathbf{x}_n, y_n)\\\\}$和一个未标记的测试集$\\\\{\\\\mathbf{u}_1, \\\\ldots, \\\\mathbf{u}_m\\\\}$。对于协变量转移，我们假设$1 \\\\leq i \\\\leq n$的$\\\\mathbf{x}_i$来自某个源分布，$\\\\mathbf{u}_i$来自目标分布。以下是校正协变量偏移的典型算法：\\n+\\n+1. 生成一个二进制分类训练集：$\\\\{(\\\\mathbf{x}_1, -1), \\\\ldots, (\\\\mathbf{x}_n, -1), (\\\\mathbf{u}_1, 1), \\\\ldots, (\\\\mathbf{u}_m, 1)\\\\}$。\\n+1. 用logistic回归训练二元分类器得到函数$h$。\\n+1. 使用$\\\\beta_i = \\\\exp(h(\\\\mathbf{x}_i))$或更好的$c$对某些常量$c$进行加权。\\n+1. 使用砝码$\\\\beta_i$进行:eqref:`eq_weighted-empirical-risk-min`中$\\\\{(\\\\mathbf{x}_1, y_1), \\\\ldots, (\\\\mathbf{x}_n, y_n)\\\\}$的训练。\\n+\\n+基于上述一个关键的假设。为了使该方案有效，我们需要目标（例如，测试时间）分布中的每个数据实例在训练时发生的概率不为零。如果我们找到$p(\\\\mathbf{x}) > 0$而$q(\\\\mathbf{x}) = 0$的一个点，那么相应的重要性权重应该是无穷大的。\\n+\\n+### 标签移位校正\\n+\\n+假设我们处理的是$k$个类别的分类任务。:numref:`subsec_covariate-shift-correction`、$q$和$p$中使用的相同符号分别是源分布（例如，训练时间）和目标分布（例如，测试时间）。假设标签的分布随时间变化：$q(y) \\\\neq p(y)$，但类条件分布保持不变：$q(\\\\mathbf{x} \\\\mid y)=p(\\\\mathbf{x} \\\\mid y)$。如果来源分布$q(y)$是“错误的”，我们可以根据:eqref:`eq_true-risk`定义的真实风险中的以下同一性进行更正：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+\\\\int\\\\int l(f(\\\\mathbf{x}), y) p(\\\\mathbf{x} \\\\mid y)p(y) \\\\;d\\\\mathbf{x}dy =\\n+\\\\int\\\\int l(f(\\\\mathbf{x}), y) q(\\\\mathbf{x} \\\\mid y)q(y)\\\\frac{p(y)}{q(y)} \\\\;d\\\\mathbf{x}dy.\\n+\\\\end{aligned}\\n+$$\\n+\\n+这里，我们的重要性权重将对应于标签似然比\\n+\\n+$$\\\\beta_i \\\\stackrel{\\\\mathrm{def}}{=} \\\\frac{p(y_i)}{q(y_i)}.$$\\n+\\n+标签转移的一个好处是，如果我们在源分布上有一个相当好的模型，那么我们就可以得到这些权重的一致估计，而不必处理环境维度。在深度学习中，输入往往是像图像这样的高维对象，而标签通常是更简单的对象，比如类别。\\n+\\n+为了估计目标标签的分布，我们首先使用我们相当好的现成分类器（通常在训练数据上进行训练），并使用验证集（也来自训练分布）计算其混淆矩阵。混淆矩阵$\\\\mathbf{C}$只是一个$k \\\\times k$矩阵，其中每列对应于标签类别（基本真相），每行对应于我们模型的预测类别。每个单元格的值$c_{ij}$是验证集中总预测的分数，其中真实标签为$j$，我们的模型预测为$i$。\\n+\\n+现在，我们不能直接计算目标数据上的混淆矩阵，因为我们无法看到我们在野外看到的示例的标签，除非我们投资于一个复杂的实时注释管道。然而，我们所能做的是将所有模型在测试时的预测平均起来，得到平均模型输出$\\\\mu(\\\\hat{\\\\mathbf{y}}) \\\\in \\\\mathbb{R}^k$，其$i^\\\\mathrm{th}$元素$\\\\mu(\\\\hat{y}_i)$是我们模型预测$i$的测试集中总预测的分数。\\n+\\n+结果表明，在一些温和的条件下——如果我们的分类器一开始就相当准确，如果目标数据只包含我们以前见过的类别，如果标签移位假设成立（这里最强的假设），然后我们可以通过求解一个简单的线性系统来估计测试集的标签分布\\n+\\n+$$\\\\mathbf{C} p(\\\\mathbf{y}) = \\\\mu(\\\\hat{\\\\mathbf{y}}),$$\\n+\\n+因为作为估计$\\\\sum_{j=1}^k c_{ij} p(y_j) = \\\\mu(\\\\hat{y}_i)$适用于所有$1 \\\\leq i \\\\leq k$，其中$p(y_j)$是$k$维度标签分布向量$p(\\\\mathbf{y})$的$j^\\\\mathrm{th}$元素。如果我们的分类器一开始就足够精确，那么混淆矩阵$\\\\mathbf{C}$将是可逆的，我们得到一个解$p(\\\\mathbf{y}) = \\\\mathbf{C}^{-1} \\\\mu(\\\\hat{\\\\mathbf{y}})$。\\n+\\n+因为我们观察源数据上的标签，所以很容易估计分布$q(y)$。那么对于标签为$i$的任何培训示例$i$，我们可以使用我们估计的$p(y_i)/q(y_i)$的比率来计算权重$\\\\beta_i$，并将其插入:eqref:`eq_weighted-empirical-risk-min`中的加权经验风险最小化中。\\n+\\n+### 概念转换修正\\n+\\n+观念转变很难用原则性的方式解决。例如，在一个问题突然从区分猫和狗转变为区分白色和黑色动物的情况下，假设我们可以做得比从零开始收集新标签和培训要好得多，这是不合理的。幸运的是，在实践中，这种极端的转变是罕见的。相反，通常情况下，任务总是在缓慢地变化。为了使事情更具体，下面是一些例子：\\n+\\n+* 在计算广告中，新产品推出，\\n+旧产品变得不那么受欢迎了。这意味着广告的分布和受欢迎程度是逐渐变化的，任何点击率预测器都需要随之逐渐变化。\\n+* 由于镜头的磨损，逐渐影响摄像头的图像质量。\\n+* 新闻内容逐渐变化（即大部分新闻保持不变，但出现新的故事）。\\n+\\n+在这种情况下，我们可以使用与训练网络相同的方法，使其适应数据的变化。换言之，我们使用现有的网络权值，简单地用新数据执行一些更新步骤，而不是从头开始训练。\\n+\\n+## 学习问题的分类法\\n+\\n+有了如何处理分布变化的知识，我们现在可以考虑机器学习问题制定的其他方面。\\n+\\n+### 批量学习\\n+\\n+在*批处理学习*中，我们可以访问培训特性和标签$\\\\{(\\\\mathbf{x}_1, y_1), \\\\ldots, (\\\\mathbf{x}_n, y_n)\\\\}$，我们使用这些特性和标签培训$f(\\\\mathbf{x})$。稍后，我们部署此模型来评分从同一分布中提取的新数据$(\\\\mathbf{x}, y)$。我们在这里讨论的是这个假设的任何问题。例如，我们可以根据猫和狗的大量图片训练猫检测器。一旦我们训练了它，我们就把它作为智能猫门计算机视觉系统的一部分，只允许猫进入。然后安装在客户家中，再也不会更新（除非极端情况下）。\\n+\\n+### 在线学习\\n+\\n+现在假设数据$(\\\\mathbf{x}_i, y_i)$一次到达一个样本。更具体地说，假设我们首先观察到$\\\\mathbf{x}_i$，然后我们需要得出一个估计值$f(\\\\mathbf{x}_i)$，只有当我们这样做了，我们观察到$y_i$，然后根据我们的决定，我们会得到奖励或招致损失。许多实际问题都属于这一类。例如，我们需要预测明天的股票价格，这样我们就可以根据这一估计进行交易，在一天结束时，我们会发现我们的估计是否允许我们盈利。换言之，在*在线学习*中，我们有以下的周期，在这个周期中，我们不断地改进我们的模型，给出新的观察结果。\\n+\\n+$$\\n+\\\\mathrm{model} ~ f_t \\\\longrightarrow\\n+\\\\mathrm{data} ~ \\\\mathbf{x}_t \\\\longrightarrow\\n+\\\\mathrm{estimate} ~ f_t(\\\\mathbf{x}_t) \\\\longrightarrow\\n+\\\\mathrm{observation} ~ y_t \\\\longrightarrow\\n+\\\\mathrm{loss} ~ l(y_t, f_t(\\\\mathbf{x}_t)) \\\\longrightarrow\\n+\\\\mathrm{model} ~ f_{t+1}\\n+$$\\n+\\n+### 强盗\\n+\\n+*强盗是上述问题的一个特例。虽然在大多数学习问题中，我们有一个连续参数化的函数$f$，我们想学习它的参数（例如，一个深网络），但在一个*bandit*问题中，我们只有有限数量的手臂可以拉，也就是说，我们可以采取的行动是有限的。对于这个更简单的问题，在最优性方面可以得到更有力的理论保证，这并不奇怪。我们把它列出来主要是因为这个问题经常被（令人困惑地）当作一个不同的学习环境来对待。\\n+\\n+### 控制\\n+\\n+在很多情况下，环境会记住我们所做的。不一定是以一种敌对的方式，但它会记住，而且反应将取决于之前发生的事情。例如，咖啡锅炉控制器将根据之前是否加热锅炉来观察不同的温度。PID（比例积分-微分）控制器算法是一个流行的选择。同样，用户在新闻网站上的行为也将取决于我们之前向他展示的内容（例如，大多数新闻他只阅读一次）。许多这样的算法形成了一个环境模型，在这个模型中，他们的行为使得他们的决策看起来不那么随机。近年来，控制理论（如PID变量）也被用于自动调整超参数，以获得更好的解缠和重建质量，提高生成文本的多样性和生成图像的重建质量:cite:`Shao.Yao.Sun.ea.2020`。\\n+\\n+### 强化学习\\n+\\n+在具有记忆的环境中，我们可能会遇到环境试图与我们合作的情况（合作博弈，尤其是非零和博弈），或其他环境试图获胜的情况。国际象棋、围棋、西洋双陆棋或星际争霸都是强化学习中的一些例子。同样地，我们可能想要为自动驾驶汽车制造一个好的控制器。其他的汽车可能会以非平凡的方式对自动驾驶汽车的驾驶风格做出反应，例如，试图避开它，试图引起事故，并试图与之合作。\\n+\\n+### 考虑到环境\\n+\\n+上述不同情况之间的一个关键区别是，在静止环境中可能一直有效的相同策略，在环境能够适应的情况下可能不会始终有效。例如，一个交易者发现的套利机会很可能在他开始利用它时消失。环境变化的速度和方式在很大程度上决定了我们可以采用的算法类型。例如，如果我们知道事情可能只会缓慢地变化，我们就可以迫使任何估计也只能缓慢地改变。如果我们知道环境可能会瞬间发生变化，但这种变化非常罕见，我们就可以考虑到这一点。这些类型的知识对于有抱负的数据科学家处理概念转变至关重要，也就是说，当他试图解决的问题随着时间的推移而发生变化时。\\n+\\n+## 机器学习中的公平、责任和透明度\\n+\\n+最后，重要的是要记住，当你部署机器学习系统时，你不仅仅是在优化一个预测模型——你通常是在提供一个工具来（部分或完全）自动化决策。这些技术系统可能会影响个人的生活，而这些人的生活则取决于最终的决定。从考虑预测到决策的飞跃不仅提出了新的技术问题，而且还提出了一系列必须仔细考虑的伦理问题。如果我们正在部署一个医疗诊断系统，我们需要知道它可能适用于哪些人群，哪些人群可能无效。忽视对一个亚群体的福利的可预见的风险可能会导致我们管理低劣的护理。此外，一旦我们考虑决策系统，我们必须退后一步，重新考虑如何评估我们的技术。在范围变化的其他后果中，我们会发现*准确*很少是正确的衡量标准。例如，当我们将预测转化为行动时，我们通常会考虑到各种方式犯错的潜在成本敏感性。如果一种对图像进行错误分类的方法可以被视为一种种族伎俩，而对另一种类型的错误分类是无害的，那么我们可能需要相应地调整我们的阈值，在设计决策协议时考虑到社会价值。我们还需要注意预测系统如何导致反馈循环。例如，考虑预测性警务系统，它将巡逻人员分配到预测犯罪率较高的地区。很容易看出一种令人担忧的模式是如何出现的：\\n+\\n+ 1. 犯罪率高的社区会得到更多的巡逻。\\n+ 1. 因此，在这些社区中会发现更多的犯罪行为，输入可用于未来迭代的训练数据。\\n+ 1. 面对更多的积极因素，该模型预测这些社区还会有更多的犯罪。\\n+ 1. 在下一次迭代中，更新后的模型针对的是同一个邻居，这会导致更多的犯罪行为被发现等等。\\n+\\n+通常，在建模过程中，模型的预测与训练数据耦合的各种机制都没有得到解释。这可能导致研究人员称之为“失控反馈回路”。此外，我们首先要注意我们是否解决了正确的问题。预测算法现在在信息传播中起着巨大的中介作用。个人遭遇的新闻应该由他们喜欢的Facebook页面决定吗？这些只是你在机器学习职业生涯中可能遇到的许多紧迫的道德困境中的一小部分。\\n+\\n+## 摘要\\n+\\n+* 在许多情况下，训练集和测试集并不来自同一个分布。这就是所谓的分配转移。\\n+* 真正的风险是从真实分布中提取的整个数据总体的损失预期。然而，这整个人口通常是无法获得的。经验风险是训练数据的平均损失近似真实风险。在实践中，我们执行经验风险最小化。\\n+* 在相应的假设条件下，可以在测试时检测和校正协变量和标签偏移。在测试时，不考虑这种偏差可能会成为问题。\\n+* 在某些情况下，环境可能会记住自动操作并以令人惊讶的方式作出响应。我们必须在建立模型和继续监测实时系统时考虑到这种可能性，我们的模型和环境可能会以意想不到的方式纠缠在一起。\\n+\\n+## 练习\\n+\\n+1. 当我们改变搜索引擎的行为时会发生什么？用户会怎么做？广告商呢？\\n+1. 实现协变量移位检测器。提示：构建分类器。\\n+1. 实现协变量移位校正。\\n+1. 除了分布转移，还有什么会影响经验风险如何接近真实风险？\\n+\\n+[Discussions](https://discuss.d2l.ai/t/105)\\ndiff --git a/chapter_multilayer-perceptrons/environment_tencent.md b/chapter_multilayer-perceptrons/environment_tencent.md\\nnew file mode 100644\\nindex 00000000..9f2d0f1a\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/environment_tencent.md\\n@@ -0,0 +1,252 @@\\n+# 环境和分销转变\\n+\\n+在前几节中，我们学习了许多机器学习的动手应用程序，将模型拟合到各种数据集。然而，我们从未停下来思考数据最初从何而来，或者我们计划最终如何处理我们模型的输出。通常情况下，拥有数据的机器学习开发人员没有停下来考虑这些基本问题就急于开发模型。\\n+\\n+许多失败的机器学习部署都可以追溯到此模式。有时候，按照测试集的准确性衡量，模型似乎表现得非常出色，但当数据分布突然改变时，部署就会灾难性地失败。更隐秘的是，有时模型的部署本身可能是扰乱数据分发的催化剂。例如，假设我们训练了一个模型来预测谁将偿还贷款或违约，发现申请者选择的鞋子与违约风险相关(牛津鞋表示偿还，运动鞋表示违约)。此后，我们可能倾向于向所有穿着牛津运动鞋的申请者发放贷款，而拒绝所有穿运动鞋的申请者。\\n+\\n+在这种情况下，我们从模式识别到决策的考虑不周的飞跃，以及我们未能批判性地考虑环境问题，可能会产生灾难性的后果。首先，一旦我们开始根据鞋类做出决定，客户就会意识到这一点，并改变他们的行为。不久之后，所有申请者都将穿着牛津鞋，信用状况没有任何相应的改善。花一分钟来消化这一点，因为在机器学习的许多应用程序中，类似的问题比比皆是：通过将我们基于模型的决策引入环境，我们可能会打破模型。\\n+\\n+虽然我们不可能在一个部分中完整地讨论这些主题，但我们在这里的目标是揭示一些常见的问题，并激发所需的批判性思维，以及早发现这些情况，减轻损害，并负责任地使用机器学习。有些解决方案很简单(要求“正确”的数据)，有些解决方案在技术上很困难(实现强化学习系统)，还有一些方案要求我们完全跳出统计预测的领域，努力解决与算法的伦理应用有关的棘手的哲学问题。\\n+\\n+## 分配班次的类型\\n+\\n+首先，我们坚持被动预测设置，考虑到数据分布可能发生变化的各种方式，以及为挽救模型性能可能采取的措施。在一种经典设置中，我们假设我们的训练数据是从某个分布$p_S(\\\\mathbf{x},y)$采样的，但是我们的测试数据将包括从某个不同的分布$p_T(\\\\mathbf{x},y)$提取的未标记的示例。我们已经必须面对一个发人深省的现实。如果没有任何关于$p_S$和$p_T$如何相互关联的假设，学习健壮的分类器是不可能的。\\n+\\n+考虑一个二进制分类问题，我们希望区分狗和猫。如果分布可以以任意方式移动，那么我们的设置允许在病理情况下，输入上的分布保持不变：$p_S(\\\\mathbf{x}) = p_T(\\\\mathbf{x})$，但是标签全部翻转：$p_S(y | \\\\mathbf{x}) = 1 - p_T(y | \\\\mathbf{x})$。换句话说，如果上帝突然决定未来所有的“猫”现在都是狗，我们以前所说的“狗”现在是猫-投入$p(\\\\mathbf{x})$的分布没有任何改变，那么我们就不可能将这个设置与分布完全没有改变的设置区分开来。\\n+\\n+幸运的是，在对我们的数据未来可能发生变化的一些有限假设下，原则性算法可以检测到偏移，有时甚至可以在飞翔上进行调整，从而提高了原始分类器的准确性。\\n+\\n+### 协变量移位\\n+\\n+在分布漂移的分类中，协变量漂移可能是研究最广泛的。这里，我们假设虽然输入的分布可以随时间改变，但标记函数(即条件分布$P(y \\\\mid \\\\mathbf{x})$)不改变。统计学家称之为“协变量转移”，因为这个问题是由于协变量(特征)分布的转移引起的。虽然我们有时可以在不引用因果关系的情况下对分布偏移进行推理，但我们注意到协变量偏移是在我们认为$\\\\mathbf{x}$导致$y$的设置中调用的自然假设。\\n+\\n+想想区分猫和狗的挑战吧。我们的训练数据可能由:numref:`fig_cat-dog-train`中的图像组成。\\n+\\n+![Training data for distinguishing cats and dogs.](../img/cat-dog-train.svg)\\n+:label:`fig_cat-dog-train`\\n+\\n+在测试时，我们被要求对:numref:`fig_cat-dog-test`中的图像进行分类。\\n+\\n+![Test data for distinguishing cats and dogs.](../img/cat-dog-test.svg)\\n+:label:`fig_cat-dog-test`\\n+\\n+训练集由照片组成，而测试集只包含卡通。在具有与测试集本质不同的特征的数据集上进行训练可能会带来麻烦，因为没有关于如何适应新领域的连贯计划。\\n+\\n+### 标签移位\\n+\\n+*LABEL SHIFT*描述了反向问题。\\n+这里，我们假设标签边缘$P(y)$可以改变，但是类别条件分布$P(\\\\mathbf{x} \\\\mid y)$跨域保持固定。当我们认为$y$导致$\\\\mathbf{x}$时，标签移位是一个合理的假设。例如，我们可能想要根据其症状(或其他表现)来预测诊断，即使诊断的相对流行率正在随着时间的推移而变化。标签移位在这里是合适的假设，因为疾病会引起症状。在某些退化情况下，标签移位和协变量移位假设可以同时成立。例如，当标签是确定性的时，即使当$y$导致$\\\\mathbf{x}$时，也将满足协变量移位假设。有趣的是，在这些情况下，使用源自标签移位假设的方法通常是有利的。这是因为这些方法往往涉及操作看起来像标签的对象(通常是低维的)，而不是看起来像输入的对象，后者在深度学习中往往是高维的。\\n+\\n+### 观念转变\\n+\\n+我们可能还会遇到相关的“概念转移”问题，这是当标签的定义可能发生变化时出现的。这听起来很奇怪-“猫”是“猫”，不是吗？但是，随着时间的推移，其他类别的使用可能会发生变化。精神疾病的诊断标准，什么是流行的，什么是职称，都会受到相当大的概念转变的影响。事实证明，如果我们在美国各地导航，根据地理位置改变我们的数据来源，我们会发现关于“软饮料”名称分布的概念发生了相当大的变化，如:numref:`fig_popvssoda`所示。\\n+\\n+![Concept shift on soft drink names in the United States.](../img/popvssoda.png)\\n+:width:`400px`\\n+:label:`fig_popvssoda`\\n+\\n+如果我们要构建机器翻译系统，则分发$P(y \\\\mid \\\\mathbf{x})$可能会因我们的位置不同而不同。这个问题可能很难发现。我们可能希望利用这样一种知识，即转变只会在时间或地理意义上逐渐发生。\\n+\\n+## 分布移位示例\\n+\\n+在深入研究形式主义和算法之前，我们可以讨论一些具体的情况，在这些情况下，协变量或概念转换可能不明显。\\n+\\n+### 医疗诊断学\\n+\\n+想象一下，您想要设计一种检测癌症的算法。你从健康人和病人那里收集数据，然后训练你的算法。它工作得很好，给你很高的精确度，你会得出结论，你已经准备好在医疗诊断领域取得成功。\\n+*不要那么快*\\n+\\n+产生训练数据的分布可能与您在野外遇到的分布有很大不同。这发生在一家不幸的初创公司身上，我们中的一些人(作者)几年前就和它合作过。他们正在开发一种主要影响老年男性的疾病的血液测试，并希望使用他们从患者那里收集的血液样本进行研究。然而，从健康男性身上获取血液样本比从系统中已经存在的病人身上获取血液样本要困难得多。为了补偿，这家初创公司向一所大学校园的学生募集献血，作为开发测试的健康对照。然后他们问我们是否可以帮助他们建立一个分类器来检测疾病。\\n+\\n+正如我们向他们解释的那样，确实很容易以近乎完美的准确性区分健康和患病的人群。然而，这是因为测试对象在年龄、激素水平、体力活动、饮食、饮酒以及更多与疾病无关的因素上存在差异。这不太可能是真正的病人的情况。由于他们的抽样程序，我们可以预料到会遇到极端的协变量漂移。此外，这种情况不太可能通过传统的方法来纠正。简而言之，他们浪费了一大笔钱。\\n+\\n+### 自动驾驶汽车\\n+\\n+假设一家公司想要利用机器学习来开发自动驾驶汽车。这里的一个关键部件是路边探测器。由于真正的注释数据获取成本很高，他们有一个(聪明而可疑的)想法，即使用来自游戏渲染引擎的合成数据作为额外的训练数据。这对从渲染引擎提取的“测试数据”非常有效。唉，在一辆真正的车里，这简直是一场灾难。事实证明，路边被渲染成了非常简单化的纹理。更重要的是，“所有”路边都是用“相同”纹理渲染的，路边探测器很快就了解到了这一“特征”。\\n+\\n+当美军第一次试图探测森林中的坦克时，类似的事情也发生在他们身上。他们在没有坦克的情况下航拍了森林的照片，然后把坦克开进森林，又拍了一组照片。分类器似乎工作得“完美”。不幸的是，它只学会了如何区分有阴影的树木和没有阴影的树木-第一组照片是在清晨拍摄的，第二组照片是在中午拍摄的。\\n+\\n+### 非平稳分布\\n+\\n+当分布变化缓慢(也称为“非平稳分布”)并且模型没有充分更新时，会出现一种更为微妙的情况。以下是一些典型案例。\\n+\\n+* 我们训练了一个计算广告模型，然后没有频繁地更新它(例如，我们忘记了纳入一款名为iPad的鲜为人知的新设备刚刚推出)。\\n+* 我们建了一个垃圾过滤。它可以很好地检测到我们到目前为止看到的所有垃圾邮件。但随后，垃圾邮件发送者变得聪明起来，精心制作了看起来与我们以前见过的任何邮件都不同的新邮件。\\n+* 我们建立了一个产品推荐系统。它在整个冬天都有效，但圣诞节过后很久还会继续推荐圣诞帽。\\n+\\n+### 更多趣闻轶事\\n+\\n+* 我们造了一个面部探测器。它在所有基准上都工作得很好。不幸的是，它在测试数据上失败了-令人不快的例子是面部充满整个图像的特写镜头(训练集中没有这样的数据)。\\n+* 我们为美国市场建立了一个网络搜索引擎，并希望将其部署在英国。\\n+* 我们通过编译一个大型数据集来训练图像分类器，其中一大组类中的每一个在数据集中都相等地表示，比如说1000个类别，每个类别由1000个图像表示。然后，我们将该系统部署在现实世界中，在现实世界中，照片的实际标签分布显然是不均匀的。\\n+\\n+## 分配偏移的修正\\n+\\n+如我们所讨论的，存在训练和测试分布$P(\\\\mathbf{x}, y)$不同的许多情况。在某些情况下，我们很幸运，尽管协变量、标签或概念发生了变化，但模型仍然有效。在其他情况下，我们可以通过采取有原则的策略来应对这种转变，做得更好。本节的其余部分将变得更加技术性。不耐烦的读者可以继续阅读下一节，因为本材料不是后续概念的先决条件。\\n+\\n+### 经验风险与真实风险\\n+\\n+让我们首先反映在模型训练期间究竟发生了什么：我们迭代训练数据$\\\\{(\\\\mathbf{x}_1, y_1), \\\\ldots, (\\\\mathbf{x}_n, y_n)\\\\}$的特征和相关联的标签，并且在每个小批量之后更新模型$f$的参数。为简单起见，我们不考虑正规化，因此我们在很大程度上将培训损失降至最低：\\n+\\n+$$\\\\mathop{\\\\mathrm{minimize}}_f \\\\frac{1}{n} \\\\sum_{i=1}^n l(f(\\\\mathbf{x}_i), y_i),$$\\n+:eqlabel:`eq_empirical-risk-min`\\n+\\n+其中$l$是测量预测$f(\\\\mathbf{x}_i)$有多差的损失函数，被给予关联的标签$y_i$。统计学家将:eqref:`eq_empirical-risk-min`中的术语称为“经验风险”。\\n+*经验风险*是训练数据的平均损失\\n+为了近似“真实风险”，其是从数据的真实分布$p(\\\\mathbf{x},y)$中提取的整个数据总体上的损失的预期：\\n+\\n+$$E_{p(\\\\mathbf{x}, y)} [l(f(\\\\mathbf{x}), y)] = \\\\int\\\\int l(f(\\\\mathbf{x}), y) p(\\\\mathbf{x}, y) \\\\;d\\\\mathbf{x}dy.$$\\n+:eqlabel:`eq_true-risk`\\n+\\n+然而，在实践中，我们通常不能获得全部数据。因此，:eqref:`eq_empirical-risk-min`提出的“经验风险最小化”是一种实用的机器学习策略，希望近似最小化真实风险。\\n+\\n+### 协变量移位校正\\n+:label:`subsec_covariate-shift-correction`\\n+\\n+假设我们想要估计一些依赖性$P(y \\\\mid \\\\mathbf{x})$，我们已经为其标记了数据$(\\\\mathbf{x}_i, y_i)$。不幸的是，观测$\\\\mathbf{x}_i$取自一些*源分布*$q(\\\\mathbf{x})$，而不是*目标分布*$p(\\\\mathbf{x})$。幸运的是，相依性假设意味着条件分布不会改变：$p(y \\\\mid \\\\mathbf{x}) = q(y \\\\mid \\\\mathbf{x})$。如果源分布$q(\\\\mathbf{x})$是“错误的”，我们可以通过使用以下真实风险中的简单身份来纠正：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+\\\\int\\\\int l(f(\\\\mathbf{x}), y) p(y \\\\mid \\\\mathbf{x})p(\\\\mathbf{x}) \\\\;d\\\\mathbf{x}dy =\\n+\\\\int\\\\int l(f(\\\\mathbf{x}), y) q(y \\\\mid \\\\mathbf{x})q(\\\\mathbf{x})\\\\frac{p(\\\\mathbf{x})}{q(\\\\mathbf{x})} \\\\;d\\\\mathbf{x}dy.\\n+\\\\end{aligned}\\n+$$\\n+\\n+换句话说，我们需要根据正确分布与错误分布的概率之比来重新衡量每个数据示例的权重：\\n+\\n+$$\\\\beta_i \\\\stackrel{\\\\mathrm{def}}{=} \\\\frac{p(\\\\mathbf{x}_i)}{q(\\\\mathbf{x}_i)}.$$\\n+\\n+插入每个数据示例$\\\\beta_i$的权重$(\\\\mathbf{x}_i, y_i)$，我们可以使用\\n+*加权经验风险最小化*：\\n+\\n+$$\\\\mathop{\\\\mathrm{minimize}}_f \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\beta_i l(f(\\\\mathbf{x}_i), y_i).$$\\n+:eqlabel:`eq_weighted-empirical-risk-min`\\n+\\n+唉，我们不知道这个比率，所以在我们可以做任何有用的事情之前，我们需要估计它。有很多方法可用，包括一些奇特的算子理论方法，它们试图使用最小范数或最大熵原理直接重新校准期望算子。注意，对于任何这样的方法，我们需要从两个分布中提取样本-例如通过访问测试数据而得到的“真”$p$，以及用于生成训练集$q$的样本(后者是普通可用的)。然而，请注意，我们只需要特征$\\\\mathbf{x} \\\\sim p(\\\\mathbf{x})$；我们不需要访问标签$y \\\\sim p(y)$。\\n+\\n+在这种情况下，存在一种非常有效的方法，它将给出几乎与原始方法一样好的结果：Logistic回归，这是二进制分类的软最大回归(见:numref:`sec_softmax`)的特例。这就是计算估计概率比所需的全部内容。我们学习了一个分类器来区分从$p(\\\\mathbf{x})$提取的数据和从$q(\\\\mathbf{x})$提取的数据。如果无法区分这两个分布，则意味着关联的实例同样可能来自这两个分布中的任何一个。另一方面，任何可以很好区分的实例都应该相应地显著增加或减少权重。\\n+\\n+为简单起见，假设我们分别拥有来自发行版$p(\\\\mathbf{x})$和$q(\\\\mathbf{x})$的相同数量的实例。现在用$z$来表示标签，这些标签对于从$p$提取的数据是$1$，对于从$q$提取的数据是$-1$。则混合数据集中的概率由下式给出\\n+\\n+$$P(z=1 \\\\mid \\\\mathbf{x}) = \\\\frac{p(\\\\mathbf{x})}{p(\\\\mathbf{x})+q(\\\\mathbf{x})} \\\\text{ and hence } \\\\frac{P(z=1 \\\\mid \\\\mathbf{x})}{P(z=-1 \\\\mid \\\\mathbf{x})} = \\\\frac{p(\\\\mathbf{x})}{q(\\\\mathbf{x})}.$$\\n+\\n+因此，如果我们使用逻辑回归方法，其中$P(z=1 \\\\mid \\\\mathbf{x})=\\\\frac{1}{1+\\\\exp(-h(\\\\mathbf{x}))}$($h$是参数化函数)，则遵循\\n+\\n+$$\\n+\\\\beta_i = \\\\frac{1/(1 + \\\\exp(-h(\\\\mathbf{x}_i)))}{\\\\exp(-h(\\\\mathbf{x}_i))/(1 + \\\\exp(-h(\\\\mathbf{x}_i)))} = \\\\exp(h(\\\\mathbf{x}_i)).\\n+$$\\n+\\n+因此，我们需要解决两个问题：第一个问题是区分从两个分布中提取的数据，然后是:eqref:`eq_weighted-empirical-risk-min`中的加权经验风险最小化问题，在这个问题中，我们将项加权$\\\\beta_i$。\\n+\\n+现在我们准备描述一种校正算法。假设我们具有训练集$\\\\{(\\\\mathbf{x}_1, y_1), \\\\ldots, (\\\\mathbf{x}_n, y_n)\\\\}$和未标记的测试集$\\\\{\\\\mathbf{u}_1, \\\\ldots, \\\\mathbf{u}_m\\\\}$。对于协变量平移，我们假设$\\\\mathbf{x}_i$的全部$1 \\\\leq i \\\\leq n$取自某个源分布，$\\\\mathbf{u}_i$的全部$1 \\\\leq i \\\\leq m$取自目标分布。以下是校正协变量偏移的典型算法：\\n+\\n+1. 生成二进制分类训练集：$\\\\{(\\\\mathbf{x}_1, -1), \\\\ldots, (\\\\mathbf{x}_n, -1), (\\\\mathbf{u}_1, 1), \\\\ldots, (\\\\mathbf{u}_m, 1)\\\\}$。\\n+1. 使用逻辑回归训练二进制分类器以获得函数$h$。\\n+1. 使用$\\\\beta_i = \\\\exp(h(\\\\mathbf{x}_i))$或更好的$\\\\beta_i = \\\\min(\\\\exp(h(\\\\mathbf{x}_i)), c)$来加权训练数据，对于某个常数$c$。\\n+1. 使用权重$\\\\beta_i$在:eqref:`eq_weighted-empirical-risk-min`的$\\\\{(\\\\mathbf{x}_1, y_1), \\\\ldots, (\\\\mathbf{x}_n, y_n)\\\\}$上进行训练。\\n+\\n+请注意，上述算法依赖于一个重要的假设。为了使该方案起作用，我们需要目标(例如，测试时间)分布中的每个数据示例具有在训练时间出现的非零概率。如果我们找到$p(\\\\mathbf{x}) > 0$但$q(\\\\mathbf{x}) = 0$的点，那么相应的重要性权重应该是无穷大。\\n+\\n+### 标签移位校正\\n+\\n+假设我们正在处理一个具有$k$个类别的分类任务。在:numref:`subsec_covariate-shift-correction`、$q$和$p$中使用相同的符号分别是源分布(例如，训练时间)和目标分布(例如，测试时间)。假设标签的分布随时间变化：$q(y) \\\\neq p(y)$，但类条件分布保持不变：$q(\\\\mathbf{x} \\\\mid y)=p(\\\\mathbf{x} \\\\mid y)$。如果源分布$q(y)$是“错误的”，我们可以根据如:eqref:`eq_true-risk`中定义的真实风险中的以下身份进行校正：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+\\\\int\\\\int l(f(\\\\mathbf{x}), y) p(\\\\mathbf{x} \\\\mid y)p(y) \\\\;d\\\\mathbf{x}dy =\\n+\\\\int\\\\int l(f(\\\\mathbf{x}), y) q(\\\\mathbf{x} \\\\mid y)q(y)\\\\frac{p(y)}{q(y)} \\\\;d\\\\mathbf{x}dy.\\n+\\\\end{aligned}\\n+$$\\n+\\n+这里，我们的重要性权重将对应于标签似然比\\n+\\n+$$\\\\beta_i \\\\stackrel{\\\\mathrm{def}}{=} \\\\frac{p(y_i)}{q(y_i)}.$$\\n+\\n+标签移动的一个好处是，如果我们在源分布上有一个相当好的模型，那么我们可以得到这些权重的一致估计，而不需要处理环境维度。在深度学习中，输入往往是高维对象，如图像，而标签通常是更简单的对象，如类别。\\n+\\n+为了估计目标标签分布，我们首先采用性能相当好的现成分类器(通常基于训练数据进行训练)，并使用验证集(也来自训练分布)计算其念力矩阵。*念力矩阵*，$\\\\mathbf{C}$，只是一个$k \\\\times k$矩阵，其中每列对应于标签类别(基本事实)，每行对应于我们模型的预测类别。每个单元格的值$c_{ij}$是验证集上总预测的分数，其中真实标签为$j$，我们的模型预测为$i$。\\n+\\n+现在，我们不能直接计算目标数据上的念力矩阵，因为我们无法看到我们在野外看到的示例的标签，除非我们投资于复杂的实时注释管道。然而，我们可以做的是一起在测试时间对我们所有的模型预测进行平均，产生平均模型输出$\\\\mu(\\\\hat{\\\\mathbf{y}}) \\\\in \\\\mathbb{R}^k$，其$i^\\\\mathrm{th}$元素$\\\\mu(\\\\hat{y}_i)$是测试集上我们的模型预测$i$的总预测的分数。\\n+\\n+结果表明，在一些温和的条件下-如果我们的分类器首先相当准确，如果目标数据只包含我们以前见过的类别，并且如果标签移动假设首先成立(这里是最强的假设)，那么我们可以通过求解一个简单的线性系统来估计测试集标签分布\\n+\\n+$$\\\\mathbf{C} p(\\\\mathbf{y}) = \\\\mu(\\\\hat{\\\\mathbf{y}}),$$\\n+\\n+因为作为估计，$\\\\sum_{j=1}^k c_{ij} p(y_j) = \\\\mu(\\\\hat{y}_i)$对于所有$1 \\\\leq i \\\\leq k$都成立，其中$p(y_j)$是$k$维标签分布向量$p(\\\\mathbf{y})$的$j^\\\\mathrm{th}$个元素。如果我们的分类器一开始就足够准确，那么念力矩阵$\\\\mathbf{C}$将是可逆的，并且我们得到解$p(\\\\mathbf{y}) = \\\\mathbf{C}^{-1} \\\\mu(\\\\hat{\\\\mathbf{y}})$。\\n+\\n+因为我们观察源数据上的标签，所以很容易估计分布$q(y)$。然后，对于任何具有标签$y_i$的训练示例$i$，我们可以取我们估计的$p(y_i)/q(y_i)$的比率来计算权重$\\\\beta_i$，并将其插入到:eqref:`eq_weighted-empirical-risk-min`中的加权经验风险最小化中。\\n+\\n+### 概念转移纠正\\n+\\n+概念转变很难有原则地解决。举例来说，当问题突然从区分猫和狗变成区分白色和黑色动物时，假设我们可以做得更好，而不仅仅是从零开始收集新标签和训练，这是不合理的。幸运的是，在实践中，这种极端的转变是罕见的。相反，通常会发生的情况是，任务一直在缓慢变化。为了让事情更具体，这里有一些例子：\\n+\\n+* 在计算广告中，新产品被推出，\\n+旧产品变得不那么受欢迎了。这意味着广告的分布和受欢迎程度会逐渐改变，任何点击率预测指标都需要随之逐渐改变。\\n+* 交通摄像镜头由于环境磨损逐渐退化，逐渐影响图像质量。\\n+* 新闻内容逐渐发生变化(即大部分新闻保持不变，但有新的故事出现)。\\n+\\n+在这种情况下，我们可以使用与训练网络相同的方法来使它们适应数据的变化。换句话说，我们使用现有的网络权重，并简单地使用新数据执行几个更新步骤，而不是从头开始训练。\\n+\\n+## 学习问题的分类学研究\\n+\\n+掌握了如何处理分发中的更改的知识后，我们现在可以考虑机器学习问题表达的其他一些方面。\\n+\\n+### 批处理学习\\n+\\n+在*批处理学习*中，我们可以访问训练特征和标签$\\\\{(\\\\mathbf{x}_1, y_1), \\\\ldots, (\\\\mathbf{x}_n, y_n)\\\\}$，我们使用它们来训练模型$f(\\\\mathbf{x})$。稍后，我们部署此模型来对来自同一分布的新数据$(\\\\mathbf{x}, y)$进行评分。这是我们这里讨论的任何问题的默认假设。例如，我们可以根据大量的猫和狗的照片来训练猫探测器。一旦我们训练了它，我们就把它作为智能猫门计算机视觉系统的一部分发货，该系统只允许猫进入。然后将其安装在客户家中，并且再也不会更新(除非在极端情况下)。\\n+\\n+### 在线学习\\n+\\n+现在假设数据$(\\\\mathbf{x}_i, y_i)$每次到达一个样本。更具体地说，假设我们首先观察到$\\\\mathbf{x}_i$，然后我们需要得出估计的$f(\\\\mathbf{x}_i)$，只有当我们做到这一点后，我们才观察到$y_i$，随之而来的是，根据我们的决定，我们会获得回报或招致损失。很多真正的问题都属于这一类。例如，我们需要预测明天的股价，这使得我们可以根据这个估计进行交易，在一天结束时，我们会发现我们的估计是否允许我们盈利。换句话说，在*在线学习*中，我们有以下循环，在这个循环中，我们不断改进我们的模型，并提供新的观察结果。\\n+\\n+$$\\n+\\\\mathrm{model} ~ f_t \\\\longrightarrow\\n+\\\\mathrm{data} ~ \\\\mathbf{x}_t \\\\longrightarrow\\n+\\\\mathrm{estimate} ~ f_t(\\\\mathbf{x}_t) \\\\longrightarrow\\n+\\\\mathrm{observation} ~ y_t \\\\longrightarrow\\n+\\\\mathrm{loss} ~ l(y_t, f_t(\\\\mathbf{x}_t)) \\\\longrightarrow\\n+\\\\mathrm{model} ~ f_{t+1}\\n+$$\\n+\\n+### 土匪\\n+\\n+*强盗*是上述问题的特例。虽然在大多数学习问题中，我们具有连续参数化的函数$f$，其中我们想要学习其参数(例如，深度网络)，但是在*强盗*问题中，我们仅具有有限数量的我们可以拉动的手臂，即，我们可以采取的有限数量的动作。对于这个更简单的问题，可以获得更强的最优性理论保证，这并不令人惊讶。我们之所以列出它，主要是因为这个问题经常(令人困惑地)被视为一个独特的学习环境。\\n+\\n+### 控制\\n+\\n+在很多情况下，环境会记住我们的所作所为。不一定是以敌对的方式，但它只会记住，反应将取决于之前发生的事情。例如，咖啡锅炉控制器将观察到不同的温度，这取决于它之前是否正在加热锅炉。PID(比例-积分-微分)控制器算法在那里是一个流行的选择。同样，用户在新闻网站上的行为将取决于我们之前向他展示的内容(例如，他将只读一次大多数新闻)。许多这样的算法形成了一个环境模型，它们在其中采取行动，从而使它们的决定看起来不那么随机。最近，控制理论(例如，PID变体)也已被用于自动调整超参数，以实现更好的解缠和重建质量，并提高生成文本的多样性和生成图像:cite:`Shao.Yao.Sun.ea.2020`的重建质量。\\n+\\n+### 强化学习\\n+\\n+在更一般的有记忆的环境中，我们可能会遇到环境试图与我们合作的情况(合作游戏，特别是非零和游戏)，或者其他环境试图取胜的情况。国际象棋、围棋、双陆棋或星际争霸都是强化学习中的一些例子。同样，我们可能想要为自动驾驶汽车制造一个好的控制器。其他汽车可能会以不平凡的方式对自动驾驶汽车的驾驶方式做出反应，例如，试图避开它，试图造成事故，并试图与其合作。\\n+\\n+### 考虑到环境问题\\n+\\n+上述不同情况之间的一个关键区别是，在固定环境中可能始终有效的相同策略，在环境可以适应的情况下可能不会始终有效。例如，交易员发现的套利机会一旦开始利用，很可能就会消失。环境变化的速度和方式在很大程度上决定了我们可以采用的算法类型。例如，如果我们知道事情可能只会缓慢改变，我们可以强制任何估计也只能缓慢改变。如果我们知道环境可能会瞬间变化，但变化的频率很低，我们就可以考虑到这一点。这些类型的知识对于有抱负的数据科学家处理概念转变至关重要，也就是说，当他试图解决的问题随着时间的推移而发生变化时。\\n+\\n+## 机器学习中的公平性、问责性和透明度\\n+\\n+最后，重要的是要记住，当您部署机器学习系统时，您不仅仅是在优化预测模型-您通常会提供一个将用于(部分或全部)自动化决策的工具。这些技术系统可能会影响个人的生活，受到由此产生的决定的影响。从考虑预测到决策的飞跃不仅提出了新的技术问题，而且还提出了一系列必须仔细考虑的伦理问题。如果我们正在部署医疗诊断系统，我们需要知道它可能适用于哪些人群，哪些可能不适用。忽视亚群福利的可预见风险可能会导致我们管理较差的护理。此外，一旦我们考虑决策系统，我们必须退后一步，重新考虑我们如何评估我们的技术。在这种范围变化的其他后果中，我们会发现“准确性”很少是正确的衡量标准。例如，在将预测转化为行动时，我们通常会想要考虑到以各种方式出错的潜在成本敏感性。如果一种错误分类图像的方式可以被认为是种族花招，而错误分类到不同的类别将是无害的，那么我们可能想要相应地调整我们的阈值，在设计决策方案时考虑到社会价值。我们还希望小心预测系统如何导致反馈循环。例如，考虑预测性警务系统，它将巡逻人员分配到预测犯罪率较高的地区。很容易看出一个令人担忧的模式是如何出现的：\\n+\\n+ 1. 犯罪率较高的社区会有更多的巡逻。\\n+ 1. 因此，在这些社区发现了更多的犯罪，输入了可供未来迭代使用的训练数据。\\n+ 1. 在接触到更多积极因素的情况下，该模型预测这些社区的犯罪率还会更高。\\n+ 1. 在下一次迭代中，更新的模型针对相同的社区甚至更严重，从而导致更多的犯罪被发现，等等。\\n+\\n+通常，模型的预测与其训练数据相耦合的各种机制在建模过程中是无法解释的。这可能会导致研究人员所说的“失控反馈循环”。此外，我们首先要小心我们是否解决了正确的问题。预测算法现在在信息传播中扮演着非常重要的角色。个人遇到的新闻是否应该由他们“喜欢”的一组Facebook页面来决定？这些只是你在机器学习的职业生涯中可能会遇到的许多紧迫的伦理困境中的一小部分。\\n+\\n+## 摘要\\n+\\n+* 在许多情况下，训练集和测试集不是来自相同的分布。这就是所谓的分配转移。\\n+* 真正的风险是从数据的真实分布中提取的数据在整个人群中丢失的预期。然而，这整个群体通常是不可用的。经验风险是对训练数据的平均损失，以近似真实风险。在实践中，我们执行经验风险最小化。\\n+* 在相应的假设条件下，可以在测试时检测并校正协变量和标签偏移。在测试时，无法解释这种偏差可能会成为问题。\\n+* 在某些情况下，环境可能会记住自动操作并以令人惊讶的方式做出响应。在构建模型时，我们必须考虑到这种可能性，并继续监控实时系统，并对我们的模型和环境以意想不到的方式纠缠在一起的可能性持开放态度。\\n+\\n+## 练习\\n+\\n+1. 当我们改变搜索引擎的行为时会发生什么？用户可能会做些什么呢？广告商呢？\\n+1. 实现了一个协变量移位检测器。提示：构建一个分类器。\\n+1. 实现一个协变量移位校正器。\\n+1. 除了分布变化，还有什么会影响经验风险如何接近真实风险？\\n+\\n+[Discussions](https://discuss.d2l.ai/t/105)\\ndiff --git a/chapter_multilayer-perceptrons/index_baidu.md b/chapter_multilayer-perceptrons/index_baidu.md\\nnew file mode 100644\\nindex 00000000..99afeb08\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/index_baidu.md\\n@@ -0,0 +1,19 @@\\n+# 多层网络\\n+:label:`chap_perceptrons`\\n+\\n+在本章中，我们将介绍您的第一个真正的“深度”网络。最简单的深层网络被称为多层感知器，它们由多层神经元组成，每一层都与下面层（它们从中接收输入）和上面的神经元（它们反过来影响它们）完全相连。当我们训练高容量的模型时，我们会冒着过度拟合的风险。因此，我们需要为您提供第一次严格的介绍，介绍过拟合、欠拟合和模型选择的概念。我们将为您介绍这些规则化技术，帮助您解决掉重和掉块等问题。我们还将讨论与数值稳定性和参数初始化有关的问题，这是成功训练深网络的关键。自始至终，我们的目标不仅是让您牢牢掌握概念，而且还掌握使用深度网络的实践。在本章的最后，我们将我们所介绍的应用到一个实际案例中：房价预测。我们将有关模型的计算性能、可伸缩性和效率的问题推到后面的章节。\\n+\\n+```toc\\n+:maxdepth: 2\\n+\\n+mlp\\n+mlp-scratch\\n+mlp-concise\\n+underfit-overfit\\n+weight-decay\\n+dropout\\n+backprop\\n+numerical-stability-and-init\\n+environment\\n+kaggle-house-price\\n+```\\ndiff --git a/chapter_multilayer-perceptrons/index_tencent.md b/chapter_multilayer-perceptrons/index_tencent.md\\nnew file mode 100644\\nindex 00000000..b3b39897\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/index_tencent.md\\n@@ -0,0 +1,19 @@\\n+# 多层感知器\\n+:label:`chap_perceptrons`\\n+\\n+在本章中，我们将介绍您的第一个真正的“深度”网络。最简单的深层网络被称为多层感知器，它们由多层神经元组成，每一层都与下面一层(它们从那里接收输入)和上面一层(它们反过来影响这些神经元)完全相连。当我们训练大容量模型时，我们面临着过度适应的风险。因此，我们需要为您提供第一次严格的概念介绍，包括过合身、欠合身和型号选择。为了帮助您解决这些问题，我们将介绍体重衰减和辍学等正规化技术。我们还将讨论与数值稳定性和参数初始化相关的问题，这些问题是成功训练深度网络的关键。从头到尾，我们的目标不仅是让您牢牢掌握概念，还希望让您牢牢掌握使用深度网络的实践。在本章的最后，我们将把我们到目前为止所介绍的内容应用到一个真实的案例：房价预测。我们将与我们的模型的计算性能、可伸缩性和效率相关的问题放在后面的章节中讨论。\\n+\\n+```toc\\n+:maxdepth: 2\\n+\\n+mlp\\n+mlp-scratch\\n+mlp-concise\\n+underfit-overfit\\n+weight-decay\\n+dropout\\n+backprop\\n+numerical-stability-and-init\\n+environment\\n+kaggle-house-price\\n+```\\ndiff --git a/chapter_multilayer-perceptrons/kaggle-house-price_baidu.md b/chapter_multilayer-perceptrons/kaggle-house-price_baidu.md\\nnew file mode 100644\\nindex 00000000..6a60ded4\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/kaggle-house-price_baidu.md\\n@@ -0,0 +1,483 @@\\n+# 用Kaggle预测房价\\n+:label:`sec_kaggle_house`\\n+\\n+现在我们已经介绍了一些构建和训练深层网络的基本工具，并使用包括权重衰减和退出的技术对其进行规范化，我们已经准备好通过参加Kaggle竞赛将所有这些知识付诸实践。房价预测大赛是一个很好的起点。这些数据是相当通用的，没有显示出可能需要特殊模型（如音频或视频）的奇异结构。该数据集由Bart de Cock于2011年收集:cite:`De-Cock.2011`，涵盖了2006年至2010年期间IA艾姆斯的房价。它比著名的哈里森和鲁宾菲尔德（1978）的[Boston housing dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)要大得多，它拥有更多的例子和更多的特性。\\n+\\n+在本节中，我们将引导您详细了解数据预处理、模型设计和超参数选择。我们希望通过亲身实践的方法，你将获得一些直觉，这些直觉将指导你作为数据科学家的职业生涯。\\n+\\n+## 下载和缓存数据集\\n+\\n+在本书中，我们将在各种下载的数据集上训练和测试模型。在这里，我们实现了几个实用函数，以方便数据下载。首先，我们维护一个字典`DATA_HUB`，它将一个字符串（数据集的*名称*映射到一个元组，该元组包含定位数据集的URL和验证文件完整性的SHA-1键。所有这些数据集都托管在地址为`DATA_URL`的站点上。\\n+\\n+```{.python .input}\\n+#@tab all\\n+import os\\n+import requests\\n+import zipfile\\n+import tarfile\\n+import hashlib\\n+\\n+#@save\\n+DATA_HUB = dict()\\n+DATA_URL = \\'http://d2l-data.s3-accelerate.amazonaws.com/\\'\\n+```\\n+\\n+下面的`download`函数下载数据集，将其缓存在本地目录中（默认为`../data`），并返回下载文件的名称。如果缓存目录中已经存在与此数据集对应的文件，并且其SHA-1与存储在`DATA_HUB`中的文件相匹配，则我们的代码将使用缓存文件，以避免冗余下载阻塞您的互联网。\\n+\\n+```{.python .input}\\n+#@tab all\\n+def download(name, cache_dir=os.path.join(\\'..\\', \\'data\\')):  #@save\\n+    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\\n+    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\\n+    url, sha1_hash = DATA_HUB[name]\\n+    d2l.mkdir_if_not_exist(cache_dir)\\n+    fname = os.path.join(cache_dir, url.split(\\'/\\')[-1])\\n+    if os.path.exists(fname):\\n+        sha1 = hashlib.sha1()\\n+        with open(fname, \\'rb\\') as f:\\n+            while True:\\n+                data = f.read(1048576)\\n+                if not data:\\n+                    break\\n+                sha1.update(data)\\n+        if sha1.hexdigest() == sha1_hash:\\n+            return fname  # Hit cache\\n+    print(f\\'Downloading {fname} from {url}...\\')\\n+    r = requests.get(url, stream=True, verify=True)\\n+    with open(fname, \\'wb\\') as f:\\n+        f.write(r.content)\\n+    return fname\\n+```\\n+\\n+我们还实现了两个附加的实用程序函数：一个是下载并提取一个zip或tar文件，另一个是将本书中使用的所有数据集从`DATA_HUB`下载到缓存目录中。\\n+\\n+```{.python .input}\\n+#@tab all\\n+def download_extract(name, folder=None):  #@save\\n+    \"\"\"Download and extract a zip/tar file.\"\"\"\\n+    fname = download(name)\\n+    base_dir = os.path.dirname(fname)\\n+    data_dir, ext = os.path.splitext(fname)\\n+    if ext == \\'.zip\\':\\n+        fp = zipfile.ZipFile(fname, \\'r\\')\\n+    elif ext in (\\'.tar\\', \\'.gz\\'):\\n+        fp = tarfile.open(fname, \\'r\\')\\n+    else:\\n+        assert False, \\'Only zip/tar files can be extracted.\\'\\n+    fp.extractall(base_dir)\\n+    return os.path.join(base_dir, folder) if folder else data_dir\\n+\\n+def download_all():  #@save\\n+    \"\"\"Download all files in the DATA_HUB.\"\"\"\\n+    for name in DATA_HUB:\\n+        download(name)\\n+```\\n+\\n+## 卡格尔\\n+\\n+[Kaggle](https://www.kaggle.com)是一个流行的平台，主办机器学习比赛。每场比赛都以一个数据集为中心，许多比赛都是由利益相关者赞助的，他们为获胜的解决方案提供奖品。该平台帮助用户通过论坛和共享代码进行互动，促进合作和竞争。虽然排行榜的追逐往往失控，研究人员目光短浅地专注于预处理步骤，而不是问一些基本问题，但客观的平台也有巨大的价值，这个平台有助于在竞争方法之间进行直接定量比较，以及代码共享，以便每个人都能学习什么起作用，什么不起作用。如果你想参加Kaggle竞赛，你首先需要注册一个帐户（见:numref:`fig_kaggle`）。\\n+\\n+![The Kaggle website.](../img/kaggle.png)\\n+:width:`400px`\\n+:label:`fig_kaggle`\\n+\\n+在房价预测竞争页面，如:numref:`fig_house_pricing`所示，您可以找到数据集（在“数据”选项卡下），提交预测，并查看您的排名，网址如下：\\n+\\n+> https://www.kaggle.com/c/house-prices-advanced-regulation-technologies\\n+\\n+![The house price prediction competition page.](../img/house_pricing.png)\\n+:width:`400px`\\n+:label:`fig_house_pricing`\\n+\\n+## 访问和读取数据集\\n+\\n+请注意，竞赛数据分为训练集和测试集。每个记录包括房屋的属性值和属性，如街道类型、建筑年份、屋顶类型、地下室状况等。特征由各种数据类型组成。例如，建筑年份由整数表示，屋顶类型由离散类别指定表示，其他要素由浮点数表示。这就是现实使事情复杂化的地方：在一些例子中，一些数据完全丢失，缺失的值被简单地标记为“na”。每套房子的价格只包含在训练设备中（毕竟这是一场比赛）。我们需要划分训练集来创建一个验证集，但是我们只有在将预测上传到Kaggle之后才能在官方测试集中评估我们的模型。:numref:`fig_house_pricing`中竞争标签上的“数据”标签有下载数据的链接。\\n+\\n+首先，我们将使用:numref:`sec_pandas`中介绍的`pandas`读入并处理数据。因此，在继续下一步之前，您需要确保已经安装了`pandas`。幸运的是，如果你在Jupyter读书，我们甚至不用离开笔记本就可以安装熊猫。\\n+\\n+```{.python .input}\\n+# If pandas is not installed, please uncomment the following line:\\n+# !pip install pandas\\n+\\n+%matplotlib inline\\n+from d2l import mxnet as d2l\\n+from mxnet import gluon, autograd, init, np, npx\\n+from mxnet.gluon import nn\\n+import pandas as pd\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+# If pandas is not installed, please uncomment the following line:\\n+# !pip install pandas\\n+\\n+%matplotlib inline\\n+from d2l import torch as d2l\\n+import torch\\n+import torch.nn as nn\\n+import pandas as pd\\n+import numpy as np\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+# If pandas is not installed, please uncomment the following line:\\n+# !pip install pandas\\n+\\n+%matplotlib inline\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+import pandas as pd\\n+import numpy as np\\n+```\\n+\\n+为了方便起见，我们可以使用上面定义的脚本下载并缓存Kaggle housing数据集。\\n+\\n+```{.python .input}\\n+#@tab all\\n+DATA_HUB[\\'kaggle_house_train\\'] = (  #@save\\n+    DATA_URL + \\'kaggle_house_pred_train.csv\\',\\n+    \\'585e9cc93e70b39160e7921475f9bcd7d31219ce\\')\\n+\\n+DATA_HUB[\\'kaggle_house_test\\'] = (  #@save\\n+    DATA_URL + \\'kaggle_house_pred_test.csv\\',\\n+    \\'fa19780a7b011d9b009e8bff8e99922a8ee2eb90\\')\\n+```\\n+\\n+我们使用`pandas`分别加载包含训练和测试数据的两个csv文件。\\n+\\n+```{.python .input}\\n+#@tab all\\n+train_data = pd.read_csv(download(\\'kaggle_house_train\\'))\\n+test_data = pd.read_csv(download(\\'kaggle_house_test\\'))\\n+```\\n+\\n+训练数据集包括1460个示例、80个特征和1个标签，而测试数据包含1459个示例和80个特征。\\n+\\n+```{.python .input}\\n+#@tab all\\n+print(train_data.shape)\\n+print(test_data.shape)\\n+```\\n+\\n+让我们看看前四个和最后两个特性，以及前四个例子中的标签（SalePrice）。\\n+\\n+```{.python .input}\\n+#@tab all\\n+print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])\\n+```\\n+\\n+我们可以看到，在每个示例中，第一个特性是ID，这有助于模型识别每个训练示例。虽然这很方便，但它不携带任何用于预测的信息。因此，在将数据输入模型之前，我们将其从数据集中删除。\\n+\\n+```{.python .input}\\n+#@tab all\\n+all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))\\n+```\\n+\\n+## 数据预处理\\n+\\n+如上所述，我们有各种各样的数据类型。在开始建模之前，我们需要对数据进行预处理。让我们从数字特征开始。首先，我们使用启发式方法，用相应特征的平均值替换所有缺失值。然后，为了将所有特征放在一个共同的尺度上，我们通过将特征重新调整为零均值和单位方差来*标准化*数据：\\n+\\n+$$x \\\\leftarrow \\\\frac{x - \\\\mu}{\\\\sigma}.$$\\n+\\n+为了验证这确实转换了我们的特征（变量），使其具有零均值和单位方差，请注意$E[\\\\frac{x-\\\\mu}{\\\\sigma}] = \\\\frac{\\\\mu - \\\\mu}{\\\\sigma} = 0$和$E[(x-\\\\mu)^2] = (\\\\sigma^2 + \\\\mu^2) - 2\\\\mu^2+\\\\mu^2 = \\\\sigma^2$。直观地说，我们标准化数据有两个原因。首先，它证明了优化的方便性。第二，因为我们不知道哪些特征是相关的，所以我们不想对分配给一个特征的系数进行惩罚。\\n+\\n+```{.python .input}\\n+#@tab all\\n+numeric_features = all_features.dtypes[all_features.dtypes != \\'object\\'].index\\n+all_features[numeric_features] = all_features[numeric_features].apply(\\n+    lambda x: (x - x.mean()) / (x.std()))\\n+# After standardizing the data all means vanish, hence we can set missing\\n+# values to 0\\n+all_features[numeric_features] = all_features[numeric_features].fillna(0)\\n+```\\n+\\n+接下来我们处理离散值。其中包括“MSZoning”等功能。我们用一个热编码来代替它们，就像我们以前把多类标签转换成向量一样（见:numref:`subsec_classification-problem`）。例如，“MSZoning”假设值为“RL”和“RM”。除去“MSZoning”功能，将创建两个新的指示符功能“MSZoning-RL”和“MSZoning-u-RM”，其值为0或1。根据一种热编码，如果“MSZoning”的原始值为“RL”，那么“MSZoning\\\\u RL”为1，“MSZoning\\\\u RM”为0。`pandas`包自动为我们做这件事。\\n+\\n+```{.python .input}\\n+#@tab all\\n+# `Dummy_na=True` considers \"na\" (missing value) as a valid feature value, and\\n+# creates an indicator feature for it\\n+all_features = pd.get_dummies(all_features, dummy_na=True)\\n+all_features.shape\\n+```\\n+\\n+您可以看到，这种转换将特性的数量从79个增加到331个。最后，通过`values`属性，我们可以从`pandas`格式中提取NumPy格式并将其转换为张量表示进行训练。\\n+\\n+```{.python .input}\\n+#@tab all\\n+n_train = train_data.shape[0]\\n+train_features = d2l.tensor(all_features[:n_train].values, dtype=d2l.float32)\\n+test_features = d2l.tensor(all_features[n_train:].values, dtype=d2l.float32)\\n+train_labels = d2l.tensor(\\n+    train_data.SalePrice.values.reshape(-1, 1), dtype=d2l.float32)\\n+```\\n+\\n+## 培训\\n+\\n+首先，我们训练一个损失平方的线性模型。毫不奇怪，我们的线性模型不会导致竞争获胜，但它提供了一个健全的检查，看看数据中是否存在有意义的信息。如果我们不能比随机猜测做得更好，那么很有可能我们有一个数据处理错误。如果一切顺利的话，线性模型将作为一个基线，让我们对简单模型与最佳报告模型的接近程度有一些直觉，让我们知道我们应该从更漂亮的模型中获得多少收益。\\n+\\n+```{.python .input}\\n+loss = gluon.loss.L2Loss()\\n+\\n+def get_net():\\n+    net = nn.Sequential()\\n+    net.add(nn.Dense(1))\\n+    net.initialize()\\n+    return net\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+loss = nn.MSELoss()\\n+in_features = train_features.shape[1]\\n+\\n+def get_net():\\n+    net = nn.Sequential(nn.Linear(in_features,1))\\n+    return net\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+loss = tf.keras.losses.MeanSquaredError()\\n+\\n+def get_net():\\n+    net = tf.keras.models.Sequential()\\n+    net.add(tf.keras.layers.Dense(\\n+        1, kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\\n+    return net\\n+```\\n+\\n+对于房价，就像股票价格一样，我们关心的是相对数量，而不是绝对数量。因此，我们更关心相对误差$\\\\frac{y - \\\\hat{y}}{y}$，而不是绝对误差$y - \\\\hat{y}$。例如，如果我们在俄亥俄州农村地区估计一栋房子的价格时，我们的预测偏差了10万美元，在那里，一栋典型的房子的价值是12.5万美元，那么我们可能做得很糟糕。另一方面，如果我们在加利福尼亚州洛斯阿尔托斯山的这一数字出现错误，这可能是一个惊人的准确预测（在那里，房价中位数超过400万美元）。\\n+\\n+解决这个问题的一种方法是测量价格估计值对数的差异。事实上，这也是比赛用来评价提交质量的官方误差指标。毕竟，$|\\\\log y - \\\\log \\\\hat{y}| \\\\leq \\\\delta$的小值$\\\\delta$转换为$e^{-\\\\delta} \\\\leq \\\\frac{\\\\hat{y}}{y} \\\\leq e^\\\\delta$。这将导致预测价格的对数与标签价格的对数之间出现以下均方根误差：\\n+\\n+$$\\\\sqrt{\\\\frac{1}{n}\\\\sum_{i=1}^n\\\\left(\\\\log y_i -\\\\log \\\\hat{y}_i\\\\right)^2}.$$\\n+\\n+```{.python .input}\\n+def log_rmse(net, features, labels):\\n+    # To further stabilize the value when the logarithm is taken, set the\\n+    # value less than 1 as 1\\n+    clipped_preds = np.clip(net(features), 1, float(\\'inf\\'))\\n+    return np.sqrt(2 * loss(np.log(clipped_preds), np.log(labels)).mean())\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def log_rmse(net, features, labels):\\n+    # To further stabilize the value when the logarithm is taken, set the\\n+    # value less than 1 as 1\\n+    clipped_preds = torch.clamp(net(features), 1, float(\\'inf\\'))\\n+    rmse = torch.sqrt(torch.mean(loss(torch.log(clipped_preds),\\n+                                       torch.log(labels))))\\n+    return rmse.item()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def log_rmse(y_true, y_pred):\\n+    # To further stabilize the value when the logarithm is taken, set the\\n+    # value less than 1 as 1\\n+    clipped_preds = tf.clip_by_value(y_pred, 1, float(\\'inf\\'))\\n+    return tf.sqrt(tf.reduce_mean(loss(\\n+        tf.math.log(y_true), tf.math.log(clipped_preds))))\\n+```\\n+\\n+与前面的部分不同，我们的培训功能将依赖于Adam优化器（我们将在后面更详细地描述它）。这种优化器的主要吸引力在于，尽管在无限制的超参数优化资源下没有做得更好（有时甚至更差），但人们倾向于发现它对初始学习速率的敏感性明显降低。\\n+\\n+```{.python .input}\\n+def train(net, train_features, train_labels, test_features, test_labels,\\n+          num_epochs, learning_rate, weight_decay, batch_size):\\n+    train_ls, test_ls = [], []\\n+    train_iter = d2l.load_array((train_features, train_labels), batch_size)\\n+    # The Adam optimization algorithm is used here\\n+    trainer = gluon.Trainer(net.collect_params(), \\'adam\\', {\\n+        \\'learning_rate\\': learning_rate, \\'wd\\': weight_decay})\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            with autograd.record():\\n+                l = loss(net(X), y)\\n+            l.backward()\\n+            trainer.step(batch_size)\\n+        train_ls.append(log_rmse(net, train_features, train_labels))\\n+        if test_labels is not None:\\n+            test_ls.append(log_rmse(net, test_features, test_labels))\\n+    return train_ls, test_ls\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def train(net, train_features, train_labels, test_features, test_labels,\\n+          num_epochs, learning_rate, weight_decay, batch_size):\\n+    train_ls, test_ls = [], []\\n+    train_iter = d2l.load_array((train_features, train_labels), batch_size)\\n+    # The Adam optimization algorithm is used here\\n+    optimizer = torch.optim.Adam(net.parameters(),\\n+                                 lr = learning_rate,\\n+                                 weight_decay = weight_decay)\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            optimizer.zero_grad()\\n+            l = loss(net(X), y)\\n+            l.backward()\\n+            optimizer.step()\\n+        train_ls.append(log_rmse(net, train_features, train_labels))\\n+        if test_labels is not None:\\n+            test_ls.append(log_rmse(net, test_features, test_labels))\\n+    return train_ls, test_ls\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def train(net, train_features, train_labels, test_features, test_labels,\\n+          num_epochs, learning_rate, weight_decay, batch_size):\\n+    train_ls, test_ls = [], []\\n+    train_iter = d2l.load_array((train_features, train_labels), batch_size)\\n+    # The Adam optimization algorithm is used here\\n+    optimizer = tf.keras.optimizers.Adam(learning_rate)\\n+    net.compile(loss=loss, optimizer=optimizer)\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            with tf.GradientTape() as tape:\\n+                y_hat = net(X)\\n+                l = loss(y, y_hat)\\n+            params = net.trainable_variables\\n+            grads = tape.gradient(l, params)\\n+            optimizer.apply_gradients(zip(grads, params))\\n+        train_ls.append(log_rmse(train_labels, net(train_features)))\\n+        if test_labels is not None:\\n+            test_ls.append(log_rmse(test_labels, net(test_features)))\\n+    return train_ls, test_ls\\n+```\\n+\\n+## $K$折叠交叉验证\\n+\\n+您可能还记得，我们在讨论如何处理模型选择的部分中介绍了$K$折叠交叉验证（:numref:`sec_model_selection`）。这将有助于模型设计的选择和超参数的调整。我们首先需要一个函数，在$K$折叠交叉验证过程中返回$i^\\\\mathrm{th}$倍的数据。它通过切片$i^\\\\mathrm{th}$段作为验证数据，其余部分作为训练数据返回。请注意，这并不是处理数据的最有效方法，如果我们的数据集大得多，我们肯定会做一些更聪明的事情。但是这种增加的复杂性可能会不必要地混淆我们的代码，因此我们可以安全地忽略它，因为我们的问题很简单。\\n+\\n+```{.python .input}\\n+#@tab all\\n+def get_k_fold_data(k, i, X, y):\\n+    assert k > 1\\n+    fold_size = X.shape[0] // k\\n+    X_train, y_train = None, None\\n+    for j in range(k):\\n+        idx = slice(j * fold_size, (j + 1) * fold_size)\\n+        X_part, y_part = X[idx, :], y[idx]\\n+        if j == i:\\n+            X_valid, y_valid = X_part, y_part\\n+        elif X_train is None:\\n+            X_train, y_train = X_part, y_part\\n+        else:\\n+            X_train = d2l.concat([X_train, X_part], 0)\\n+            y_train = d2l.concat([y_train, y_part], 0)\\n+    return X_train, y_train, X_valid, y_valid\\n+```\\n+\\n+当我们在$K$次交叉验证中训练$K$次时，返回训练和验证误差的平均值。\\n+\\n+```{.python .input}\\n+#@tab all\\n+def k_fold(k, X_train, y_train, num_epochs,\\n+           learning_rate, weight_decay, batch_size):\\n+    train_l_sum, valid_l_sum = 0, 0\\n+    for i in range(k):\\n+        data = get_k_fold_data(k, i, X_train, y_train)\\n+        net = get_net()\\n+        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\\n+                                   weight_decay, batch_size)\\n+        train_l_sum += train_ls[-1]\\n+        valid_l_sum += valid_ls[-1]\\n+        if i == 0:\\n+            d2l.plot(list(range(1, num_epochs+1)), [train_ls, valid_ls],\\n+                     xlabel=\\'epoch\\', ylabel=\\'rmse\\',\\n+                     legend=[\\'train\\', \\'valid\\'], yscale=\\'log\\')\\n+        print(f\\'fold {i + 1}, train log rmse {float(train_ls[-1]):f}, \\'\\n+              f\\'valid log rmse {float(valid_ls[-1]):f}\\')\\n+    return train_l_sum / k, valid_l_sum / k\\n+```\\n+\\n+## 选型\\n+\\n+在本例中，我们选取了一组未经调整的超参数，并将其留给读者来改进模型。找到一个好的选择需要时间，这取决于优化了多少变量。在足够大的数据集和正常类型的超参数的情况下，$K$倍交叉验证对于多重测试具有相当的弹性。然而，如果我们尝试了不合理的大量选项，我们可能会幸运地发现，我们的验证性能不再代表真正的错误。\\n+\\n+```{.python .input}\\n+#@tab all\\n+k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64\\n+train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,\\n+                          weight_decay, batch_size)\\n+print(f\\'{k}-fold validation: avg train log rmse: {float(train_l):f}, \\'\\n+      f\\'avg valid log rmse: {float(valid_l):f}\\')\\n+```\\n+\\n+请注意，有时一组超参数的训练错误数可能非常低，即使$K$倍交叉验证的错误数要高得多。这表明我们过度适应了。在整个培训过程中，您需要监控这两个数字。较少的过度拟合可能表明我们的数据可以支持一个更强大的模型。大规模的过度拟合可能意味着我们可以通过合并正则化技术来获得收益。\\n+\\n+##  提交Kaggle的预测\\n+\\n+既然我们知道了超参数的最佳选择是什么，我们还可以使用所有的数据对其进行训练（而不仅仅是交叉验证切片中使用的$1-1/K$个数据）。我们以这种方式获得的模型可以应用于测试集。将预测保存在csv文件中可以简化将结果上载到Kaggle的过程。\\n+\\n+```{.python .input}\\n+#@tab all\\n+def train_and_pred(train_features, test_feature, train_labels, test_data,\\n+                   num_epochs, lr, weight_decay, batch_size):\\n+    net = get_net()\\n+    train_ls, _ = train(net, train_features, train_labels, None, None,\\n+                        num_epochs, lr, weight_decay, batch_size)\\n+    d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel=\\'epoch\\',\\n+             ylabel=\\'log rmse\\', yscale=\\'log\\')\\n+    print(f\\'train log rmse {float(train_ls[-1]):f}\\')\\n+    # Apply the network to the test set\\n+    preds = d2l.numpy(net(test_features))\\n+    # Reformat it to export to Kaggle\\n+    test_data[\\'SalePrice\\'] = pd.Series(preds.reshape(1, -1)[0])\\n+    submission = pd.concat([test_data[\\'Id\\'], test_data[\\'SalePrice\\']], axis=1)\\n+    submission.to_csv(\\'submission.csv\\', index=False)\\n+```\\n+\\n+一个很好的健全性检查是看看测试集上的预测是否类似于$K$倍交叉验证过程的预测。如果有，是时候把它们上传到Kaggle了。下面的代码将生成一个名为`submission.csv`的文件。\\n+\\n+```{.python .input}\\n+#@tab all\\n+train_and_pred(train_features, test_features, train_labels, test_data,\\n+               num_epochs, lr, weight_decay, batch_size)\\n+```\\n+\\n+接下来，如:numref:`fig_kaggle_submit2`所示，我们可以提交我们对Kaggle的预测，看看它们与测试集上实际房价（标签）的比较情况。步骤非常简单：\\n+\\n+* 登录Kaggle网站，访问房价预测竞赛页面。\\n+* 点击“提交预测”或“延迟提交”按钮（在本文撰写之际，按钮位于右侧）。\\n+* 点击页面底部虚线框中的“上传提交文件”按钮，选择要上传的预测文件。\\n+* 点击页面底部的“提交”按钮查看结果。\\n+\\n+![Submitting data to Kaggle](../img/kaggle_submit2.png)\\n+:width:`400px`\\n+:label:`fig_kaggle_submit2`\\n+\\n+## 摘要\\n+\\n+* 实际数据通常包含不同数据类型的混合，需要进行预处理。\\n+* 将实值数据重新调整为零均值和单位方差是一个很好的默认设置。用平均值替换缺失值也是如此。\\n+* 将范畴特征转化为指标特征，可以使我们把它们当作一个热点向量来对待。\\n+* 我们可以使用$K$折叠交叉验证来选择模型和调整超参数。\\n+* 对数对于相对误差很有用。\\n+\\n+## 练习\\n+\\n+1. 把你对这一部分的预测提交给卡格尔。你的预测有多好？\\n+1. 你能通过直接最小化价格对数来改进你的模型吗？如果你试图预测价格的对数而不是价格，会发生什么？\\n+1. 用它们的平均值替换缺失值总是一个好主意吗？提示：你能构造一个不随机丢失值的情况吗？\\n+1. 通过$K$倍交叉验证调整超参数，提高Kaggle的得分。\\n+1. 通过改进模型来提高分数（例如，层数、重量衰减和脱落）。\\n+1. 如果我们没有像本节所做的那样标准化连续的数值特征，会发生什么？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/106)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/107)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/237)\\n+:end_tab:\\ndiff --git a/chapter_multilayer-perceptrons/kaggle-house-price_tencent.md b/chapter_multilayer-perceptrons/kaggle-house-price_tencent.md\\nnew file mode 100644\\nindex 00000000..b0b95bf4\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/kaggle-house-price_tencent.md\\n@@ -0,0 +1,483 @@\\n+# 基于Kaggle的房价预测\\n+:label:`sec_kaggle_house`\\n+\\n+现在我们已经介绍了一些建立和训练深层网络的基本工具，并使用重量衰减和辍学等技术将它们正规化，我们准备通过参加Kaggle比赛来将所有这些知识付诸实践。房价预测大赛是一个很好的起点。这些数据是相当通用的，不会表现出可能需要特殊模型的奇异结构(就像音频或视频可能需要的那样)。此数据集由Bart de Cock于2011年收集，涵盖了:cite:`De-Cock.2011`-2010年期间亚利桑那州埃姆斯的房价。它比著名的哈里森和鲁宾菲尔德(1978年)的[Boston housing dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)要大得多，既有更多的例子，也有更多的特征。\\n+\\n+在本节中，我们将详细介绍数据预处理、模型设计和超参数选择。我们希望通过亲身实践的方式，您将获得一些直觉，这些直觉将指导您作为数据科学家的职业生涯。\\n+\\n+## 下载和缓存数据集\\n+\\n+在整本书中，我们将在各种下载的数据集上训练和测试模型。在这里，我们实现了几个实用程序函数来方便数据下载。首先，我们维护字典`DATA_HUB`，其将字符串(数据集的*名称*)映射到包含定位数据集的url和验证文件完整性的sha-1密钥两者的元组。所有这样的数据集都托管在地址为`DATA_URL`的站点上。\\n+\\n+```{.python .input}\\n+#@tab all\\n+import os\\n+import requests\\n+import zipfile\\n+import tarfile\\n+import hashlib\\n+\\n+#@save\\n+DATA_HUB = dict()\\n+DATA_URL = \\'http://d2l-data.s3-accelerate.amazonaws.com/\\'\\n+```\\n+\\n+以下`download`函数下载数据集，将其缓存在本地目录(默认情况下为`../data`)中，并返回下载文件的名称。如果缓存目录中已经存在与此数据集对应的文件，并且其sha-1与存储在`DATA_HUB`中的文件匹配，我们的代码将使用缓存的文件，以避免冗余下载阻塞您的互联网。\\n+\\n+```{.python .input}\\n+#@tab all\\n+def download(name, cache_dir=os.path.join(\\'..\\', \\'data\\')):  #@save\\n+    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\\n+    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\\n+    url, sha1_hash = DATA_HUB[name]\\n+    d2l.mkdir_if_not_exist(cache_dir)\\n+    fname = os.path.join(cache_dir, url.split(\\'/\\')[-1])\\n+    if os.path.exists(fname):\\n+        sha1 = hashlib.sha1()\\n+        with open(fname, \\'rb\\') as f:\\n+            while True:\\n+                data = f.read(1048576)\\n+                if not data:\\n+                    break\\n+                sha1.update(data)\\n+        if sha1.hexdigest() == sha1_hash:\\n+            return fname  # Hit cache\\n+    print(f\\'Downloading {fname} from {url}...\\')\\n+    r = requests.get(url, stream=True, verify=True)\\n+    with open(fname, \\'wb\\') as f:\\n+        f.write(r.content)\\n+    return fname\\n+```\\n+\\n+我们还实现了两个附加的实用程序功能：一个是下载并解压缩一个zip或tar文件，另一个是将本书中使用的所有数据集从`DATA_HUB`下载到缓存目录中。\\n+\\n+```{.python .input}\\n+#@tab all\\n+def download_extract(name, folder=None):  #@save\\n+    \"\"\"Download and extract a zip/tar file.\"\"\"\\n+    fname = download(name)\\n+    base_dir = os.path.dirname(fname)\\n+    data_dir, ext = os.path.splitext(fname)\\n+    if ext == \\'.zip\\':\\n+        fp = zipfile.ZipFile(fname, \\'r\\')\\n+    elif ext in (\\'.tar\\', \\'.gz\\'):\\n+        fp = tarfile.open(fname, \\'r\\')\\n+    else:\\n+        assert False, \\'Only zip/tar files can be extracted.\\'\\n+    fp.extractall(base_dir)\\n+    return os.path.join(base_dir, folder) if folder else data_dir\\n+\\n+def download_all():  #@save\\n+    \"\"\"Download all files in the DATA_HUB.\"\"\"\\n+    for name in DATA_HUB:\\n+        download(name)\\n+```\\n+\\n+## 卡格尔\\n+\\n+[Kaggle](https://www.kaggle.com)是一个流行的举办机器学习比赛的平台。每场比赛都以一个数据集为中心，许多比赛都是由利益相关者赞助的，他们为获胜的解决方案提供奖金。该平台帮助用户通过论坛和共享代码进行互动，促进协作和竞争。虽然排行榜的追逐往往失控，研究人员短视地专注于预处理步骤，而不是询问基本问题，但一个平台的客观性也有巨大的价值，该平台促进了竞争方法之间的直接定量比较，以及代码共享，以便每个人都可以了解哪些方法起作用了，哪些没有起作用。如果你想参加Kagle比赛，你首先需要注册一个账户(见:numref:`fig_kaggle`)。\\n+\\n+![The Kaggle website.](../img/kaggle.png)\\n+:width:`400px`\\n+:label:`fig_kaggle`\\n+\\n+在房价预测竞赛页面，如:numref:`fig_house_pricing`所示，您可以找到数据集(在[数据]选项卡下)，提交预测，并查看您的排名，网址就在这里：\\n+\\n+>https://www.kaggle.com/c/house-prices-advanced-regression-techniques\\n+\\n+![The house price prediction competition page.](../img/house_pricing.png)\\n+:width:`400px`\\n+:label:`fig_house_pricing`\\n+\\n+## 访问和读取数据集\\n+\\n+请注意，竞赛数据分为训练集和测试集。每条记录都包括房屋的属性值和属性，如街道类型、施工年份、屋顶类型、地下室状况等。这些功能由各种数据类型组成。例如，建筑年份由整数表示，屋顶类型由离散类别指定表示，其他要素由浮点数表示。这就是现实让事情变得复杂的地方：例如，一些数据完全丢失了，缺失值被简单地标记为“NA”。每套房子的价格只包括培训套装(毕竟这是一场比赛)。我们将希望划分训练集以创建验证集，但是在将预测上传到Kaggle之后，我们只能在官方测试集中评估我们的模型。在:numref:`fig_house_pricing`中，竞争选项卡上的“数据”选项卡有下载数据的链接。\\n+\\n+开始之前，我们将使用`pandas`读入并处理数据，这是我们在:numref:`sec_pandas`中引入的。因此，在继续操作之前，您需要确保已安装`pandas`。幸运的是，如果你正在用木星阅读，我们甚至可以在不离开笔记本的情况下安装熊猫。\\n+\\n+```{.python .input}\\n+# If pandas is not installed, please uncomment the following line:\\n+# !pip install pandas\\n+\\n+%matplotlib inline\\n+from d2l import mxnet as d2l\\n+from mxnet import gluon, autograd, init, np, npx\\n+from mxnet.gluon import nn\\n+import pandas as pd\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+# If pandas is not installed, please uncomment the following line:\\n+# !pip install pandas\\n+\\n+%matplotlib inline\\n+from d2l import torch as d2l\\n+import torch\\n+import torch.nn as nn\\n+import pandas as pd\\n+import numpy as np\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+# If pandas is not installed, please uncomment the following line:\\n+# !pip install pandas\\n+\\n+%matplotlib inline\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+import pandas as pd\\n+import numpy as np\\n+```\\n+\\n+为方便起见，我们可以使用上面定义的脚本下载并缓存Kaggle住房数据集。\\n+\\n+```{.python .input}\\n+#@tab all\\n+DATA_HUB[\\'kaggle_house_train\\'] = (  #@save\\n+    DATA_URL + \\'kaggle_house_pred_train.csv\\',\\n+    \\'585e9cc93e70b39160e7921475f9bcd7d31219ce\\')\\n+\\n+DATA_HUB[\\'kaggle_house_test\\'] = (  #@save\\n+    DATA_URL + \\'kaggle_house_pred_test.csv\\',\\n+    \\'fa19780a7b011d9b009e8bff8e99922a8ee2eb90\\')\\n+```\\n+\\n+我们使用`pandas`加载分别包含训练数据和测试数据的两个csv文件。\\n+\\n+```{.python .input}\\n+#@tab all\\n+train_data = pd.read_csv(download(\\'kaggle_house_train\\'))\\n+test_data = pd.read_csv(download(\\'kaggle_house_test\\'))\\n+```\\n+\\n+训练数据集包括1460个样本、80个特征和1个标签，而测试数据包含1459个样本和80个特征。\\n+\\n+```{.python .input}\\n+#@tab all\\n+print(train_data.shape)\\n+print(test_data.shape)\\n+```\\n+\\n+让我们看看前四个和最后两个功能，以及前四个示例中的标签(SalePrice)。\\n+\\n+```{.python .input}\\n+#@tab all\\n+print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])\\n+```\\n+\\n+我们可以看到，在每个示例中，第一个特征是ID，这有助于模型识别每个训练示例。虽然这很方便，但它不携带任何用于预测目的的信息。因此，在将数据提供给模型之前，我们将其从数据集中删除。\\n+\\n+```{.python .input}\\n+#@tab all\\n+all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))\\n+```\\n+\\n+## 数据预处理\\n+\\n+如上所述，我们有各种各样的数据类型。在开始建模之前，我们需要对数据进行预处理。让我们从数字特征开始。首先，我们应用启发式方法，将所有缺失的值替换为相应特征的平均值。然后，为了将所有要素放在一个共同的尺度上，我们通过将要素重新缩放到零均值和单位方差来“标准化”数据：\\n+\\n+$$x \\\\leftarrow \\\\frac{x - \\\\mu}{\\\\sigma}.$$\\n+\\n+要验证这是否确实转换了我们的特征(变量)，使其具有零均值和单位方差，请注意$E[\\\\frac{x-\\\\mu}{\\\\sigma}] = \\\\frac{\\\\mu - \\\\mu}{\\\\sigma} = 0$和$E[(x-\\\\mu)^2] = (\\\\sigma^2 + \\\\mu^2) - 2\\\\mu^2+\\\\mu^2 = \\\\sigma^2$。直观地说，我们标准化数据有两个原因。首先，它证明了优化的方便性。其次，因为我们不知道“先验”哪些特征将是相关的，所以我们不想惩罚分配给一个特征的系数比分配给其他任何特征的系数更多。\\n+\\n+```{.python .input}\\n+#@tab all\\n+numeric_features = all_features.dtypes[all_features.dtypes != \\'object\\'].index\\n+all_features[numeric_features] = all_features[numeric_features].apply(\\n+    lambda x: (x - x.mean()) / (x.std()))\\n+# After standardizing the data all means vanish, hence we can set missing\\n+# values to 0\\n+all_features[numeric_features] = all_features[numeric_features].fillna(0)\\n+```\\n+\\n+接下来，我们处理离散值。这包括诸如“MSZONING”之类的功能。我们用一次热编码替换它们，方法与前面将多类标签转换为向量的方式相同(请参见:numref:`subsec_classification-problem`)。例如，“MSZtioning”采用值“RL”和“Rm”。删除“MSZONING”特性，将创建两个新的指示器特性“MSZONING_RL”和“MSZONING_RM”，其值为0或1。根据一热编码，如果“MSZONING”的原始值为“RL”，则“MSZONING_RL”为1，“MSZONING_RM”为0。`pandas`软件包会自动为我们实现这一点。\\n+\\n+```{.python .input}\\n+#@tab all\\n+# `Dummy_na=True` considers \"na\" (missing value) as a valid feature value, and\\n+# creates an indicator feature for it\\n+all_features = pd.get_dummies(all_features, dummy_na=True)\\n+all_features.shape\\n+```\\n+\\n+您可以看到，此转换会将要素的数量从79个增加到331个。最后，通过`values`属性，我们可以从`pandas`格式中提取NumPy格式，并将其转换为张量表示进行训练。\\n+\\n+```{.python .input}\\n+#@tab all\\n+n_train = train_data.shape[0]\\n+train_features = d2l.tensor(all_features[:n_train].values, dtype=d2l.float32)\\n+test_features = d2l.tensor(all_features[n_train:].values, dtype=d2l.float32)\\n+train_labels = d2l.tensor(\\n+    train_data.SalePrice.values.reshape(-1, 1), dtype=d2l.float32)\\n+```\\n+\\n+## 培训\\n+\\n+首先，我们训练一个损失平方的线性模型。不足为奇的是，我们的线性模型不会产生赢得竞争的深渊翻滚，但它提供了一种理智检查，以查看数据中是否存在有意义的信息。如果我们在这里不能做得比随机猜测更好，那么我们很可能有一个数据处理错误。如果一切顺利，线性模型将作为基线，给我们一些直观的信息，让我们知道简单的模型离最好的报告模型有多近，让我们感觉到我们应该从更漂亮的模型中获得多少收益。\\n+\\n+```{.python .input}\\n+loss = gluon.loss.L2Loss()\\n+\\n+def get_net():\\n+    net = nn.Sequential()\\n+    net.add(nn.Dense(1))\\n+    net.initialize()\\n+    return net\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+loss = nn.MSELoss()\\n+in_features = train_features.shape[1]\\n+\\n+def get_net():\\n+    net = nn.Sequential(nn.Linear(in_features,1))\\n+    return net\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+loss = tf.keras.losses.MeanSquaredError()\\n+\\n+def get_net():\\n+    net = tf.keras.models.Sequential()\\n+    net.add(tf.keras.layers.Dense(\\n+        1, kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\\n+    return net\\n+```\\n+\\n+对于房价，就像股票价格一样，我们更关心的是相对量而不是绝对量。因此，我们倾向于更关心相对误差$\\\\frac{y - \\\\hat{y}}{y}$而不是绝对误差$y - \\\\hat{y}$。例如，如果我们在估计俄亥俄州乡村地区一套房子的价格时，我们的预测偏差了100,000美元，那里的典型房屋价值为125,000美元，那么我们可能做得很糟糕。另一方面，如果我们在加利福尼亚州的洛斯阿尔托斯山(Los Altos Hills)这个数字上出错，这可能代表着一个令人震惊的准确预测(在那里，房价中值超过400万美元)。\\n+\\n+解决这个问题的一种方法是测量价格估计对数中的差异。事实上，这也是大赛用来评估投稿质量的官方误差衡量标准。毕竟，$|\\\\log y - \\\\log \\\\hat{y}| \\\\leq \\\\delta$的小值$\\\\delta$将转换为$e^{-\\\\delta} \\\\leq \\\\frac{\\\\hat{y}}{y} \\\\leq e^\\\\delta$。这导致预测价格的对数和标签价格的对数之间的均方根误差如下：\\n+\\n+$$\\\\sqrt{\\\\frac{1}{n}\\\\sum_{i=1}^n\\\\left(\\\\log y_i -\\\\log \\\\hat{y}_i\\\\right)^2}.$$\\n+\\n+```{.python .input}\\n+def log_rmse(net, features, labels):\\n+    # To further stabilize the value when the logarithm is taken, set the\\n+    # value less than 1 as 1\\n+    clipped_preds = np.clip(net(features), 1, float(\\'inf\\'))\\n+    return np.sqrt(2 * loss(np.log(clipped_preds), np.log(labels)).mean())\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def log_rmse(net, features, labels):\\n+    # To further stabilize the value when the logarithm is taken, set the\\n+    # value less than 1 as 1\\n+    clipped_preds = torch.clamp(net(features), 1, float(\\'inf\\'))\\n+    rmse = torch.sqrt(torch.mean(loss(torch.log(clipped_preds),\\n+                                       torch.log(labels))))\\n+    return rmse.item()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def log_rmse(y_true, y_pred):\\n+    # To further stabilize the value when the logarithm is taken, set the\\n+    # value less than 1 as 1\\n+    clipped_preds = tf.clip_by_value(y_pred, 1, float(\\'inf\\'))\\n+    return tf.sqrt(tf.reduce_mean(loss(\\n+        tf.math.log(y_true), tf.math.log(clipped_preds))))\\n+```\\n+\\n+与前几节不同的是，我们的培训功能将依赖于ADAM优化器(稍后我们将对其进行更详细的描述)。这种优化器的主要吸引力在于，尽管在为超参数优化提供无限资源的情况下，它并没有做得更好(有时甚至更差)，但人们往往会发现它对初始学习率的敏感度要低得多。\\n+\\n+```{.python .input}\\n+def train(net, train_features, train_labels, test_features, test_labels,\\n+          num_epochs, learning_rate, weight_decay, batch_size):\\n+    train_ls, test_ls = [], []\\n+    train_iter = d2l.load_array((train_features, train_labels), batch_size)\\n+    # The Adam optimization algorithm is used here\\n+    trainer = gluon.Trainer(net.collect_params(), \\'adam\\', {\\n+        \\'learning_rate\\': learning_rate, \\'wd\\': weight_decay})\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            with autograd.record():\\n+                l = loss(net(X), y)\\n+            l.backward()\\n+            trainer.step(batch_size)\\n+        train_ls.append(log_rmse(net, train_features, train_labels))\\n+        if test_labels is not None:\\n+            test_ls.append(log_rmse(net, test_features, test_labels))\\n+    return train_ls, test_ls\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def train(net, train_features, train_labels, test_features, test_labels,\\n+          num_epochs, learning_rate, weight_decay, batch_size):\\n+    train_ls, test_ls = [], []\\n+    train_iter = d2l.load_array((train_features, train_labels), batch_size)\\n+    # The Adam optimization algorithm is used here\\n+    optimizer = torch.optim.Adam(net.parameters(),\\n+                                 lr = learning_rate,\\n+                                 weight_decay = weight_decay)\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            optimizer.zero_grad()\\n+            l = loss(net(X), y)\\n+            l.backward()\\n+            optimizer.step()\\n+        train_ls.append(log_rmse(net, train_features, train_labels))\\n+        if test_labels is not None:\\n+            test_ls.append(log_rmse(net, test_features, test_labels))\\n+    return train_ls, test_ls\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def train(net, train_features, train_labels, test_features, test_labels,\\n+          num_epochs, learning_rate, weight_decay, batch_size):\\n+    train_ls, test_ls = [], []\\n+    train_iter = d2l.load_array((train_features, train_labels), batch_size)\\n+    # The Adam optimization algorithm is used here\\n+    optimizer = tf.keras.optimizers.Adam(learning_rate)\\n+    net.compile(loss=loss, optimizer=optimizer)\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            with tf.GradientTape() as tape:\\n+                y_hat = net(X)\\n+                l = loss(y, y_hat)\\n+            params = net.trainable_variables\\n+            grads = tape.gradient(l, params)\\n+            optimizer.apply_gradients(zip(grads, params))\\n+        train_ls.append(log_rmse(train_labels, net(train_features)))\\n+        if test_labels is not None:\\n+            test_ls.append(log_rmse(test_labels, net(test_features)))\\n+    return train_ls, test_ls\\n+```\\n+\\n+## $K$倍交叉验证\\n+\\n+您可能还记得，我们在讨论如何处理模型选择(:numref:`sec_model_selection`)一节中介绍了$K$次交叉验证。我们将很好地利用这一点来选择模型设计和调整超参数。我们首先需要一个函数，它在$i^\\\\mathrm{th}$次交叉验证过程中返回$K$次数据。它继续将$i^\\\\mathrm{th}$个片段作为验证数据切分，并将睡觉作为训练数据返回。请注意，这不是处理数据的最有效方式，如果我们的数据集大得多，我们肯定会做一些更聪明的事情。但是这种增加的复杂性可能会不必要地混淆我们的代码，因此我们可以安全地在这里省略它，因为我们的问题很简单。\\n+\\n+```{.python .input}\\n+#@tab all\\n+def get_k_fold_data(k, i, X, y):\\n+    assert k > 1\\n+    fold_size = X.shape[0] // k\\n+    X_train, y_train = None, None\\n+    for j in range(k):\\n+        idx = slice(j * fold_size, (j + 1) * fold_size)\\n+        X_part, y_part = X[idx, :], y[idx]\\n+        if j == i:\\n+            X_valid, y_valid = X_part, y_part\\n+        elif X_train is None:\\n+            X_train, y_train = X_part, y_part\\n+        else:\\n+            X_train = d2l.concat([X_train, X_part], 0)\\n+            y_train = d2l.concat([y_train, y_part], 0)\\n+    return X_train, y_train, X_valid, y_valid\\n+```\\n+\\n+当我们在$K$次交叉验证中训练$K$次时，返回训练和验证误差的平均值。\\n+\\n+```{.python .input}\\n+#@tab all\\n+def k_fold(k, X_train, y_train, num_epochs,\\n+           learning_rate, weight_decay, batch_size):\\n+    train_l_sum, valid_l_sum = 0, 0\\n+    for i in range(k):\\n+        data = get_k_fold_data(k, i, X_train, y_train)\\n+        net = get_net()\\n+        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\\n+                                   weight_decay, batch_size)\\n+        train_l_sum += train_ls[-1]\\n+        valid_l_sum += valid_ls[-1]\\n+        if i == 0:\\n+            d2l.plot(list(range(1, num_epochs+1)), [train_ls, valid_ls],\\n+                     xlabel=\\'epoch\\', ylabel=\\'rmse\\',\\n+                     legend=[\\'train\\', \\'valid\\'], yscale=\\'log\\')\\n+        print(f\\'fold {i + 1}, train log rmse {float(train_ls[-1]):f}, \\'\\n+              f\\'valid log rmse {float(valid_ls[-1]):f}\\')\\n+    return train_l_sum / k, valid_l_sum / k\\n+```\\n+\\n+## 选型\\n+\\n+在本例中，我们选择了一组未调优的超参数，并将其留给读者来改进模型。找到一个好的选择可能需要时间，这取决于一个人优化了多少变量。有了足够大的数据集和正常类型的超参数，$K$倍交叉验证往往对多个测试具有相当的弹性。然而，如果我们尝试了不合理的大量选项，我们可能会很幸运地发现我们的验证性能不再代表真正的错误。\\n+\\n+```{.python .input}\\n+#@tab all\\n+k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64\\n+train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,\\n+                          weight_decay, batch_size)\\n+print(f\\'{k}-fold validation: avg train log rmse: {float(train_l):f}, \\'\\n+      f\\'avg valid log rmse: {float(valid_l):f}\\')\\n+```\\n+\\n+请注意，有时一组超参数的训练错误数量可能非常低，即使$K$倍交叉验证的错误数量要高得多。这表明我们过于适应了。在整个培训过程中，您将希望监控这两个数字。较少的过度拟合可能表明我们的数据可以支持更强大的模型。大规模的过度拟合可能表明，我们可以通过纳入正则化技术来获益。\\n+\\n+##  提交关于Kaggle的预测\\n+\\n+既然我们知道应该选择什么样的超参数，我们不妨使用所有数据对其进行训练(而不是仅使用交叉验证片中使用的$1-1/K$的数据)。然后，我们通过这种方式获得的模型可以应用于测试集。将预测保存在CSV文件中可以简化将结果上传到Kaggle的过程。\\n+\\n+```{.python .input}\\n+#@tab all\\n+def train_and_pred(train_features, test_feature, train_labels, test_data,\\n+                   num_epochs, lr, weight_decay, batch_size):\\n+    net = get_net()\\n+    train_ls, _ = train(net, train_features, train_labels, None, None,\\n+                        num_epochs, lr, weight_decay, batch_size)\\n+    d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel=\\'epoch\\',\\n+             ylabel=\\'log rmse\\', yscale=\\'log\\')\\n+    print(f\\'train log rmse {float(train_ls[-1]):f}\\')\\n+    # Apply the network to the test set\\n+    preds = d2l.numpy(net(test_features))\\n+    # Reformat it to export to Kaggle\\n+    test_data[\\'SalePrice\\'] = pd.Series(preds.reshape(1, -1)[0])\\n+    submission = pd.concat([test_data[\\'Id\\'], test_data[\\'SalePrice\\']], axis=1)\\n+    submission.to_csv(\\'submission.csv\\', index=False)\\n+```\\n+\\n+一个不错的理智检查是查看测试集上的预测是否与$K$倍交叉验证过程中的预测相似。如果他们这样做了，那就是时候把它们上传到Kaggle了。下面的代码将生成一个名为`submission.csv`的文件。\\n+\\n+```{.python .input}\\n+#@tab all\\n+train_and_pred(train_features, test_features, train_labels, test_data,\\n+               num_epochs, lr, weight_decay, batch_size)\\n+```\\n+\\n+接下来，如:numref:`fig_kaggle_submit2`中所示，我们可以提交对Kaggle的预测，并查看它们与测试集上的实际房价(标签)的比较情况。步骤非常简单：\\n+\\n+* 登录Kaggle网站，访问房价预测竞赛页面。\\n+* 点击“提交预测”或“延迟深渊翻滚”按钮(在撰写本文时，该按钮位于右侧)。\\n+* 点击页面底部虚线框中的[上传深渊翻滚文件]按钮，选择您要上传的预测文件。\\n+* 点击页面底部的“制作深渊翻滚”按钮，即可查看您的结果。\\n+\\n+![Submitting data to Kaggle](../img/kaggle_submit2.png)\\n+:width:`400px`\\n+:label:`fig_kaggle_submit2`\\n+\\n+## 摘要\\n+\\n+* 真实数据通常包含不同数据类型的混合，需要进行预处理。\\n+* 将实值数据重新缩放为零均值和单位方差是一个很好的默认设置。用它们的平均值替换缺失的值也是如此。\\n+* 将分类特征转换为指示特征允许我们把它们当作一个热点向量来对待。\\n+* 我们可以使用$K$倍交叉验证来选择模型并调整超参数。\\n+* 对数对于相对误差很有用。\\n+\\n+## 练习\\n+\\n+1. 将您对此部分的预测提交给Kaggle。你的预测有多准确？\\n+1. 你能通过直接最小化价格的对数来改进你的模型吗？如果你试图预测价格的对数而不是价格，会发生什么？\\n+1. 用平均值替换缺少的值总是好主意吗？提示：您能构造一个值不会随机丢失的情况吗？\\n+1. 通过$K$倍交叉验证调整超参数，从而提高Kaggle的得分。\\n+1. 通过改进模型(例如，层、权重衰减和丢失)来提高分数。\\n+1. 如果我们不像我们在本节中所做的那样对连续的数字特征进行标准化，会发生什么情况？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/106)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/107)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/237)\\n+:end_tab:\\ndiff --git a/chapter_multilayer-perceptrons/mlp-concise_baidu.md b/chapter_multilayer-perceptrons/mlp-concise_baidu.md\\nnew file mode 100644\\nindex 00000000..7c289f39\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/mlp-concise_baidu.md\\n@@ -0,0 +1,110 @@\\n+# 多层感知器的简明实现\\n+:label:`sec_mlp_concise`\\n+\\n+如您所料，通过依赖高级api，我们可以更简洁地实现mlp。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import gluon, init, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+```\\n+\\n+## 模型\\n+\\n+与我们的softmax回归实现的简明实现（:numref:`sec_softmax_concise`）相比，唯一的区别是我们添加了\\n+*两个*完全连接的层\\n+（之前，我们添加了*1*）。第一个是我们的隐藏层，它包含256个隐藏单元并应用ReLU激活函数。第二个是我们的输出层。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nn.Dense(256, activation=\\'relu\\'),\\n+        nn.Dense(10))\\n+net.initialize(init.Normal(sigma=0.01))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(nn.Flatten(),\\n+                    nn.Linear(784, 256),\\n+                    nn.ReLU(),\\n+                    nn.Linear(256, 10))\\n+\\n+def init_weights(m):\\n+    if type(m) == nn.Linear:\\n+        torch.nn.init.normal_(m.weight, std=0.01)\\n+\\n+net.apply(init_weights)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Flatten(),\\n+    tf.keras.layers.Dense(256, activation=\\'relu\\'),\\n+    tf.keras.layers.Dense(10)])\\n+```\\n+\\n+训练循环与我们实现softmax回归时完全相同。这种模块化使我们能够将与模型体系结构有关的事项与正交考虑因素分开。\\n+\\n+```{.python .input}\\n+batch_size, lr, num_epochs = 256, 0.1, 10\\n+loss = gluon.loss.SoftmaxCrossEntropyLoss()\\n+trainer = gluon.Trainer(net.collect_params(), \\'sgd\\', {\\'learning_rate\\': lr})\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+batch_size, lr, num_epochs = 256, 0.1, 10\\n+loss = nn.CrossEntropyLoss()\\n+trainer = torch.optim.SGD(net.parameters(), lr=lr)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+batch_size, lr, num_epochs = 256, 0.1, 10\\n+loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\\n+trainer = tf.keras.optimizers.SGD(learning_rate=lr)\\n+```\\n+\\n+```{.python .input}\\n+#@tab all\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\\n+```\\n+\\n+## 摘要\\n+\\n+* 使用高级api，我们可以更简洁地实现mlp。\\n+* 对于同一个分类问题，MLP的实现与softmax回归的实现相同，只是附加了具有激活函数的隐藏层。\\n+\\n+## 练习\\n+\\n+1. 尝试添加不同数量的隐藏层（也可以修改学习率）。什么设置效果最好？\\n+1. 尝试不同的激活功能。哪个最有效？\\n+1. 尝试不同的方案来初始化权重。什么方法最有效？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/94)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/95)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/262)\\n+:end_tab:\\ndiff --git a/chapter_multilayer-perceptrons/mlp-concise_tencent.md b/chapter_multilayer-perceptrons/mlp-concise_tencent.md\\nnew file mode 100644\\nindex 00000000..e21c90c7\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/mlp-concise_tencent.md\\n@@ -0,0 +1,110 @@\\n+# 多层感知器的简明实现\\n+:label:`sec_mlp_concise`\\n+\\n+正如您可能预期的那样，通过依赖高级API，我们可以更简洁地实现MLP。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import gluon, init, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+```\\n+\\n+## 型号\\n+\\n+与我们的Softmax回归实现(:numref:`sec_softmax_concise`)的简明实现相比，唯一的区别是我们添加了\\n+*两个*完全连接的层\\n+(之前我们增加了*一*)。第一个是我们的隐藏层，它包含256个隐藏单元，并应用了RELU激活功能。第二个是我们的输出层。\\n+\\n+```{.python .input}\\n+net = nn.Sequential()\\n+net.add(nn.Dense(256, activation=\\'relu\\'),\\n+        nn.Dense(10))\\n+net.initialize(init.Normal(sigma=0.01))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+net = nn.Sequential(nn.Flatten(),\\n+                    nn.Linear(784, 256),\\n+                    nn.ReLU(),\\n+                    nn.Linear(256, 10))\\n+\\n+def init_weights(m):\\n+    if type(m) == nn.Linear:\\n+        torch.nn.init.normal_(m.weight, std=0.01)\\n+\\n+net.apply(init_weights)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+net = tf.keras.models.Sequential([\\n+    tf.keras.layers.Flatten(),\\n+    tf.keras.layers.Dense(256, activation=\\'relu\\'),\\n+    tf.keras.layers.Dense(10)])\\n+```\\n+\\n+训练循环与我们实施Softmax回归时完全相同。这种模块性使我们能够将有关模型体系结构的事项从正交考虑中分离出来。\\n+\\n+```{.python .input}\\n+batch_size, lr, num_epochs = 256, 0.1, 10\\n+loss = gluon.loss.SoftmaxCrossEntropyLoss()\\n+trainer = gluon.Trainer(net.collect_params(), \\'sgd\\', {\\'learning_rate\\': lr})\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+batch_size, lr, num_epochs = 256, 0.1, 10\\n+loss = nn.CrossEntropyLoss()\\n+trainer = torch.optim.SGD(net.parameters(), lr=lr)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+batch_size, lr, num_epochs = 256, 0.1, 10\\n+loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\\n+trainer = tf.keras.optimizers.SGD(learning_rate=lr)\\n+```\\n+\\n+```{.python .input}\\n+#@tab all\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\\n+```\\n+\\n+## 摘要\\n+\\n+* 使用高级API，我们可以更简洁地实现MLP。\\n+* 对于相同的分类问题，MLP的实现与Softmax回归的实现相同，只是增加了具有激活函数的隐含层。\\n+\\n+## 练习\\n+\\n+1. 尝试添加不同数量的隐藏层(您也可以修改学习速率)。哪种设置效果最好？\\n+1. 尝试不同的激活功能。哪一个效果最好？\\n+1. 尝试不同的初始化权重方案。什么方法最有效？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/94)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/95)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/262)\\n+:end_tab:\\ndiff --git a/chapter_multilayer-perceptrons/mlp-scratch_baidu.md b/chapter_multilayer-perceptrons/mlp-scratch_baidu.md\\nnew file mode 100644\\nindex 00000000..a5eb807b\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/mlp-scratch_baidu.md\\n@@ -0,0 +1,202 @@\\n+# 从零开始实现多层感知器\\n+:label:`sec_mlp_scratch`\\n+\\n+既然我们已经在数学上描述了多层感知器（mlp），让我们自己尝试实现一个。为了与我们之前用softmax回归（:numref:`sec_softmax_scratch`）获得的结果进行比较，我们将继续使用Fashion MNIST图像分类数据集（:numref:`sec_fashion_mnist`）。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import gluon, np, npx\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+```\\n+\\n+```{.python .input}\\n+#@tab all\\n+batch_size = 256\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\\n+```\\n+\\n+## 初始化模型参数\\n+\\n+回想一下，Fashion MNIST包含10个类，每个图像由$28 \\\\times 28 = 784$个灰度像素值网格组成。同样，我们暂时不考虑像素之间的空间结构，因此我们可以将其视为一个包含784个输入特征和10个类的分类数据集。首先，我们将实现一个具有一个隐藏层和256个隐藏单元的MLP。注意，我们可以把这两个量都看作超参数。通常，我们选择2的幂次的层宽度，由于在硬件中如何分配和寻址内存，这往往是计算效率高的。\\n+\\n+同样，我们将用几个张量来表示我们的参数。注意，对于每一层*，我们必须跟踪一个权重矩阵和一个偏差向量。一如既往，我们为这些参数的损耗梯度分配内存。\\n+\\n+```{.python .input}\\n+num_inputs, num_outputs, num_hiddens = 784, 10, 256\\n+\\n+W1 = np.random.normal(scale=0.01, size=(num_inputs, num_hiddens))\\n+b1 = np.zeros(num_hiddens)\\n+W2 = np.random.normal(scale=0.01, size=(num_hiddens, num_outputs))\\n+b2 = np.zeros(num_outputs)\\n+params = [W1, b1, W2, b2]\\n+\\n+for param in params:\\n+    param.attach_grad()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+num_inputs, num_outputs, num_hiddens = 784, 10, 256\\n+\\n+W1 = nn.Parameter(torch.randn(\\n+    num_inputs, num_hiddens, requires_grad=True) * 0.01)\\n+b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\\n+W2 = nn.Parameter(torch.randn(\\n+    num_hiddens, num_outputs, requires_grad=True) * 0.01)\\n+b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\\n+\\n+params = [W1, b1, W2, b2]\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+num_inputs, num_outputs, num_hiddens = 784, 10, 256\\n+\\n+W1 = tf.Variable(tf.random.normal(\\n+    shape=(num_inputs, num_hiddens), mean=0, stddev=0.01))\\n+b1 = tf.Variable(tf.zeros(num_hiddens))\\n+W2 = tf.Variable(tf.random.normal(\\n+    shape=(num_hiddens, num_outputs), mean=0, stddev=0.01))\\n+b2 = tf.Variable(tf.random.normal([num_outputs], stddev=.01))\\n+\\n+params = [W1, b1, W2, b2]\\n+```\\n+\\n+## 激活函数\\n+\\n+为了确保我们知道所有的工作原理，我们将使用maximum函数自己实现ReLU激活，而不是直接调用内置的`relu`函数。\\n+\\n+```{.python .input}\\n+def relu(X):\\n+    return np.maximum(X, 0)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def relu(X):\\n+    a = torch.zeros_like(X)\\n+    return torch.max(X, a)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def relu(X):\\n+    return tf.math.maximum(X, 0)\\n+```\\n+\\n+## 模型\\n+\\n+因为我们忽略了空间结构，我们将`reshape`每个二维图像转换成一个长度为`num_inputs`的平面向量。最后，我们只需要几行代码来实现我们的模型。\\n+\\n+```{.python .input}\\n+def net(X):\\n+    X = d2l.reshape(X, (-1, num_inputs))\\n+    H = relu(np.dot(X, W1) + b1)\\n+    return np.dot(H, W2) + b2\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def net(X):\\n+    X = d2l.reshape(X, (-1, num_inputs))\\n+    H = relu(X@W1 + b1)  # Here \\'@\\' stands for matrix multiplication\\n+    return (H@W2 + b2)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def net(X):\\n+    X = d2l.reshape(X, (-1, num_inputs))\\n+    H = relu(tf.matmul(X, W1) + b1)\\n+    return tf.matmul(H, W2) + b2\\n+```\\n+\\n+## 损失函数\\n+\\n+为了确保数值的稳定性，并且因为我们已经从头开始实现了softmax函数（:numref:`sec_softmax_scratch`），我们利用高级api中的集成函数来计算softmax和交叉熵损失。回想一下我们在:numref:`subsec_softmax-implementation-revisited`中对这些错综复杂的讨论。我们鼓励感兴趣的读者检查loss函数的源代码，以加深对实现细节的了解。\\n+\\n+```{.python .input}\\n+loss = gluon.loss.SoftmaxCrossEntropyLoss()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+loss = nn.CrossEntropyLoss()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def loss(y_hat, y):\\n+    return tf.losses.sparse_categorical_crossentropy(\\n+        y, y_hat, from_logits=True)\\n+```\\n+\\n+## 培训\\n+\\n+幸运的是，mlp的训练循环与softmax回归的训练循环完全相同。再次利用`d2l`包，我们调用`train_ch3`函数（参见:numref:`sec_softmax_scratch`），将时代数设置为10，学习率设置为0.1。\\n+\\n+```{.python .input}\\n+num_epochs, lr = 10, 0.1\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs,\\n+              lambda batch_size: d2l.sgd(params, lr, batch_size))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+num_epochs, lr = 10, 0.1\\n+updater = torch.optim.SGD(params, lr=lr)\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+num_epochs, lr = 10, 0.1\\n+updater = d2l.Updater([W1, W2, b1, b2], lr)\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\\n+```\\n+\\n+为了评估所学习的模型，我们将其应用于一些测试数据。\\n+\\n+```{.python .input}\\n+#@tab all\\n+d2l.predict_ch3(net, test_iter)\\n+```\\n+\\n+## 摘要\\n+\\n+* 我们看到实现一个简单的MLP是很容易的，即使是手动完成的。\\n+* 然而，对于大量的层，从头开始实现mlp仍然会很麻烦（例如，命名和跟踪模型的参数）。\\n+\\n+## 练习\\n+\\n+1. 更改超参数`num_hiddens`的值，并查看此超参数如何影响结果。确定此超参数的最佳值，并保持所有其他值不变。\\n+1. 尝试添加一个额外的隐藏层，看看它如何影响结果。\\n+1. 改变学习率如何改变你的成绩？修正模型结构和其他超参数（包括时代数），什么样的学习率能给你最好的结果？\\n+1. 通过对所有超参数（学习率、时代数、隐藏层数、每层隐藏单元数）的联合优化，你能得到的最佳结果是什么？\\n+1. 描述为什么处理多个超参数更具挑战性。\\n+1. 你能想到的在多个超参数上构建搜索的最聪明的策略是什么？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/92)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/93)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/227)\\n+:end_tab:\\ndiff --git a/chapter_multilayer-perceptrons/mlp-scratch_tencent.md b/chapter_multilayer-perceptrons/mlp-scratch_tencent.md\\nnew file mode 100644\\nindex 00000000..28767d16\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/mlp-scratch_tencent.md\\n@@ -0,0 +1,202 @@\\n+# 多层感知器的从头开始实现\\n+:label:`sec_mlp_scratch`\\n+\\n+既然我们已经从数学上描述了多层感知器(MLP)的特征，让我们尝试自己实现一个多层感知器(MLP)。为了与我们之前使用Softmax回归(:numref:`sec_softmax_scratch`)获得的结果进行比较，我们将继续使用Fashion-MNIST图像分类数据集(:numref:`sec_fashion_mnist`)。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import gluon, np, npx\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+```\\n+\\n+```{.python .input}\\n+#@tab all\\n+batch_size = 256\\n+train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\\n+```\\n+\\n+## 正在初始化模型参数\\n+\\n+回想一下，Fashion-MNIST包含10个类，每个图像由$28 \\\\times 28 = 784$个灰度像素值网格组成。同样，我们现在将忽略像素之间的空间结构，因此我们可以将其视为具有784个输入特征和10个类的简单分类数据集。首先，我们将实现一个具有一个隐藏层和256个隐藏单元的MLP。请注意，我们可以将这两个量都视为超参数。通常，我们选择2的幂的层宽度，由于内存在硬件中的分配和寻址方式，这往往在计算上是高效的。\\n+\\n+同样，我们将用几个张量来表示我们的参数。请注意，*对于每一层*，我们必须跟踪一个权重矩阵和一个偏差向量。一如既往，我们为相对于这些参数的损耗梯度分配内存。\\n+\\n+```{.python .input}\\n+num_inputs, num_outputs, num_hiddens = 784, 10, 256\\n+\\n+W1 = np.random.normal(scale=0.01, size=(num_inputs, num_hiddens))\\n+b1 = np.zeros(num_hiddens)\\n+W2 = np.random.normal(scale=0.01, size=(num_hiddens, num_outputs))\\n+b2 = np.zeros(num_outputs)\\n+params = [W1, b1, W2, b2]\\n+\\n+for param in params:\\n+    param.attach_grad()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+num_inputs, num_outputs, num_hiddens = 784, 10, 256\\n+\\n+W1 = nn.Parameter(torch.randn(\\n+    num_inputs, num_hiddens, requires_grad=True) * 0.01)\\n+b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\\n+W2 = nn.Parameter(torch.randn(\\n+    num_hiddens, num_outputs, requires_grad=True) * 0.01)\\n+b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\\n+\\n+params = [W1, b1, W2, b2]\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+num_inputs, num_outputs, num_hiddens = 784, 10, 256\\n+\\n+W1 = tf.Variable(tf.random.normal(\\n+    shape=(num_inputs, num_hiddens), mean=0, stddev=0.01))\\n+b1 = tf.Variable(tf.zeros(num_hiddens))\\n+W2 = tf.Variable(tf.random.normal(\\n+    shape=(num_hiddens, num_outputs), mean=0, stddev=0.01))\\n+b2 = tf.Variable(tf.random.normal([num_outputs], stddev=.01))\\n+\\n+params = [W1, b1, W2, b2]\\n+```\\n+\\n+## 激活函数\\n+\\n+为了确保我们知道一切是如何工作的，我们将使用Maximum函数自己实现RELU激活，而不是直接调用内置的`relu`函数。\\n+\\n+```{.python .input}\\n+def relu(X):\\n+    return np.maximum(X, 0)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def relu(X):\\n+    a = torch.zeros_like(X)\\n+    return torch.max(X, a)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def relu(X):\\n+    return tf.math.maximum(X, 0)\\n+```\\n+\\n+## 型号\\n+\\n+因为我们忽略了空间结构，所以我们将每个二维图像`reshape`成一个长度为`num_inputs`的平面矢量。最后，我们只需几行代码就可以实现我们的模型。\\n+\\n+```{.python .input}\\n+def net(X):\\n+    X = d2l.reshape(X, (-1, num_inputs))\\n+    H = relu(np.dot(X, W1) + b1)\\n+    return np.dot(H, W2) + b2\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def net(X):\\n+    X = d2l.reshape(X, (-1, num_inputs))\\n+    H = relu(X@W1 + b1)  # Here \\'@\\' stands for matrix multiplication\\n+    return (H@W2 + b2)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def net(X):\\n+    X = d2l.reshape(X, (-1, num_inputs))\\n+    H = relu(tf.matmul(X, W1) + b1)\\n+    return tf.matmul(H, W2) + b2\\n+```\\n+\\n+## 损失函数\\n+\\n+为了确保数值稳定性，并且由于我们已经从头开始实施了Softmax函数(:numref:`sec_softmax_scratch`)，因此我们利用来自高级API的集成函数来计算Softmax和交叉熵损失。回想一下我们早些时候在:numref:`subsec_softmax-implementation-revisited`中对这些错综复杂的问题的讨论。我们鼓励感兴趣的读者查看Loss函数的源代码，以加深对实现细节的了解。\\n+\\n+```{.python .input}\\n+loss = gluon.loss.SoftmaxCrossEntropyLoss()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+loss = nn.CrossEntropyLoss()\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def loss(y_hat, y):\\n+    return tf.losses.sparse_categorical_crossentropy(\\n+        y, y_hat, from_logits=True)\\n+```\\n+\\n+## 培训\\n+\\n+幸运的是，MLP的训练循环与Softmax回归的训练循环完全相同。再次利用`d2l`包，我们调用`train_ch3`函数(参见:numref:`sec_softmax_scratch`)，将纪元数设置为10，并将学习率设置为0.1.\\n+\\n+```{.python .input}\\n+num_epochs, lr = 10, 0.1\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs,\\n+              lambda batch_size: d2l.sgd(params, lr, batch_size))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+num_epochs, lr = 10, 0.1\\n+updater = torch.optim.SGD(params, lr=lr)\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+num_epochs, lr = 10, 0.1\\n+updater = d2l.Updater([W1, W2, b1, b2], lr)\\n+d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\\n+```\\n+\\n+为了对学习到的模型进行评估，我们将其应用于一些测试数据。\\n+\\n+```{.python .input}\\n+#@tab all\\n+d2l.predict_ch3(net, test_iter)\\n+```\\n+\\n+## 摘要\\n+\\n+* 我们看到实现一个简单的MLP是很容易的，即使手动完成也是如此。\\n+* 然而，对于大量的层，从头开始实现MLP仍然会变得杂乱无章(例如，命名和跟踪我们模型的参数)。\\n+\\n+## 练习\\n+\\n+1. 更改超参数`num_hiddens`的值，并查看此超参数对结果有何影响。确定此超参数的最佳值，使所有其他参数保持不变。\\n+1. 尝试添加额外的隐藏层以查看它对结果有何影响。\\n+1. 改变学习速度会如何改变你的成绩？固定模型架构和其他超参数(包括纪元数)，多大的学习率会给您带来最好的结果？\\n+1. 通过对所有超参数(学习率、历元数、隐藏层数、每层隐藏单元数)进行联合优化，可以得到什么最佳结果？\\n+1. 描述为什么处理多个超参数更具挑战性。\\n+1. 对于在多个超参数上构建搜索，您能想到的最聪明的策略是什么？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/92)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/93)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/227)\\n+:end_tab:\\ndiff --git a/chapter_multilayer-perceptrons/mlp_baidu.md b/chapter_multilayer-perceptrons/mlp_baidu.md\\nnew file mode 100644\\nindex 00000000..06697161\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/mlp_baidu.md\\n@@ -0,0 +1,287 @@\\n+# 多层网络\\n+:label:`sec_mlp`\\n+\\n+在:numref:`chap_linear`中，我们引入了softmax回归（:numref:`sec_softmax`），实现了从头开始的算法（:numref:`sec_softmax_scratch`）和使用高级api（:numref:`sec_softmax_concise`），并训练分类器从低分辨率图像中识别出10类服装。在此过程中，我们学习了如何对数据进行纠结，将输出强制为有效的概率分布，应用适当的损失函数，并根据模型参数最小化它。既然我们已经在简单线性模型的背景下掌握了这些机制，我们就可以开始探索深层神经网络，这是本书主要关注的一类相对丰富的模型。\\n+\\n+## 隐藏层\\n+\\n+我们已经在:numref:`subsec_linear_model`中描述了仿射变换，它是一个带有偏差的线性变换。首先，回顾一下与我们的softmax回归示例相对应的模型体系结构，如:numref:`fig_softmaxreg`所示。这个模型通过一个单一的仿射变换直接将我们的输入映射到我们的输出，然后是一个softmax操作。如果我们的标签确实通过仿射变换与我们的输入数据相关，那么这种方法就足够了。但是仿射变换中的线性是一个很强的假设。\\n+\\n+### 线性模型可能会出错\\n+\\n+例如，线性意味着*较弱*单调性*假设：我们特征的任何增加必须总是导致模型输出的增加（如果相应的权重是正的），或者总是导致模型输出的减少（如果相应的权重是负的）。有时候这是有道理的。例如，如果我们试图预测一个人是否会偿还贷款，我们可以合理地设想，在其他条件不变的情况下，收入较高的申请人总是比收入较低的申请人更有可能偿还贷款。虽然是单调的，但这种关系可能与偿还概率没有线性关系。收入从0万增加到5万，与从100万增加到105万相比，还款可能性的增加可能更大。处理这一问题的一种方法可能是对我们的数据进行预处理，使线性变得更合理，比如说，使用收入对数作为我们的特征。\\n+\\n+请注意，我们可以很容易地想出违反单调性的例子。比如说，我们要根据体温来预测死亡概率。对于体温高于37°C（98.6°F）的人来说，温度越高风险越大。但是，对于体温低于37°C的人来说，温度越高风险越低！在这种情况下，我们也可以通过一些巧妙的预处理来解决这个问题。也就是说，我们可以使用37°C的距离作为我们的特征。\\n+\\n+但是，如何对猫和狗的图像进行分类呢？在位置（13，17）增加像素的强度是否应该总是增加（或总是降低）图像描绘狗的可能性？对线性模型的依赖对应于一个隐含的假设：区分猫和狗的唯一要求是评估单个像素的亮度。这种方法注定要失败，在一个反转图像保留类别的世界中。\\n+\\n+然而，尽管与我们之前的例子相比，这里的线性显然是荒谬的，但是我们可以用一个简单的预处理修复来解决这个问题就不那么明显了。这是因为任何像素的重要性都以复杂的方式依赖于其上下文（周围像素的值）。虽然我们的数据可能存在一种表示形式，它将考虑到我们的特征之间的相关交互作用，在此基础上建立一个线性模型是合适的，但我们根本不知道如何手工计算它。对于深度神经网络，我们使用观测数据来共同学习通过隐藏层的表示和作用于该表示的线性预测器。\\n+\\n+### 合并隐藏层\\n+\\n+我们可以克服线性模型的这些局限性，通过合并一个或多个隐藏层来处理更一般的函数类。最简单的方法是将许多完全连接的层堆叠在一起。每一层输入到它上面的层，直到我们产生输出。我们可以把前$L-1$层看作我们的表示，最后一层作为我们的线性预测器。这种结构通常称为*多层感知器*，通常缩写为*MLP*。下面，我们用图表描述了一个MLP（:numref:`fig_mlp`）。\\n+\\n+![An MLP with a hidden layer of 5 hidden units. ](../img/mlp.svg)\\n+:label:`fig_mlp`\\n+\\n+该MLP有4个输入，3个输出，其隐藏层包含5个隐藏单元。由于输入层不涉及任何计算，使用该网络生成输出需要同时实现隐藏层和输出层的计算；因此，该MLP中的层数为2。请注意，这些层都是完全连接的。每一个输入都会影响隐藏层中的每一个神经元，而每一个输入又会影响输出层中的每一个神经元。\\n+\\n+### 从线性到非线性\\n+\\n+如前所述，通过矩阵$\\\\mathbf{X} \\\\in \\\\mathbb{R}^{n \\\\times d}$，我们表示$n$的小批量示例，其中每个示例有$d$个输入（特征）。对于隐藏层具有$h$个隐藏单元的一个隐藏层MLP，用$\\\\mathbf{H} \\\\in \\\\mathbb{R}^{n \\\\times h}$表示隐藏层的输出，这些输出是\\n+*隐藏表示*。\\n+在数学或代码中，$\\\\mathbf{H}$也被称为*隐藏层变量*或*隐藏变量*。由于隐藏层和输出层都是完全连接的，我们有隐藏层权重$\\\\mathbf{W}^{(1)} \\\\in \\\\mathbb{R}^{d \\\\times h}$和偏移$\\\\mathbf{b}^{(1)} \\\\in \\\\mathbb{R}^{1 \\\\times h}$，输出层权重$\\\\mathbf{W}^{(2)} \\\\in \\\\mathbb{R}^{h \\\\times q}$和偏移$\\\\mathbf{b}^{(2)} \\\\in \\\\mathbb{R}^{1 \\\\times q}$。形式上，我们计算一个隐藏层MLP的输出$\\\\mathbf{O} \\\\in \\\\mathbb{R}^{n \\\\times q}$如下：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+    \\\\mathbf{H} & = \\\\mathbf{X} \\\\mathbf{W}^{(1)} + \\\\mathbf{b}^{(1)}, \\\\\\\\\\n+    \\\\mathbf{O} & = \\\\mathbf{H}\\\\mathbf{W}^{(2)} + \\\\mathbf{b}^{(2)}.\\n+\\\\end{aligned}\\n+$$\\n+\\n+请注意，在添加隐藏层之后，我们的模型现在要求我们跟踪和更新其他参数集。那么我们从中得到了什么呢？你可能会惊讶地发现——在上面定义的模型中——*我们从麻烦中得不到任何好处*！原因很简单。上面的隐藏单元由输入的仿射函数给出，而输出（pre softmax）只是隐藏单元的仿射函数。仿射函数的仿射函数本身就是仿射函数。此外，我们的线性模型已经能够表示任何仿射函数。\\n+\\n+我们可以通过证明，对于任何权重值，我们都可以折叠隐藏层，得到一个参数为$\\\\mathbf{W} = \\\\mathbf{W}^{(1)}\\\\mathbf{W}^{(2)}$和$\\\\mathbf{b} = \\\\mathbf{b}^{(1)} \\\\mathbf{W}^{(2)} + \\\\mathbf{b}^{(2)}$的等效单层模型：\\n+\\n+$$\\n+\\\\mathbf{O} = (\\\\mathbf{X} \\\\mathbf{W}^{(1)} + \\\\mathbf{b}^{(1)})\\\\mathbf{W}^{(2)} + \\\\mathbf{b}^{(2)} = \\\\mathbf{X} \\\\mathbf{W}^{(1)}\\\\mathbf{W}^{(2)} + \\\\mathbf{b}^{(1)} \\\\mathbf{W}^{(2)} + \\\\mathbf{b}^{(2)} = \\\\mathbf{X} \\\\mathbf{W} + \\\\mathbf{b}.\\n+$$\\n+\\n+为了实现多层体系结构的潜力，我们还需要一个更关键的因素：在仿射变换之后，将一个非线性*激活函数*$\\\\sigma$应用于每个隐藏单元。激活函数（例如$\\\\sigma(\\\\cdot)$）的输出称为*激活*。一般来说，在激活函数到位的情况下，不再可能将MLP压缩为线性模型：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+    \\\\mathbf{H} & = \\\\sigma(\\\\mathbf{X} \\\\mathbf{W}^{(1)} + \\\\mathbf{b}^{(1)}), \\\\\\\\\\n+    \\\\mathbf{O} & = \\\\mathbf{H}\\\\mathbf{W}^{(2)} + \\\\mathbf{b}^{(2)}.\\\\\\\\\\n+\\\\end{aligned}\\n+$$\\n+\\n+由于$\\\\mathbf{X}$中的每一行都对应于minibatch中的一个示例，因此我们定义了非线性$\\\\sigma$以行方式应用于其输入，即一次一个示例。注意，在:numref:`subsec_softmax_vectorization`中，我们以同样的方式使用softmax的符号来表示行操作。通常，如本节所述，我们应用于隐藏层的激活函数不仅仅是行方式的，而是元素方面的。这意味着在计算了层的线性部分之后，我们可以计算每个激活，而不必查看其他隐藏单元的值。大多数激活函数都是这样。\\n+\\n+为了构建更一般的mlp，我们可以继续堆叠这样的隐藏层，例如$\\\\mathbf{H}^{(1)} = \\\\sigma_1(\\\\mathbf{X} \\\\mathbf{W}^{(1)} + \\\\mathbf{b}^{(1)})$和$\\\\mathbf{H}^{(2)} = \\\\sigma_2(\\\\mathbf{H}^{(1)} \\\\mathbf{W}^{(2)} + \\\\mathbf{b}^{(2)})$，一个接一个，产生更具表现力的模型。\\n+\\n+### 通用逼近器\\n+\\n+mlp可以通过它们的隐藏神经元捕捉我们输入之间的复杂交互作用，这些神经元依赖于每个输入的值。我们可以很容易地设计隐藏节点来执行任意计算，例如，对一对输入进行基本逻辑运算。此外，对于激活函数的某些选择，众所周知，mlp是通用逼近器。即使只有一个隐藏层网络，给定足够多的节点（可能有荒谬的多）和正确的权重集，我们也可以为任何函数建模，尽管实际上学习该函数是困难的部分。你可能会认为你的神经网络有点像C编程语言。这种语言和其他现代语言一样，能够表达任何可计算程序。但实际上，制定出一个符合你的规范的程序是很困难的。\\n+\\n+而且，仅仅因为一个单一的隐藏层网络\\n+*可以学习任何函数\\n+但这并不意味着你应该用一个隐藏层网络来解决所有的问题。事实上，我们可以通过使用更深（而不是更广）的网络来更紧凑地近似许多函数。我们将在后面的章节中讨论更严格的论点。\\n+\\n+```{.python .input}\\n+%matplotlib inline\\n+from d2l import mxnet as d2l\\n+from mxnet import autograd, np, npx\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+%matplotlib inline\\n+from d2l import torch as d2l\\n+import torch\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+%matplotlib inline\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+```\\n+\\n+## 激活函数\\n+\\n+激活函数通过计算加权和并进一步加上偏差来决定神经元是否应该被激活。它们是将输入信号转换为输出信号的可微算子，但大多数算子都加入了非线性。因为激活函数是深度学习的基础，让我们简单介绍一些常见的激活函数。\\n+\\n+### ReLU函数\\n+\\n+由于实现的简单性和它在各种预测任务上的良好性能，最流行的选择是*校正线性单元*（*ReLU*）。ReLU提供了一个非常简单的非线性变换。给定一个元素$x$，函数定义为该元素和$0$的最大值：\\n+\\n+$$\\\\operatorname{ReLU}(x) = \\\\max(x, 0).$$\\n+\\n+非正式地说，ReLU函数只保留正元素，并通过将相应的激活设置为0来丢弃所有负元素。为了获得一些直觉，我们可以画出函数。如你所见，激活函数是分段线性的。\\n+\\n+```{.python .input}\\n+x = np.arange(-8.0, 8.0, 0.1)\\n+x.attach_grad()\\n+with autograd.record():\\n+    y = npx.relu(x)\\n+d2l.plot(x, y, \\'x\\', \\'relu(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\\n+y = torch.relu(x)\\n+d2l.plot(x.detach(), y.detach(), \\'x\\', \\'relu(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+x = tf.Variable(tf.range(-8.0, 8.0, 0.1), dtype=tf.float32)\\n+y = tf.nn.relu(x)\\n+d2l.plot(x.numpy(), y.numpy(), \\'x\\', \\'relu(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+当输入为负时，ReLU函数的导数为0；当输入为正时，ReLU函数的导数为1。注意，当输入值精确等于0时，ReLU函数是不可微的。在这些情况下，我们默认使用左边的导数，当输入为0时，我们假设导数为0。我们可以摆脱这种情况，因为输入可能永远不会是零。有句老话说，如果微妙的边界条件很重要，我们可能在做数学，而不是工程。传统的智慧在这里可能适用。我们绘制了下面绘制的ReLU函数的导数。\\n+\\n+```{.python .input}\\n+y.backward()\\n+d2l.plot(x, x.grad, \\'x\\', \\'grad of relu\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+y.backward(torch.ones_like(x), retain_graph=True)\\n+d2l.plot(x.detach(), x.grad, \\'x\\', \\'grad of relu\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+with tf.GradientTape() as t:\\n+    y = tf.nn.relu(x)\\n+d2l.plot(x.numpy(), t.gradient(y, x).numpy(), \\'x\\', \\'grad of relu\\',\\n+         figsize=(5, 2.5))\\n+```\\n+\\n+使用ReLU的原因是它的导数表现得特别好：要么消失，要么就让争论通过。这使得优化表现得更好，并且它减轻了困扰以前版本的神经网络的梯度消失的问题（稍后将详细介绍）。\\n+\\n+请注意，ReLU函数有许多变体，包括*参数化ReLU*（*pReLU*）函数:cite:`He.Zhang.Ren.ea.2015`。这种变化为ReLU添加了一个线性项，因此即使参数为负数，某些信息仍然可以通过：\\n+\\n+$$\\\\operatorname{pReLU}(x) = \\\\max(0, x) + \\\\alpha \\\\min(0, x).$$\\n+\\n+### 乙状窦函数\\n+\\n+*sigmoid函数*将其输入（其值位于域$\\\\mathbb{R}$）转换为间隔（0，1）上的输出。因此，sigmoid通常被称为*压缩函数*：它将范围（-inf，inf）中的任何输入压缩为范围（0，1）中的某个值：\\n+\\n+$$\\\\operatorname{sigmoid}(x) = \\\\frac{1}{1 + \\\\exp(-x)}.$$\\n+\\n+在最早的神经网络中，科学家们对模拟生物神经元很感兴趣，这些神经元要么发射，要么不发射。因此，这一领域的先驱者，一直追溯到人工神经元的发明者McCulloch和Pitts，把重点放在阈值单元上。阈值激活在输入低于某个阈值时取值0，当输入超过阈值时取值1。\\n+\\n+当注意力转向基于梯度的学习时，sigmoid函数是一个自然的选择，因为它是一个平滑的，可微的阈值单元近似。当我们想将输出解释为二进制分类问题的概率时，sigmoid仍然被广泛用作输出单元的激活函数（您可以将sigmoid看作softmax的一个特例）。然而，sigmoid大部分已经被更简单和更容易训练的ReLU所取代，大多数用于隐藏层。在后面关于递归神经网络的章节中，我们将描述利用sigmoid单元来控制跨时间的信息流的体系结构。\\n+\\n+下面，我们绘制sigmoid函数。注意，当输入接近0时，sigmoid函数接近线性变换。\\n+\\n+```{.python .input}\\n+with autograd.record():\\n+    y = npx.sigmoid(x)\\n+d2l.plot(x, y, \\'x\\', \\'sigmoid(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+y = torch.sigmoid(x)\\n+d2l.plot(x.detach(), y.detach(), \\'x\\', \\'sigmoid(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+y = tf.nn.sigmoid(x)\\n+d2l.plot(x.numpy(), y.numpy(), \\'x\\', \\'sigmoid(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+sigmoid函数的导数由以下方程给出：\\n+\\n+$$\\\\frac{d}{dx} \\\\operatorname{sigmoid}(x) = \\\\frac{\\\\exp(-x)}{(1 + \\\\exp(-x))^2} = \\\\operatorname{sigmoid}(x)\\\\left(1-\\\\operatorname{sigmoid}(x)\\\\right).$$\\n+\\n+sigmoid函数的导数如下所示。注意，当输入为0时，sigmoid函数的导数达到最大值0.25。当输入从0向任一方向发散时，导数接近0。\\n+\\n+```{.python .input}\\n+y.backward()\\n+d2l.plot(x, x.grad, \\'x\\', \\'grad of sigmoid\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+# Clear out previous gradients\\n+x.grad.data.zero_()\\n+y.backward(torch.ones_like(x),retain_graph=True)\\n+d2l.plot(x.detach(), x.grad, \\'x\\', \\'grad of sigmoid\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+with tf.GradientTape() as t:\\n+    y = tf.nn.sigmoid(x)\\n+d2l.plot(x.numpy(), t.gradient(y, x).numpy(), \\'x\\', \\'grad of sigmoid\\',\\n+         figsize=(5, 2.5))\\n+```\\n+\\n+### Tanh函数\\n+\\n+与sigmoid函数一样，tanh（双曲正切）函数也会压缩其输入，将其转换为-1和1之间间隔上的元素：\\n+\\n+$$\\\\operatorname{tanh}(x) = \\\\frac{1 - \\\\exp(-2x)}{1 + \\\\exp(-2x)}.$$\\n+\\n+我们在下面绘制tanh函数。注意，当输入接近0时，tanh函数接近线性变换。尽管函数的形状类似于sigmoid函数，tanh函数在坐标系原点处表现出点对称性。\\n+\\n+```{.python .input}\\n+with autograd.record():\\n+    y = np.tanh(x)\\n+d2l.plot(x, y, \\'x\\', \\'tanh(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+y = torch.tanh(x)\\n+d2l.plot(x.detach(), y.detach(), \\'x\\', \\'tanh(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+y = tf.nn.tanh(x)\\n+d2l.plot(x.numpy(), y.numpy(), \\'x\\', \\'tanh(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+tanh函数的导数为：\\n+\\n+$$\\\\frac{d}{dx} \\\\operatorname{tanh}(x) = 1 - \\\\operatorname{tanh}^2(x).$$\\n+\\n+tanh函数的导数绘制如下。当输入接近0时，tanh函数的导数接近最大值1。正如我们在sigmoid函数中看到的，当输入从0向任意方向移动时，tanh函数的导数接近0。\\n+\\n+```{.python .input}\\n+y.backward()\\n+d2l.plot(x, x.grad, \\'x\\', \\'grad of tanh\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+# Clear out previous gradients.\\n+x.grad.data.zero_()\\n+y.backward(torch.ones_like(x),retain_graph=True)\\n+d2l.plot(x.detach(), x.grad, \\'x\\', \\'grad of tanh\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+with tf.GradientTape() as t:\\n+    y = tf.nn.tanh(x)\\n+d2l.plot(x.numpy(), t.gradient(y, x).numpy(), \\'x\\', \\'grad of tanh\\',\\n+         figsize=(5, 2.5))\\n+```\\n+\\n+总之，我们现在知道了如何结合非线性来构建具有表现力的多层神经网络结构。顺便说一句，你的知识已经让你在1990年左右掌握了一个类似于从业者的工具箱。在某些方面，您比90年代的任何人都有优势，因为您可以利用强大的开源深度学习框架快速构建模型，只需几行代码。以前，训练这些网络需要研究人员编写数千行C和Fortran代码。\\n+\\n+## 摘要\\n+\\n+* MLP在输出层和输入层之间添加一个或多个完全连接的隐藏层，并通过激活函数转换隐藏层的输出。\\n+* 常用的激活函数包括ReLU函数、sigmoid函数和tanh函数。\\n+\\n+## 练习\\n+\\n+1. 计算pReLU激活函数的导数。\\n+1. 证明了只使用ReLU（或pReLU）的MLP构造了一个连续的分段线性函数。\\n+1. 显示$\\\\operatorname{tanh}(x) + 1 = 2 \\\\operatorname{sigmoid}(2x)$。\\n+1. 假设我们有一个非线性，一次适用于一个小批量。你认为这会导致什么样的问题？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/90)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/91)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/226)\\n+:end_tab:\\ndiff --git a/chapter_multilayer-perceptrons/mlp_tencent.md b/chapter_multilayer-perceptrons/mlp_tencent.md\\nnew file mode 100644\\nindex 00000000..f2e386a2\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/mlp_tencent.md\\n@@ -0,0 +1,287 @@\\n+# 多层感知器\\n+:label:`sec_mlp`\\n+\\n+在:numref:`chap_linear`，我们引入了Softmax回归(:numref:`sec_softmax`)，从头开始实现算法(:numref:`sec_softmax_scratch`)，使用高级API(:numref:`sec_softmax_concise`)，并训练分类器从低分辨率图像中识别10类服装。在此过程中，我们学习了如何处理数据，将输出强制为有效的概率分布，应用适当的损失函数，并根据模型参数将其最小化。既然我们已经在简单的线性模型的背景下掌握了这些力学，我们就可以开始探索深度神经网络，这是本书主要涉及的相对丰富的模型类别。\\n+\\n+## 隐藏层\\n+\\n+我们在:numref:`subsec_linear_model`中描述了仿射变换，它是一个加了偏差的线性变换。首先，回想一下与我们的Softmax回归示例相对应的模型体系结构，如:numref:`fig_softmaxreg`中所示。该模型通过单个仿射变换将我们的输入直接映射到我们的输出，然后进行Softmax操作。如果我们的标签确实通过仿射变换与我们的输入数据相关，那么这种方法就足够了。但是，仿射变换中的线性是一个“强有力的”假设。\\n+\\n+### 线性模型可能会出错\\n+\\n+例如，线性意味着*单调性*的“较弱”假设：特征的任何增加都必须总是导致模型输出的增加(如果对应的权重为正)，或者总是导致模型的输出减少(如果对应的权重为负)。有时候这是有道理的。例如，如果我们试图预测一个人是否会偿还贷款，我们可能会合理地认为，在其他条件不变的情况下，收入较高的申请人总是比收入较低的申请人更有可能偿还贷款。虽然单调，但这种关系可能与还款概率不是线性相关的。收入从0增加到5万，可能比从100万增加到105万更有可能带来更大的还款可能性。处理这一问题的一种方法可能是对我们的数据进行预处理，使线性变得更可信，比如说，通过使用收入的对数作为我们的特征。\\n+\\n+请注意，我们可以很容易地找出违反单调性的示例。例如，我们想要根据体温预测死亡概率。对于体温高于37摄氏度(98.6华氏度)的个人来说，温度越高风险越大。然而，对于体温低于37摄氏度的人来说，温度越高，风险就越低！在这种情况下，我们也可以通过一些巧妙的预处理来解决问题。也就是说，我们可以使用37°C的距离作为我们的特征。\\n+\\n+但是，如何对猫和狗的图像进行分类呢？增加位置(13，17)处像素的强度是否总是增加(或总是降低)图像描绘狗的可能性？对线性模型的依赖对应于一个隐含的假设，即区分猫和狗的唯一要求是评估单个像素的亮度。在一个倒置图像保留类别的世界里，这种方法注定要失败。\\n+\\n+然而，尽管这里的线性显然是荒谬的，与我们前面的示例相比，我们可以通过简单的预处理修复来解决这个问题就不那么明显了。这是因为任何像素的重要性都以复杂的方式取决于其上下文(周围像素的值)。虽然我们的数据可能会有一种表示，它会考虑到我们的特征之间的相关交互作用，在此基础上建立一个线性模型将是合适的，但我们只是不知道如何手动计算它。对于深度神经网络，我们使用观测数据来联合学习通过隐藏层的表示和作用于该表示的线性预测器。\\n+\\n+### 合并隐藏图层\\n+\\n+我们可以克服线性模型的这些限制，通过合并一个或多个隐藏层来处理更一般的函数类。要做到这一点，最简单的方法是将许多完全连接的层堆叠在一起。每一层都提供给它上面的层，直到我们生成输出。我们可以把前$L-1$层看作我们的表示，把最后一层看作我们的线性预测器。这种架构通常称为“多层感知器”，通常缩写为*MLP*。下面，我们以图表方式描述了一个mlp(:numref:`fig_mlp`)。\\n+\\n+![An MLP with a hidden layer of 5 hidden units. ](../img/mlp.svg)\\n+:label:`fig_mlp`\\n+\\n+该MLP有4个输入，3个输出，其隐藏层包含5个隐藏单元。由于输入层不涉及任何计算，因此使用此网络产生输出需要同时实现隐藏层和输出层的计算；因此，此MLP中的层数为2。请注意，这两个层都是完全连接的。每一次输入都会影响隐层中的每一个神经元，而每一次输入又会影响输出层中的每一个神经元。\\n+\\n+### 从线性到非线性\\n+\\n+如前所述，通过矩阵$\\\\mathbf{X} \\\\in \\\\mathbb{R}^{n \\\\times d}$，我们表示$n$个示例的小批量，其中每个示例具有$d$个输入(特征)。对于其隐藏层具有$h$个隐藏单元的单隐藏层mlp，用$\\\\mathbf{H} \\\\in \\\\mathbb{R}^{n \\\\times h}$表示隐藏层的输出，它们是\\n+*隐藏表示*。\\n+在数学或代码中，$\\\\mathbf{H}$也称为“隐藏层变量”或“隐藏变量”。因为隐藏层和输出层都是完全连接的，所以我们具有隐藏层权重$\\\\mathbf{W}^{(1)} \\\\in \\\\mathbb{R}^{d \\\\times h}$和偏置$\\\\mathbf{b}^{(1)} \\\\in \\\\mathbb{R}^{1 \\\\times h}$以及输出层权重$\\\\mathbf{W}^{(2)} \\\\in \\\\mathbb{R}^{h \\\\times q}$和偏置$\\\\mathbf{b}^{(2)} \\\\in \\\\mathbb{R}^{1 \\\\times q}$。形式上，我们按如下方式计算单隐层最大似然比的输出$\\\\mathbf{O} \\\\in \\\\mathbb{R}^{n \\\\times q}$：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+    \\\\mathbf{H} & = \\\\mathbf{X} \\\\mathbf{W}^{(1)} + \\\\mathbf{b}^{(1)}, \\\\\\\\\\n+    \\\\mathbf{O} & = \\\\mathbf{H}\\\\mathbf{W}^{(2)} + \\\\mathbf{b}^{(2)}.\\n+\\\\end{aligned}\\n+$$\\n+\\n+请注意，在添加隐藏层之后，我们的模型现在需要跟踪和更新其他参数集。那么我们在交换中得到了什么呢？您可能会惊讶地发现-在上面定义的模型中-*我们的麻烦一无所获*！原因很简单。上面的隐藏单元由输入的仿射函数给出，而输出(Pre-Softmax)只是隐藏单元的仿射函数。仿射函数的仿射函数本身就是仿射函数。此外，我们的线性模型已经能够表示任何仿射函数。\\n+\\n+我们可以正式地查看等价性，方法是证明对于任意权重值，我们只需折叠隐藏层，即可产生具有参数$\\\\mathbf{W} = \\\\mathbf{W}^{(1)}\\\\mathbf{W}^{(2)}$和$\\\\mathbf{b} = \\\\mathbf{b}^{(1)} \\\\mathbf{W}^{(2)} + \\\\mathbf{b}^{(2)}$的等效单层模型：\\n+\\n+$$\\n+\\\\mathbf{O} = (\\\\mathbf{X} \\\\mathbf{W}^{(1)} + \\\\mathbf{b}^{(1)})\\\\mathbf{W}^{(2)} + \\\\mathbf{b}^{(2)} = \\\\mathbf{X} \\\\mathbf{W}^{(1)}\\\\mathbf{W}^{(2)} + \\\\mathbf{b}^{(1)} \\\\mathbf{W}^{(2)} + \\\\mathbf{b}^{(2)} = \\\\mathbf{X} \\\\mathbf{W} + \\\\mathbf{b}.\\n+$$\\n+\\n+为了实现多层结构的潜力，我们还需要一个关键因素：在仿射变换之后对每个隐藏单元应用非线性*激活函数*$\\\\sigma$。激活功能(例如，$\\\\sigma(\\\\cdot)$)的输出被称为*激活*。一般来说，有了激活函数，就不可能再将我们的MLP折叠成线性模型：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+    \\\\mathbf{H} & = \\\\sigma(\\\\mathbf{X} \\\\mathbf{W}^{(1)} + \\\\mathbf{b}^{(1)}), \\\\\\\\\\n+    \\\\mathbf{O} & = \\\\mathbf{H}\\\\mathbf{W}^{(2)} + \\\\mathbf{b}^{(2)}.\\\\\\\\\\n+\\\\end{aligned}\\n+$$\\n+\\n+由于$\\\\mathbf{X}$中的每一行对应于小批量中的一个示例，具有一些符号的滥用，所以我们定义非线性$\\\\sigma$以行的方式应用于其输入，即，一次一个示例。请注意，在:numref:`subsec_softmax_vectorization`中，我们以相同的方式使用了SoftMAX的符号来表示行式操作。通常，如本节所述，我们应用于隐藏层的激活函数不仅是按行的，而且是按元素的。这意味着在计算层的线性部分之后，我们可以计算每个激活，而不需要查看其他隐藏单元所取的值。对于大多数激活功能都是如此。\\n+\\n+为了构建更通用的MLP，我们可以继续堆叠这样的隐藏层，例如，$\\\\mathbf{H}^{(1)} = \\\\sigma_1(\\\\mathbf{X} \\\\mathbf{W}^{(1)} + \\\\mathbf{b}^{(1)})$和$\\\\mathbf{H}^{(2)} = \\\\sigma_2(\\\\mathbf{H}^{(1)} \\\\mathbf{W}^{(2)} + \\\\mathbf{b}^{(2)})$，一个在另一个之上，产生更具表现力的模型。\\n+\\n+### 通用逼近器\\n+\\n+MLP可以通过它们隐藏的神经元捕捉到我们输入之间的复杂相互作用，这取决于每个输入的值。我们可以很容易地设计隐藏节点来执行任意计算，例如，对一对输入进行基本逻辑操作。此外，对于激活函数的某些选择，众所周知，MLP是万能逼近器。即使是单隐层网络，给定足够的节点(可能非常多)和正确的权重集，我们也可以对任何函数建模，尽管实际上学习该函数是困难的部分。您可能认为您的神经网络有点像C编程语言。这种语言和任何其他现代语言一样，能够表达任何可计算的程序。但实际上要想出一个符合您的规范的程序才是最困难的部分。\\n+\\n+而且，仅仅因为一个单隐层网络\\n+*可以*学习任何函数\\n+并不意味着您应该尝试使用单隐藏层网络来解决所有问题。事实上，通过使用更深(而不是更广)的网络，我们可以更紧凑地逼近许多函数。我们将在接下来的章节中涉及更严格的论点。\\n+\\n+```{.python .input}\\n+%matplotlib inline\\n+from d2l import mxnet as d2l\\n+from mxnet import autograd, np, npx\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+%matplotlib inline\\n+from d2l import torch as d2l\\n+import torch\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+%matplotlib inline\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+```\\n+\\n+## 激活函数\\n+\\n+激活函数通过计算加权和并进一步加上偏差来决定神经元是否应该被激活。它们是将输入信号转换为输出的可微运算符，而它们中的大多数都增加了非线性。由于激活函数是深度学习的基础，让我们简要介绍一些常见的激活函数。\\n+\\n+### RELU函数\\n+\\n+最受欢迎的选择是*校正线性单元*(*RELU*)，因为它既实现简单，又在各种预测任务中表现良好。RELU提供了一种非常简单的非线性变换。给定元素$x$，函数被定义为该元素和$0$的最大值：\\n+\\n+$$\\\\operatorname{ReLU}(x) = \\\\max(x, 0).$$\\n+\\n+非正式地，RELU函数通过将相应的激活设置为0来仅保留正元素并丢弃所有负元素。为了获得一些直觉，我们可以画出函数的曲线图。如你所见，激活函数是分段线性的。\\n+\\n+```{.python .input}\\n+x = np.arange(-8.0, 8.0, 0.1)\\n+x.attach_grad()\\n+with autograd.record():\\n+    y = npx.relu(x)\\n+d2l.plot(x, y, \\'x\\', \\'relu(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\\n+y = torch.relu(x)\\n+d2l.plot(x.detach(), y.detach(), \\'x\\', \\'relu(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+x = tf.Variable(tf.range(-8.0, 8.0, 0.1), dtype=tf.float32)\\n+y = tf.nn.relu(x)\\n+d2l.plot(x.numpy(), y.numpy(), \\'x\\', \\'relu(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+当输入为负时，RELU函数的导数为0，而当输入为正时，RELU函数的导数为1。请注意，当输入取值精确等于0时，RELU函数不可微。在这些情况下，我们缺省为左侧导数，并假设当输入为0时导数为0。我们可以逃脱惩罚，因为输入可能永远不会是零。有一句古老的谚语说，如果微妙的边界条件很重要，我们很可能是在做(*真正*)数学，而不是工程。这一传统观点可能适用于这里。我们绘制下图所示的RELU函数的导数。\\n+\\n+```{.python .input}\\n+y.backward()\\n+d2l.plot(x, x.grad, \\'x\\', \\'grad of relu\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+y.backward(torch.ones_like(x), retain_graph=True)\\n+d2l.plot(x.detach(), x.grad, \\'x\\', \\'grad of relu\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+with tf.GradientTape() as t:\\n+    y = tf.nn.relu(x)\\n+d2l.plot(x.numpy(), t.gradient(y, x).numpy(), \\'x\\', \\'grad of relu\\',\\n+         figsize=(5, 2.5))\\n+```\\n+\\n+使用RELU的原因是，它的衍生品表现得特别好：要么它们消失了，要么它们只是让论点通过了。这使得优化表现得更好，并缓解了困扰神经网络以前版本(稍后将详细介绍)的已有文献记载的梯度消失问题。\\n+\\n+注意，RELU函数有许多变体，包括*参数化RELU*(*pReLU*)函数:cite:`He.Zhang.Ren.ea.2015`。此变体为relu添加了一个线性项，因此即使参数是否定的，某些信息仍然可以通过：\\n+\\n+$$\\\\operatorname{pReLU}(x) = \\\\max(0, x) + \\\\alpha \\\\min(0, x).$$\\n+\\n+### Sigmoid函数\\n+\\n+*Sigmoid函数*将其值位于域$\\\\mathbb{R}$中的输入变换为位于区间(0，1)上的输出。因此，Sigmoid通常称为*挤压函数*：它将范围(-inf，inf)中的任何输入压缩到范围(0，1)中的某个值：\\n+\\n+$$\\\\operatorname{sigmoid}(x) = \\\\frac{1}{1 + \\\\exp(-x)}.$$\\n+\\n+在最早的神经网络中，科学家们感兴趣的是对“有火”或“不火”的生物神经元进行建模。因此，这一领域的先驱，从人工神经元的发明者麦卡洛克和皮茨开始，专注于阈值单位。阈值激活在其输入低于某个阈值时取值0，当输入超过阈值时取值1。\\n+\\n+当注意力转移到基于梯度的学习时，Sigmoid函数是一个自然而然的选择，因为它是一个平滑的、可微分的阈值单元近似值。当我们想要将输出解释为二进制分类问题的概率时，Sigmoid仍然被广泛用作输出单元上的激活函数(您可以将Sigmoid视为Softmax的特例)。然而，乙状窦大部分已经被更简单、更容易训练的REU所取代，以便在隐藏的层中使用。在后面关于递归神经网络的章节中，我们将描述利用S型单元来控制跨时间信息流的体系结构。\\n+\\n+下面，我们绘制Sigmoid函数。请注意，当输入接近0时，Sigmoid函数接近线性变换。\\n+\\n+```{.python .input}\\n+with autograd.record():\\n+    y = npx.sigmoid(x)\\n+d2l.plot(x, y, \\'x\\', \\'sigmoid(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+y = torch.sigmoid(x)\\n+d2l.plot(x.detach(), y.detach(), \\'x\\', \\'sigmoid(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+y = tf.nn.sigmoid(x)\\n+d2l.plot(x.numpy(), y.numpy(), \\'x\\', \\'sigmoid(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+Sigmoid函数的导数由以下公式给出：\\n+\\n+$$\\\\frac{d}{dx} \\\\operatorname{sigmoid}(x) = \\\\frac{\\\\exp(-x)}{(1 + \\\\exp(-x))^2} = \\\\operatorname{sigmoid}(x)\\\\left(1-\\\\operatorname{sigmoid}(x)\\\\right).$$\\n+\\n+Sigmoid函数的导数如下所示。请注意，当输入为0时，Sigmoid函数的导数达到最大值0.25。当输入在任一方向上从0发散时，导数接近0。\\n+\\n+```{.python .input}\\n+y.backward()\\n+d2l.plot(x, x.grad, \\'x\\', \\'grad of sigmoid\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+# Clear out previous gradients\\n+x.grad.data.zero_()\\n+y.backward(torch.ones_like(x),retain_graph=True)\\n+d2l.plot(x.detach(), x.grad, \\'x\\', \\'grad of sigmoid\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+with tf.GradientTape() as t:\\n+    y = tf.nn.sigmoid(x)\\n+d2l.plot(x.numpy(), t.gradient(y, x).numpy(), \\'x\\', \\'grad of sigmoid\\',\\n+         figsize=(5, 2.5))\\n+```\\n+\\n+### TANH函数\\n+\\n+与Sigmoid函数类似，tanh(双曲正切)函数也压缩其输入，将其转换为-1和1之间的间隔上的元素：\\n+\\n+$$\\\\operatorname{tanh}(x) = \\\\frac{1 - \\\\exp(-2x)}{1 + \\\\exp(-2x)}.$$\\n+\\n+我们绘制下面的tanh函数。请注意，当输入接近0时，tanh函数接近线性变换。虽然函数的形状类似于Sigmoid函数，但tanh函数关于坐标系的原点表现出点对称。\\n+\\n+```{.python .input}\\n+with autograd.record():\\n+    y = np.tanh(x)\\n+d2l.plot(x, y, \\'x\\', \\'tanh(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+y = torch.tanh(x)\\n+d2l.plot(x.detach(), y.detach(), \\'x\\', \\'tanh(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+y = tf.nn.tanh(x)\\n+d2l.plot(x.numpy(), y.numpy(), \\'x\\', \\'tanh(x)\\', figsize=(5, 2.5))\\n+```\\n+\\n+tanh函数的导数是：\\n+\\n+$$\\\\frac{d}{dx} \\\\operatorname{tanh}(x) = 1 - \\\\operatorname{tanh}^2(x).$$\\n+\\n+tanh函数的导数如下所示。当输入接近0时，tanh函数的导数接近最大值1。正如我们在Sigmoid函数中看到的，当输入在任一方向远离0时，tanh函数的导数接近0。\\n+\\n+```{.python .input}\\n+y.backward()\\n+d2l.plot(x, x.grad, \\'x\\', \\'grad of tanh\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+# Clear out previous gradients.\\n+x.grad.data.zero_()\\n+y.backward(torch.ones_like(x),retain_graph=True)\\n+d2l.plot(x.detach(), x.grad, \\'x\\', \\'grad of tanh\\', figsize=(5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+with tf.GradientTape() as t:\\n+    y = tf.nn.tanh(x)\\n+d2l.plot(x.numpy(), t.gradient(y, x).numpy(), \\'x\\', \\'grad of tanh\\',\\n+         figsize=(5, 2.5))\\n+```\\n+\\n+总而言之，我们现在知道如何结合非线性来构建富有表现力的多层神经网络结构。顺便说一句，您的知识已经让您掌握了一个类似于1990年左右的实践者的工具包。在某些方面，您比在20世纪90年代工作的任何人都有优势，因为您可以利用功能强大的开源深度学习框架快速构建模型，只需使用几行代码。以前，训练这些网络需要研究人员编写数千行C和Fortran代码。\\n+\\n+## 摘要\\n+\\n+* MLP在输出层和输入层之间增加一个或多个完全连接的隐藏层，并通过激活功能转换隐藏层的输出。\\n+* 常用的激活功能包括RELU功能、Sigmoid功能和TOH功能。\\n+\\n+## 练习\\n+\\n+1. 计算pReLU激活函数的导数。\\n+1. 证明了仅使用RELU(或pReLU)的MLP构造了一个连续的分段线性函数。\\n+1. 拿出$\\\\operatorname{tanh}(x) + 1 = 2 \\\\operatorname{sigmoid}(2x)$美元。\\n+1. 假设我们有一个非线性，一次只适用于一个小批量。您预计这会造成什么样的问题？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/90)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/91)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/226)\\n+:end_tab:\\ndiff --git a/chapter_multilayer-perceptrons/numerical-stability-and-init_baidu.md b/chapter_multilayer-perceptrons/numerical-stability-and-init_baidu.md\\nnew file mode 100644\\nindex 00000000..5ca79ced\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/numerical-stability-and-init_baidu.md\\n@@ -0,0 +1,182 @@\\n+# 数值稳定性和初始化\\n+:label:`sec_numerical_stability`\\n+\\n+到目前为止，我们实现的每个模型都要求我们根据预先指定的分布初始化其参数。到目前为止，我们认为初始化方案是理所当然的，掩盖了如何做出这些选择的细节。你甚至可能会觉得这些选择并不特别重要。相反，初始化方案的选择在神经网络学习中起着非常重要的作用，对保持数值稳定性至关重要。此外，这些选择可以与非线性激活函数的选择以有趣的方式联系在一起。我们选择哪个函数以及如何初始化参数可以决定优化算法收敛的速度。在这里糟糕的选择会导致我们在训练时遇到爆炸或消失的梯度。在本节中，我们将更详细地探讨这些主题，并讨论一些有用的启发式方法，您将发现这些方法在您的职业生涯中对深度学习非常有用。\\n+\\n+## 消失和爆炸梯度\\n+\\n+考虑一个有$L$层的深层网络，输入$\\\\mathbf{x}$，输出$\\\\mathbf{o}$。每一层$l$由一个由权重$f_l$参数化的变换$f_l$定义，其隐藏变量为$\\\\mathbf{h}^{(l)}$（设$\\\\mathbf{h}^{(0)} = \\\\mathbf{x}$），我们的网络可以表示为：\\n+\\n+$$\\\\mathbf{h}^{(l)} = f_l (\\\\mathbf{h}^{(l-1)}) \\\\text{ and thus } \\\\mathbf{o} = f_L \\\\circ \\\\ldots \\\\circ f_1(\\\\mathbf{x}).$$\\n+\\n+如果所有隐藏变量和输入都是向量，我们可以将$\\\\mathbf{o}$相对于任何一组参数$\\\\mathbf{W}^{(l)}$的梯度写如下：\\n+\\n+$$\\\\partial_{\\\\mathbf{W}^{(l)}} \\\\mathbf{o} = \\\\underbrace{\\\\partial_{\\\\mathbf{h}^{(L-1)}} \\\\mathbf{h}^{(L)}}_{ \\\\mathbf{M}^{(L)} \\\\stackrel{\\\\mathrm{def}}{=}} \\\\cdot \\\\ldots \\\\cdot \\\\underbrace{\\\\partial_{\\\\mathbf{h}^{(l)}} \\\\mathbf{h}^{(l+1)}}_{ \\\\mathbf{M}^{(l+1)} \\\\stackrel{\\\\mathrm{def}}{=}} \\\\underbrace{\\\\partial_{\\\\mathbf{W}^{(l)}} \\\\mathbf{h}^{(l)}}_{ \\\\mathbf{v}^{(l)} \\\\stackrel{\\\\mathrm{def}}{=}}.$$\\n+\\n+换句话说，这个梯度是$L-l$矩阵$\\\\mathbf{M}^{(L)} \\\\cdot \\\\ldots \\\\cdot \\\\mathbf{M}^{(l+1)}$和梯度向量$\\\\mathbf{v}^{(l)}$的乘积。因此，我们容易受到同样的数值下溢问题的影响，这些问题往往是在将太多的概率相乘时突然出现的。在处理概率时，一个常见的技巧是切换到对数空间，即将压力从尾数转换为数值表示的指数。不幸的是，我们上面的问题更严重：最初，矩阵$\\\\mathbf{M}^{(l)}$可能有各种各样的特征值。他们的产品可能很大，也可能非常小。\\n+\\n+不稳定梯度带来的风险超出了数值表示。不可预测的梯度也威胁到我们的优化算法的稳定性。我们可能面临的参数更新要么（i）过大，破坏我们的模型（爆炸梯度*问题）；要么（ii）太小（消失梯度*问题），使得学习变得不可能，因为参数几乎每次更新都很难移动。\\n+\\n+### 消失梯度\\n+\\n+导致消失梯度问题的一个常见的罪魁祸首是在每个层的线性运算之后附加的激活函数$\\\\sigma$的选择。从历史上看，sigmoid函数$1/(1 + \\\\exp(-x))$（:numref:`sec_mlp`年引入）很受欢迎，因为它类似于阈值函数。由于早期的人工神经网络受到生物神经网络的启发，神经元发出“完全”或“完全不”的想法（就像生物神经元）似乎很吸引人。让我们仔细看看乙状结肠，看看它为什么会导致梯度消失。\\n+\\n+```{.python .input}\\n+%matplotlib inline\\n+from d2l import mxnet as d2l\\n+from mxnet import autograd, np, npx\\n+npx.set_np()\\n+\\n+x = np.arange(-8.0, 8.0, 0.1)\\n+x.attach_grad()\\n+with autograd.record():\\n+    y = npx.sigmoid(x)\\n+y.backward()\\n+\\n+d2l.plot(x, [y, x.grad], legend=[\\'sigmoid\\', \\'gradient\\'], figsize=(4.5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+%matplotlib inline\\n+from d2l import torch as d2l\\n+import torch\\n+\\n+x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\\n+y = torch.sigmoid(x)\\n+y.backward(torch.ones_like(x))\\n+\\n+d2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()],\\n+         legend=[\\'sigmoid\\', \\'gradient\\'], figsize=(4.5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+%matplotlib inline\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+x = tf.Variable(tf.range(-8.0, 8.0, 0.1))\\n+with tf.GradientTape() as t:\\n+    y = tf.nn.sigmoid(x)\\n+d2l.plot(x.numpy(), [y.numpy(), t.gradient(y, x).numpy()],\\n+         legend=[\\'sigmoid\\', \\'gradient\\'], figsize=(4.5, 2.5))\\n+```\\n+\\n+如你所见，sigmoid的梯度在它的输入大和小的时候都会消失。此外，当反向传播通过许多层时，除非我们在金发姑娘区，那里许多乙状体的输入接近于零，否则整个产品的梯度可能会消失。当我们的网络拥有许多层时，除非我们小心，否则在某一层可能会切断梯度。事实上，这个问题曾经困扰着深度网络培训。因此，相对稳定（但在神经上不太可信）的ReLUs已经成为从业者的默认选择。\\n+\\n+### 爆炸梯度\\n+\\n+相反的问题，当梯度爆炸时，同样令人烦恼。为了更好地说明这一点，我们画了100个高斯随机矩阵，并与一些初始矩阵相乘。对于我们选择的尺度（方差$\\\\sigma^2=1$的选择），矩阵乘积爆炸。当这是由于深层网络的初始化而发生的，我们没有机会得到一个梯度下降优化器来收敛。\\n+\\n+```{.python .input}\\n+M = np.random.normal(size=(4, 4))\\n+print(\\'a single matrix\\', M)\\n+for i in range(100):\\n+    M = np.dot(M, np.random.normal(size=(4, 4)))\\n+\\n+print(\\'after multiplying 100 matrices\\', M)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+M = torch.normal(0, 1, size=(4,4))\\n+print(\\'a single matrix \\\\n\\',M)\\n+for i in range(100):\\n+    M = torch.mm(M,torch.normal(0, 1, size=(4, 4)))\\n+\\n+print(\\'after multiplying 100 matrices\\\\n\\', M)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+M = tf.random.normal((4, 4))\\n+print(\\'a single matrix \\\\n\\', M)\\n+for i in range(100):\\n+    M = tf.matmul(M, tf.random.normal((4, 4)))\\n+\\n+print(\\'after multiplying 100 matrices\\\\n\\', M.numpy())\\n+```\\n+\\n+### 打破对称\\n+\\n+神经网络设计中的另一个问题是参数化过程中固有的对称性。假设我们有一个简单的MLP，有一个隐藏层和两个单元。在这种情况下，我们可以对第一层的权重$\\\\mathbf{W}^{(1)}$进行置换，同样地，也可以对输出层的权重进行置换以获得相同的函数。第一个隐藏单元和第二个隐藏单元没有什么特别的区别。换句话说，我们在每一层的隐藏单元之间有排列对称性。\\n+\\n+这不仅仅是理论上的麻烦。考虑前面提到的带有两个隐藏单元的一个隐藏层MLP。为了便于说明，假设输出层将两个隐藏单元转换为一个输出单元。我们想象一下，如果93层的某个常数被初始化了，会发生什么。在这种情况下，在前向传播过程中，任何一个隐藏单元采用相同的输入和参数，产生相同的激活，并将其馈送给输出单元。在反向传播期间，根据参数$\\\\mathbf{W}^{(1)}$对输出单元进行微分，得到一个梯度，其元素都取相同的值。因此，在基于梯度的迭代（例如，小批量随机梯度下降）之后，$\\\\mathbf{W}^{(1)}$的所有元素仍然取相同的值。这样的迭代永远不会打破对称性，我们可能永远也无法实现网络的表达能力。隐藏层的行为就好像只有一个单元。请注意，虽然小批量随机梯度下降不会打破这种对称性，辍学正则化会！\\n+\\n+## 参数初始化\\n+\\n+解决（或至少减轻）上述问题的一种方法是通过仔细初始化。优化过程中的额外注意和适当的正则化可以进一步提高稳定性。\\n+\\n+### 默认初始化\\n+\\n+在前面的章节中，例如在:numref:`sec_linear_concise`中，我们使用正态分布来初始化权重值。如果我们不指定初始化方法，框架将使用默认的随机初始化方法，对于中等规模的问题，这种方法通常很有效。\\n+\\n+### Xavier初始化\\n+:label:`subsec_xavier`\\n+\\n+让我们看看某个完全连接层的输出（例如，隐藏变量）$o_{i}$的比例分布\\n+*没有非线性*。\\n+对于该层$n_\\\\mathrm{in}$输入$x_j$及其相关权重$w_{ij}$，输出由\\n+\\n+$$o_{i} = \\\\sum_{j=1}^{n_\\\\mathrm{in}} w_{ij} x_j.$$\\n+\\n+重量$w_{ij}$都是独立于同一分布绘制的。此外，我们假设这个分布具有零均值和方差$\\\\sigma^2$。注意，这并不意味着分布必须是高斯分布，只是平均值和方差必须存在。现在，让我们假设层$x_j$的输入也具有零均值和方差$\\\\gamma^2$，并且它们独立于$w_{ij}$并且彼此独立。在这种情况下，我们可以计算$o_i$的平均值和方差，如下所示：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+    E[o_i] & = \\\\sum_{j=1}^{n_\\\\mathrm{in}} E[w_{ij} x_j] \\\\\\\\&= \\\\sum_{j=1}^{n_\\\\mathrm{in}} E[w_{ij}] E[x_j] \\\\\\\\&= 0, \\\\\\\\\\n+    \\\\mathrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 \\\\\\\\\\n+        & = \\\\sum_{j=1}^{n_\\\\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\\\\\\\\\n+        & = \\\\sum_{j=1}^{n_\\\\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\\\\\\\\\n+        & = n_\\\\mathrm{in} \\\\sigma^2 \\\\gamma^2.\\n+\\\\end{aligned}\\n+$$\\n+\\n+保持方差不变的一种方法是设置$n_\\\\mathrm{in} \\\\sigma^2 = 1$。现在考虑反向传播。在那里，我们面临着一个类似的问题，尽管梯度是从离输出更近的层传播的。使用与正向传播相同的推理，我们可以看到梯度的方差可以放大，除非$n_\\\\mathrm{out} \\\\sigma^2 = 1$，其中$n_\\\\mathrm{out}$是该层的输出数量。这使我们陷入两难境地：我们不可能同时满足这两个条件。相反，我们只需满足：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+\\\\frac{1}{2} (n_\\\\mathrm{in} + n_\\\\mathrm{out}) \\\\sigma^2 = 1 \\\\text{ or equivalently }\\n+\\\\sigma = \\\\sqrt{\\\\frac{2}{n_\\\\mathrm{in} + n_\\\\mathrm{out}}}.\\n+\\\\end{aligned}\\n+$$\\n+\\n+这就是现在标准且实用的Xavier初始化*的基础，它以其创建者:cite:`Glorot.Bengio.2010`的第一作者命名。通常，Xavier初始化从均值和方差为零的高斯分布中采样权重$\\\\sigma^2 = \\\\frac{2}{n_\\\\mathrm{in} + n_\\\\mathrm{out}}$。我们也可以利用Xavier的直觉来选择从均匀分布中抽样权重时的方差。注意均匀分布$U(-a, a)$的方差为$\\\\frac{a^2}{3}$。将$\\\\frac{a^2}{3}$插入到$\\\\sigma^2$的条件中，可以根据\\n+\\n+$$U\\\\left(-\\\\sqrt{\\\\frac{6}{n_\\\\mathrm{in} + n_\\\\mathrm{out}}}, \\\\sqrt{\\\\frac{6}{n_\\\\mathrm{in} + n_\\\\mathrm{out}}}\\\\right).$$\\n+\\n+虽然上述数学推理中不存在非线性的假设在神经网络中很容易被违背，但Xavier初始化方法在实际应用中效果良好。\\n+\\n+### 超越\\n+\\n+上面的推理仅仅触及了参数初始化的现代方法的表面。深度学习框架通常实现十几种不同的启发式方法。此外，参数初始化一直是深度学习基础研究的热点。其中包括专门用于绑定（共享）参数、超分辨率、序列模型和其他情况的启发式方法。例如，肖等。通过使用精心设计的初始化方法:cite:`Xiao.Bahri.Sohl-Dickstein.ea.2018`，证明了无需构造技巧训练10000层神经网络的可能性。\\n+\\n+如果您对该主题感兴趣，我们建议您深入研究本模块的内容，阅读提出并分析每种启发式方法的论文，然后探索有关该主题的最新出版物。也许你会偶然发现甚至发明一个聪明的想法，并为深度学习框架提供一个实现。\\n+\\n+## 摘要\\n+\\n+* 梯度消失和爆炸是深部网络中常见的问题。在参数初始化时需要非常小心，以确保梯度和参数保持良好的控制。\\n+* 需要初始化启发式来确保初始梯度既不太大也不太小。\\n+* ReLU激活函数缓解了消失梯度问题。这可以加速收敛。\\n+* 随机初始化是保证优化前对称性被破坏的关键。\\n+* Xavier初始化表明，对于每一层，任何输出的方差不受输入数目的影响，任何梯度的方差不受输出数量的影响。\\n+\\n+## 练习\\n+\\n+1. 你能设计出其他的情况吗？除了MLP层的排列对称性之外，神经网络可能会表现出需要破坏的对称性吗？\\n+1. 我们是否可以将线性回归或softmax回归中的所有权重参数初始化为相同的值？\\n+1. 求两个矩阵乘积特征值的解析界。这说明了如何确保渐变条件良好？\\n+1. 如果我们知道有些术语有分歧，我们能在事后解决吗？看看关于分层自适应速率缩放的论文:cite:`You.Gitman.Ginsburg.2017`。\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/103)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/104)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/235)\\n+:end_tab:\\ndiff --git a/chapter_multilayer-perceptrons/numerical-stability-and-init_tencent.md b/chapter_multilayer-perceptrons/numerical-stability-and-init_tencent.md\\nnew file mode 100644\\nindex 00000000..c38c6c5f\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/numerical-stability-and-init_tencent.md\\n@@ -0,0 +1,182 @@\\n+# 数值稳定性和初始化\\n+:label:`sec_numerical_stability`\\n+\\n+到目前为止，我们实现的每个模型都要求我们根据某个预先指定的分布来初始化它的参数。到目前为止，我们认为初始化方案是理所当然的，忽略了这些选择是如何做出的细节。你甚至可能会得到这样的印象：这些选择并不是特别重要。相反，初始化方案的选择在神经网络学习中起着非常重要的作用，对保持数值稳定性至关重要。此外，这些选择可以通过选择非线性激活函数以有趣的方式捆绑在一起。我们选择哪个函数以及如何初始化参数可以决定我们的优化算法收敛的速度有多快。这里的糟糕选择可能会导致我们在训练时遇到爆炸或消失的梯度。在本节中，我们将更详细地研究这些主题，并讨论一些有用的启发式方法，您会发现这些启发式方法在您的整个深度学习生涯中都很有用。\\n+\\n+## 消失和爆炸梯度\\n+\\n+考虑一个具有$L$层、输入$\\\\mathbf{x}$和输出$\\\\mathbf{o}$的深层网络。每一层$l$由变换$f_l$定义，该变换由权重$\\\\mathbf{W}^{(l)}$参数化，其隐藏变量是$\\\\mathbf{h}^{(l)}$(令$\\\\mathbf{h}^{(0)} = \\\\mathbf{x}$)，我们的网络可以表示为：\\n+\\n+$$\\\\mathbf{h}^{(l)} = f_l (\\\\mathbf{h}^{(l-1)}) \\\\text{ and thus } \\\\mathbf{o} = f_L \\\\circ \\\\ldots \\\\circ f_1(\\\\mathbf{x}).$$\\n+\\n+如果所有隐藏变量和输入都是向量，我们可以将$\\\\mathbf{o}$相对于任何一组参数$\\\\mathbf{W}^{(l)}$的梯度写如下：\\n+\\n+$$\\\\partial_{\\\\mathbf{W}^{(l)}} \\\\mathbf{o} = \\\\underbrace{\\\\partial_{\\\\mathbf{h}^{(L-1)}} \\\\mathbf{h}^{(L)}}_{ \\\\mathbf{M}^{(L)} \\\\stackrel{\\\\mathrm{def}}{=}} \\\\cdot \\\\ldots \\\\cdot \\\\underbrace{\\\\partial_{\\\\mathbf{h}^{(l)}} \\\\mathbf{h}^{(l+1)}}_{ \\\\mathbf{M}^{(l+1)} \\\\stackrel{\\\\mathrm{def}}{=}} \\\\underbrace{\\\\partial_{\\\\mathbf{W}^{(l)}} \\\\mathbf{h}^{(l)}}_{ \\\\mathbf{v}^{(l)} \\\\stackrel{\\\\mathrm{def}}{=}}.$$\\n+\\n+换言之，该梯度是$L-l$个矩阵$\\\\mathbf{M}^{(L)} \\\\cdot \\\\ldots \\\\cdot \\\\mathbf{M}^{(l+1)}$和梯度向量$\\\\mathbf{v}^{(l)}$的乘积。因此，我们容易受到相同的数值下溢问题的影响，当将太多的概率乘在一起时，这些问题经常会出现。在处理概率时，一个常见的技巧是切换到对数空间，即将压力从尾数转移到数值表示的指数。不幸的是，我们上面的问题更为严重：最初，矩阵$\\\\mathbf{M}^{(l)}$可能具有各种各样的特征值。他们可能很小，也可能很大，他们的产品可能“非常大”，也可能“非常小”。\\n+\\n+不稳定梯度带来的风险超出了数值表示的范围。不可预测的梯度也威胁到我们优化算法的稳定性。我们可能面临的参数更新要么(I)过大，破坏了我们的模型(*爆炸梯度*问题)；要么(Ii)过小(*消失梯度*问题)，使得学习变得不可能，因为参数几乎不会在每次更新时移动。\\n+\\n+### 消失梯度\\n+\\n+导致消失梯度问题的一个常见罪魁祸首是选择附加在每层的线性运算之后的激活函数$\\\\sigma$。从历史上看，S形函数$1/(1 + \\\\exp(-x))$(:numref:`sec_mlp`引入)很流行，因为它类似于阈值函数。由于早期的人工神经网络受到生物神经网络的启发，神经元要么“完全”激发，要么“完全不激发”(就像生物神经元)的想法似乎很有吸引力。让我们仔细看看乙状窦，看看它为什么会导致渐变消失。\\n+\\n+```{.python .input}\\n+%matplotlib inline\\n+from d2l import mxnet as d2l\\n+from mxnet import autograd, np, npx\\n+npx.set_np()\\n+\\n+x = np.arange(-8.0, 8.0, 0.1)\\n+x.attach_grad()\\n+with autograd.record():\\n+    y = npx.sigmoid(x)\\n+y.backward()\\n+\\n+d2l.plot(x, [y, x.grad], legend=[\\'sigmoid\\', \\'gradient\\'], figsize=(4.5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+%matplotlib inline\\n+from d2l import torch as d2l\\n+import torch\\n+\\n+x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\\n+y = torch.sigmoid(x)\\n+y.backward(torch.ones_like(x))\\n+\\n+d2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()],\\n+         legend=[\\'sigmoid\\', \\'gradient\\'], figsize=(4.5, 2.5))\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+%matplotlib inline\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+\\n+x = tf.Variable(tf.range(-8.0, 8.0, 0.1))\\n+with tf.GradientTape() as t:\\n+    y = tf.nn.sigmoid(x)\\n+d2l.plot(x.numpy(), [y.numpy(), t.gradient(y, x).numpy()],\\n+         legend=[\\'sigmoid\\', \\'gradient\\'], figsize=(4.5, 2.5))\\n+```\\n+\\n+正如你所看到的，无论是当它的输入很大，还是当它们很小时，乙状窦的梯度都会消失。此外，当反向传播通过许多层时，除非我们在金发地带，在那里许多S型线的输入接近于零，否则整个乘积的梯度可能会消失。当我们的网络夸耀有很多层时，除非我们小心，否则梯度很可能会在某一层被切断。事实上，这个问题曾经困扰着深度网络培训。因此，更稳定(但在神经上不太可信)的RELU已经成为从业者的默认选择。\\n+\\n+### 爆炸梯度\\n+\\n+相反的问题，当梯度爆炸时，可能同样令人烦恼。为了更好地说明这一点，我们绘制了100个高斯随机矩阵，并将它们与一些初始矩阵相乘。对于我们选择的比例(选择方差$\\\\sigma^2=1$)，矩阵乘积爆炸。当由于深度网络的初始化而发生这种情况时，我们没有机会让梯度下降优化器收敛。\\n+\\n+```{.python .input}\\n+M = np.random.normal(size=(4, 4))\\n+print(\\'a single matrix\\', M)\\n+for i in range(100):\\n+    M = np.dot(M, np.random.normal(size=(4, 4)))\\n+\\n+print(\\'after multiplying 100 matrices\\', M)\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+M = torch.normal(0, 1, size=(4,4))\\n+print(\\'a single matrix \\\\n\\',M)\\n+for i in range(100):\\n+    M = torch.mm(M,torch.normal(0, 1, size=(4, 4)))\\n+\\n+print(\\'after multiplying 100 matrices\\\\n\\', M)\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+M = tf.random.normal((4, 4))\\n+print(\\'a single matrix \\\\n\\', M)\\n+for i in range(100):\\n+    M = tf.matmul(M, tf.random.normal((4, 4)))\\n+\\n+print(\\'after multiplying 100 matrices\\\\n\\', M.numpy())\\n+```\\n+\\n+### 打破对称\\n+\\n+神经网络设计中的另一个问题是其参数化所固有的对称性。假设我们有一个简单的MLP，它有一个隐藏层和两个单元。在这种情况下，我们可以对第一层的权重$\\\\mathbf{W}^{(1)}$进行置换，并且同样对输出层的权重进行置换以获得相同的函数。第一隐藏单元与第二隐藏单元没有什么特别的区别。换句话说，我们在每一层的隐藏单元之间具有排列对称性。\\n+\\n+这不仅仅是理论上的麻烦。考虑前述具有两个隐藏单元的单隐藏层MLP。为便于说明，假设输出层将两个隐藏单元转换为仅一个输出单元。想象一下，如果我们将隐藏层的所有参数初始化为$\\\\mathbf{W}^{(1)} = c$，而某个常量为$c$，会发生什么情况。在这种情况下，在前向传播期间，两个隐藏单元采用相同的输入和参数，产生相同的激活，该激活被馈送到输出单元。在反向传播期间，相对于参数$\\\\mathbf{W}^{(1)}$对输出单元进行微分给出其元素全部取相同值的梯度。因此，在基于梯度的迭代(例如，小批量随机梯度下降)之后，$\\\\mathbf{W}^{(1)}$的所有元素仍然采用相同的值。这样的迭代本身永远不会“打破对称性”，我们可能永远不会意识到网络的表现力。隐藏层的行为就像它只有一个单元一样。请注意，虽然小批量随机梯度下降不会打破这种对称性，但辍学正则化将打破这种对称性！\\n+\\n+## 参数初始化\\n+\\n+解决-或者至少缓解-上面提出的问题的一种方法是通过仔细的初始化。优化过程中的额外注意和适当的规则化可以进一步提高稳定性。\\n+\\n+### 默认初始化\\n+\\n+在前面的部分中，例如在:numref:`sec_linear_concise`中，我们使用正态分布来初始化权重值。如果我们不指定初始化方法，框架将使用默认的随机初始化方法，这种方法在实践中通常适用于中等大小的问题。\\n+\\n+### Xavier初始化\\n+:label:`subsec_xavier`\\n+\\n+让我们看一下某些完全连接层的输出(例如，隐藏变量)$o_{i}$的比例分布\\n+*没有非线性*。\\n+利用该层的$n_\\\\mathrm{in}$个输入$x_j$及其关联权重$w_{ij}$，通过以下方式给出输出\\n+\\n+$$o_{i} = \\\\sum_{j=1}^{n_\\\\mathrm{in}} w_{ij} x_j.$$\\n+\\n+权重$w_{ij}$都独立于相同的分布绘制。此外，让我们假设该分布具有零均值和方差$\\\\sigma^2$。请注意，这并不意味着分布必须是高斯的，只是均值和方差需要存在。现在，让我们假设层$x_j$的输入也具有零均值和方差$\\\\gamma^2$，并且它们独立于$w_{ij}$并且彼此独立。在这种情况下，我们可以按如下方式计算$o_i$的平均值和方差：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+    E[o_i] & = \\\\sum_{j=1}^{n_\\\\mathrm{in}} E[w_{ij} x_j] \\\\\\\\&= \\\\sum_{j=1}^{n_\\\\mathrm{in}} E[w_{ij}] E[x_j] \\\\\\\\&= 0, \\\\\\\\\\n+    \\\\mathrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 \\\\\\\\\\n+        & = \\\\sum_{j=1}^{n_\\\\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\\\\\\\\\n+        & = \\\\sum_{j=1}^{n_\\\\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\\\\\\\\\n+        & = n_\\\\mathrm{in} \\\\sigma^2 \\\\gamma^2.\\n+\\\\end{aligned}\\n+$$\\n+\\n+保持方差固定的一种方法是设置$n_\\\\mathrm{in} \\\\sigma^2 = 1$。现在考虑反向传播。在那里，我们面临着类似的问题，尽管梯度是从更靠近输出的层传播的。使用与正向传播相同的推理，我们可以看到，除非达到$n_\\\\mathrm{out} \\\\sigma^2 = 1$，否则梯度的方差可能会增大，其中$n_\\\\mathrm{out}$是该层的输出数。这使我们进退两难：我们不可能同时满足这两个条件。相反，我们只是试图满足：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+\\\\frac{1}{2} (n_\\\\mathrm{in} + n_\\\\mathrm{out}) \\\\sigma^2 = 1 \\\\text{ or equivalently }\\n+\\\\sigma = \\\\sqrt{\\\\frac{2}{n_\\\\mathrm{in} + n_\\\\mathrm{out}}}.\\n+\\\\end{aligned}\\n+$$\\n+\\n+这就是以其创建者:cite:`Glorot.Bengio.2010`的第一作者的名字命名的现在标准且实际有益的“泽维尔初始化”背后的推理。通常，泽维尔初始化从具有零均值和零方差的高斯分布中采样权重$\\\\sigma^2 = \\\\frac{2}{n_\\\\mathrm{in} + n_\\\\mathrm{out}}$。当从均匀分布中抽样权重时，我们也可以采用Xavier的直觉来选择方差。注意，均匀分布$U(-a, a)$具有方差$\\\\frac{a^2}{3}$。将$\\\\frac{a^2}{3}$插入我们在$\\\\sigma^2$上的条件会产生根据以下内容进行初始化的建议\\n+\\n+$$U\\\\left(-\\\\sqrt{\\\\frac{6}{n_\\\\mathrm{in} + n_\\\\mathrm{out}}}, \\\\sqrt{\\\\frac{6}{n_\\\\mathrm{in} + n_\\\\mathrm{out}}}\\\\right).$$\\n+\\n+虽然上述数学推理中不存在非线性的假设在神经网络中很容易被违反，但Xavier初始化方法在实践中证明是有效的。\\n+\\n+### 超越\\n+\\n+上面的推理仅仅触及了现代参数初始化方法的皮毛。深度学习框架通常实现十几种不同的启发式方法。此外，参数初始化一直是深度学习基础研究的热点领域。其中包括专门用于绑定(共享)参数、超分辨率、序列模型和其他情况的启发式算法。例如，肖等人。演示了通过使用精心设计的初始化方法10000来训练无体系结构技巧的:cite:`Xiao.Bahri.Sohl-Dickstein.ea.2018`层神经网络的可能性。\\n+\\n+如果您对该主题感兴趣，我们建议您深入研究本模块提供的内容，阅读提出并分析每个启发式方法的论文，然后浏览有关该主题的最新出版物。也许您会偶然发现甚至发明一个聪明的想法，并为深度学习框架贡献一个实现。\\n+\\n+## 摘要\\n+\\n+* 梯度的消失和爆炸是深层网络中普遍存在的问题。参数初始化需要非常小心，以确保梯度和参数保持良好控制。\\n+* 需要初始化试探法来确保初始梯度既不太大也不太小。\\n+* REU激活函数缓解了消失梯度问题。这可以加速融合。\\n+* 随机初始化是确保在优化之前对称被打破的关键。\\n+* Xavier初始化表明，对于每一层，任何输出的方差不受输入数目的影响，任何梯度的方差也不受输出数目的影响。\\n+\\n+## 练习\\n+\\n+1. 除了MLP层中的排列对称性之外，您还能设计出神经网络可能表现出对称性需要破缺的其他情况吗？\\n+1. 我们可以将线性回归或Softmax回归中的所有权重参数初始化为相同的值吗？\\n+1. 查找两个矩阵乘积的特征值的解析界。这对确保渐变条件良好有什么启示？\\n+1. 如果我们知道有些条款有分歧，我们能在事后解决这个问题吗？请参阅有关分层自适应速率缩放的论文，了解Inspiration :cite:`You.Gitman.Ginsburg.2017`。\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/103)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/104)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/235)\\n+:end_tab:\\ndiff --git a/chapter_multilayer-perceptrons/underfit-overfit_baidu.md b/chapter_multilayer-perceptrons/underfit-overfit_baidu.md\\nnew file mode 100644\\nindex 00000000..43595caa\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/underfit-overfit_baidu.md\\n@@ -0,0 +1,343 @@\\n+# 选型、欠拟合和过拟合\\n+:label:`sec_model_selection`\\n+\\n+作为机器学习科学家，我们的目标是发现*模式*。但是，我们怎样才能确定我们已经真正发现了一个*一般*模式，而不是简单地记忆我们的数据呢？例如，想象一下，我们想在基因标记中寻找将病人与痴呆状态联系起来的模式，其中的标签来自$\\\\{\\\\text{dementia}, \\\\text{mild cognitive impairment}, \\\\text{healthy}\\\\}$。因为每个人的基因唯一地识别他们（忽略相同的兄弟姐妹），所以有可能记住整个数据集。\\n+\\n+我们不想让我们的模型说\\n+*“那是鲍勃！我记得他！他得了痴呆症！”*\\n+原因很简单。当我们将来部署该模型时，我们将遇到模型从未见过的患者。我们的预测只有在我们的模型真正发现了一个*一般*模式的情况下才会有用。\\n+\\n+为了更正式地重述一下，我们的目标是发现捕捉基础人群中规律性的模式，我们的训练集就是从中提取的。如果我们在这方面取得了成功，那么我们甚至可以成功地评估我们从未遇到过的个人的风险。这个问题——如何发现泛化的模式——是机器学习的基本问题。\\n+\\n+危险在于，当我们训练模型时，我们只访问一小部分数据样本。最大的公共图像数据集包含大约一百万张图像。更多的时候，我们必须从成千上万的数据示例中学习。在一个大型医院系统中，我们可能会访问数十万份医疗记录。当我们使用有限的样本时，我们可能会冒着这样的风险：当我们收集更多的数据时，我们可能会发现明显的关联性，而这些关联最终却无法成立。\\n+\\n+将我们的训练数据拟合得比我们对潜在分布拟合得更紧密的现象称为“过度拟合”，而用于防止过度拟合的技术称为“正则化”。在前面的部分中，您可能在使用时尚MNIST数据集时观察到了这种效果。如果你在实验过程中改变了模型结构或超参数，你可能已经注意到，有了足够的神经元、层和训练时间，模型最终可以在训练集上达到完美的精度，即使测试数据的精确度下降了。\\n+\\n+## 训练误差和泛化误差\\n+\\n+为了更正式地讨论这一现象，我们需要区分训练误差和泛化误差。*training error*是我们在训练数据集上计算的模型误差，而*generalization error*是我们对模型误差的预期，如果我们将其应用于从与原始样本相同的基础数据分布中提取的无限多个额外数据示例。\\n+\\n+有问题的是，我们永远无法精确计算泛化误差。这是因为无限数据流是一个虚构的对象。在实践中，我们必须通过将我们的模型应用于一个独立的测试集，该测试集由从我们的训练集中截取的数据样本组成。\\n+\\n+以下三个思考实验将有助于更好地说明这种情况。假设一个大学生正在准备期末考试。一个勤奋的学生会努力练习好，用往年的考试来检验自己的能力。尽管如此，在过去的考试中取得好成绩并不能保证他会在重要的时候取得优异成绩。例如，学生可以通过死记硬背来准备考试问题的答案。这需要学生记住很多东西。她甚至可以完美地记住过去考试的答案。另一个学生可能会试图理解给出某些答案的原因。在大多数情况下，后一个学生会做得更好。\\n+\\n+同样，考虑一个简单地使用查找表来回答问题的模型。如果允许输入的集合是离散的并且相当小，那么在查看了*许多*训练示例之后，这种方法将表现良好。然而，当面对以前从未见过的例子时，这个模型没有比随机猜测更好的能力。实际上，输入空间太大，无法记住与每个可想象的输入相对应的答案。例如，考虑黑白$28\\\\times28$图像。如果每个像素可以取$256$灰度值中的一个，则存在$256^{784}$个可能的图像。这意味着低分辨率灰度缩略图像的数量远远超过宇宙中原子的数量。即使我们能遇到这样的数据，我们也永远付不起存储查找表的费用。\\n+\\n+最后，考虑一下根据一些可能可用的上下文特征对掷硬币的结果（0级：正面，1级：反面）进行分类的问题。假设硬币是公平的。不管我们提出什么算法，泛化误差总是$\\\\frac{1}{2}$。然而，对于大多数算法，我们应该期望我们的训练误差会大大降低，这取决于抽签的运气，即使我们没有任何特征！考虑数据集{0，1，1，1，0，1}。我们的无特征算法必须依赖于总是预测*多数类*，从我们有限的样本来看，它似乎是*1*。在这种情况下，总是预测类1的模型将产生$\\\\frac{1}{3}$的错误，这比我们的泛化错误要好得多。随着数据量的增加，头的分数显著偏离$\\\\frac{1}{2}$的概率减小，我们的训练误差将与泛化误差相匹配。\\n+\\n+### 统计学习理论\\n+\\n+由于泛化是机器学习中的基本问题，您可能不会惊讶于许多数学家和理论家毕生致力于发展形式化理论来描述这种现象。在他们的[同名定理](https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_定理)Glivenko和Cantelli导出了训练误差收敛到泛化误差的速率。在一系列开创性的论文中(https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_理论)把这个理论推广到更一般的函数类。这项工作奠定了统计学习理论的基础。\\n+\\n+在标准的监督学习设置中，我们到目前为止一直在讨论，并且在本书的大部分内容中，我们假设训练数据和测试数据都是从相同的分布中独立提取的。这通常被称为“i.i.d.假设”，这意味着对数据进行采样的过程没有内存。换句话说，抽取的第二个样本和第三个样本的相关性并不比第二个样本和第两百万个样本的相关性高。\\n+\\n+作为一个好的机器学习科学家需要批判性的思考，你应该已经在这个假设上捅了个洞，想出一些假设失败的常见案例。如果我们根据从加州大学旧金山分校医学中心收集的数据训练一个死亡率风险预测因子，并将其应用于马萨诸塞州综合医院的患者，会怎么样？这些分布完全不同。此外，绘制可能在时间上是相关的。如果我们把推特的话题分类呢？新闻周期会在所讨论的主题中产生时间依赖性，违反任何独立性假设。\\n+\\n+有时，我们可以逃脱对i.i.d.假设的轻微违反，我们的模型将继续非常有效地工作。毕竟，几乎每一个现实世界中的应用程序都至少涉及到一些轻微的违反i.i.d.假设的情况，然而我们有许多有用的工具用于各种应用，如人脸识别、语音识别和语言翻译。\\n+\\n+其他违规行为肯定会引起麻烦。试想一下，例如，如果我们试图训练一个面部识别系统，只对大学生进行训练，然后想把它作为一个工具来监测养老院人群中的老年病。这不太可能奏效，因为大学生看起来往往与老年人大不相同。\\n+\\n+在接下来的章节中，我们将讨论因违反i.i.d.假设而产生的问题。目前，即使把i.i.d.假设视为理所当然，理解泛化也是一个令人生畏的问题。此外，解释精确的理论基础，也许可以解释为什么深层神经网络会像他们那样泛化，继续困扰学习理论中最伟大的头脑。\\n+\\n+当我们训练我们的模型时，我们试图寻找一个尽可能适合训练数据的函数。如果函数非常灵活，它可以像捕捉真正的关联一样容易地捕捉到虚假的模式，那么它可能会执行得*太好*而不会生成一个能很好地推广到不可见数据的模型。这正是我们想要避免或至少控制的。深度学习中的许多技术都是旨在防止过度适应的启发式和技巧。\\n+\\n+### 模型复杂性\\n+\\n+当模型简单，数据丰富时，我们期望泛化误差与训练误差相似。当我们使用更复杂的模型和更少的例子时，我们期望训练误差减小，而泛化差距增大。精确地构成模型复杂性的是一个复杂的问题。一个模型是否能很好地推广有许多因素。例如，具有更多参数的模型可能会被认为更复杂。一个参数可以取更大范围值的模型可能更复杂。通常对于神经网络，我们认为一个需要更多训练迭代的模型更复杂，而一个需要提前停止（较少的训练迭代）的模型就不那么复杂。\\n+\\n+很难比较本质上不同的模型类成员之间的复杂性（例如，决策树与神经网络）。就目前而言，一个简单的经验法则是相当有用的：一个能轻易解释任意事实的模型是统计学家认为复杂的，而表达能力有限但仍能很好地解释数据的模型可能更接近事实。在哲学上，这与波普尔关于科学理论可证伪性的标准密切相关：如果一个理论符合数据，并且有具体的测试可以用来证明它是正确的。这一点很重要，因为所有的统计估计\\n+*事后*，\\n+i、 我们在观察事实之后进行估计，因此容易受到相关谬论的影响。目前，我们将把哲学放在一边，坚持更具体的问题。\\n+\\n+在本节中，为了给您一些直观的印象，我们将集中讨论一些影响模型类泛化的因素：\\n+\\n+1. 可调参数的数目。当可调参数（有时称为“自由度”）的数量很大时，模型更容易过度拟合。\\n+1. 参数获取的值。当权重可以取更大范围的值时，模型更容易过度拟合。\\n+1. 训练实例的数量。即使模型很简单，也很容易过度拟合只包含一个或两个示例的数据集。但是，用数百万个例子过度拟合数据集需要一个非常灵活的模型。\\n+\\n+## 选型\\n+\\n+在机器学习中，我们通常在评估几个候选模型后选择最终模型。此过程称为*型号选择*。有时，需要比较的模型本质上是不同的（比如，决策树与线性模型）。其他时候，我们比较的是同一类模型的成员，这些模型已经过不同的超参数设置训练。\\n+\\n+例如，对于mlp，我们可能希望比较具有不同数量的隐藏层、不同数量的隐藏单元以及应用于每个隐藏层的激活函数的各种选择的模型。为了确定候选模型中的最佳模型，我们通常会使用一个验证数据集。\\n+\\n+### 验证数据集\\n+\\n+原则上，在我们选择了所有的超参数之后，我们才应该接触我们的测试集。如果我们在模型选择过程中使用测试数据，则有可能过度拟合测试数据。那我们就麻烦大了。如果我们过度拟合我们的训练数据，总会有对测试数据的评估来保持我们的诚实。但是如果我们过度拟合测试数据，我们怎么会知道呢？\\n+\\n+因此，我们决不能依赖试验数据来选择模型。然而，我们也不能仅仅依靠训练数据来选择模型，因为我们不能根据我们用来训练模型的数据来估计泛化误差。\\n+\\n+在实际应用中，图像变得更加模糊。虽然理想情况下我们只接触一次测试数据，以评估最佳模型或将少量模型相互比较，但实际测试数据很少会在一次使用后被丢弃。我们很少能负担得起每一轮实验的新测试设备。\\n+\\n+解决这个问题的常见做法是将我们的数据分成三种方式，除了训练和测试数据集外，还合并一个*验证数据集*（或*验证集*）。结果是一个模糊的实践，验证和测试数据之间的界限令人担忧地模糊不清。除非另有明确说明，在这本书中的实验中，我们实际使用的是正确的训练数据和验证数据，没有真正的测试集。因此，本书每次实验报告的准确度实际上是验证准确度，而不是真实的测试集准确度。\\n+\\n+### $K$折叠交叉验证\\n+\\n+当训练数据不足时，我们甚至可能无法提供足够的数据来构成一个适当的验证集。解决这个问题的一个流行的解决方案是使用$K$*折叠交叉验证*。在这里，原始训练数据被分成$K$个不重叠的子集。然后进行$K$次模型训练和验证，每次训练$K-1$个子集，并在不同的子集上验证（该轮中没有用于训练的子集）。最后，通过对$K$个实验结果的平均值来估计训练和验证误差。\\n+\\n+## 不合身还是过度合身？\\n+\\n+当我们比较培训和验证错误时，我们需要注意两种常见情况。首先，我们希望注意这样的情况：我们的培训错误和验证错误都是巨大的，但它们之间有一点差距。如果模型不能减少训练误差，这可能意味着我们的模型过于简单（即，表达能力不足），无法捕捉我们试图建模的模式。此外，由于我们的训练和验证误差之间的“泛化差距”很小，我们有理由相信我们可以用一个更复杂的模型逃脱惩罚。这种现象被称为“不合身”。\\n+\\n+另一方面，正如我们上面所讨论的，当我们的训练误差明显低于我们的验证误差时，我们要注意这种情况，这表明严重的“过度拟合”。请注意，过拟合并不总是坏事。特别是在深入学习的情况下，众所周知，最好的预测模型在训练数据上的表现往往比在保留数据上表现得好得多。最终，我们通常更关心验证错误，而不是培训和验证错误之间的差距。\\n+\\n+我们是否过拟合或欠拟合都取决于模型的复杂性和可用训练数据集的大小，这两个主题将在下面讨论。\\n+\\n+### 模型复杂性\\n+\\n+为了说明一些关于过拟合和模型复杂性的经典直觉，我们给出了一个使用多项式的例子。给定由单一特征$x$和相应的实值标签$x$组成的训练数据，我们试图找到$d$次多项式\\n+\\n+$$\\\\hat{y}= \\\\sum_{i=0}^d x^i w_i$$\\n+\\n+估计标签$y$。这只是一个线性回归问题，我们的特征由$x$的幂次给出，模型的权重由$w_i$给出，而偏差是$x^0 = 1$以来的$x$。因为这是一个线性损失函数。\\n+\\n+高阶多项式函数比低阶多项式函数更复杂，因为高阶多项式具有更多的参数，且模型函数的选择范围更广。在固定训练数据集的情况下，高阶多项式函数相对于低阶多项式的训练误差总是较低的（最坏的情况是相等的）。事实上，当每个数据示例都有一个不同的值$x$时，一个次数等于数据示例数的多项式函数可以完美地拟合训练集。在:numref:`fig_capacity_vs_error`中，我们将多项式次数与欠拟合与过拟合之间的关系可视化。\\n+\\n+![Influence of model complexity on underfitting and overfitting](../img/capacity_vs_error.svg)\\n+:label:`fig_capacity_vs_error`\\n+\\n+### 数据集大小\\n+\\n+另一个需要记住的重要因素是数据集的大小。修正我们的模型，训练数据集中的样本越少，我们就越有可能（而且更严重）遇到过度拟合。随着训练数据量的增加，泛化误差通常会减小。此外，总的来说，更多的数据不会造成伤害。对于固定的任务和数据分布，模型复杂度和数据集大小之间通常存在关系。如果有更多的数据，我们可以尝试拟合一个更复杂的模型。如果没有足够的数据，更简单的模型可能更难被击败。对于许多任务，深度学习只有在成千上万个训练实例可用时才能优于线性模型。在一定程度上，深度学习目前的成功归功于互联网公司、廉价存储、联网设备以及经济的广泛数字化所带来的海量数据集。\\n+\\n+## 多项式回归\\n+\\n+我们现在可以通过拟合多项式来交互式地研究这些概念。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import gluon, np, npx\\n+from mxnet.gluon import nn\\n+import math\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+import numpy as np\\n+import math\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+import numpy as np\\n+import math\\n+```\\n+\\n+### 生成数据集\\n+\\n+首先我们需要数据。给定$x$，我们将使用以下三次多项式生成训练和测试数据上的标签：\\n+\\n+$$y = 5 + 1.2x - 3.4\\\\frac{x^2}{2!} + 5.6 \\\\frac{x^3}{3!} + \\\\epsilon \\\\text{ where }\\n+\\\\epsilon \\\\sim \\\\mathcal{N}(0, 0.1^2).$$\\n+\\n+噪声项$\\\\epsilon$服从正态分布，平均值为0，标准偏差为0.1。对于优化，我们通常希望避免非常大的梯度值或损失。这就是为什么*features*从$x^i$重新缩放到$\\\\frac{x^i}{i！}$. 它允许我们避免大指数$i$的很大值。我们将合成训练集和测试集各100个样本。\\n+\\n+```{.python .input}\\n+#@tab all\\n+max_degree = 20  # Maximum degree of the polynomial\\n+n_train, n_test = 100, 100  # Training and test dataset sizes\\n+true_w = np.zeros(max_degree)  # Allocate lots of empty space\\n+true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\\n+\\n+features = np.random.normal(size=(n_train + n_test, 1))\\n+np.random.shuffle(features)\\n+poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))\\n+for i in range(max_degree):\\n+    poly_features[:, i] /= math.gamma(i + 1)  # `gamma(n)` = (n-1)!\\n+# Shape of `labels`: (`n_train` + `n_test`,)\\n+labels = np.dot(poly_features, true_w)\\n+labels += np.random.normal(scale=0.1, size=labels.shape)\\n+```\\n+\\n+同样，存储在`poly_features`中的单项式被gamma函数重新缩放，其中$\\\\gamma（n）=（n-1）！$. 从生成的数据集中查看前2个示例。值1在技术上是一个特征，即与偏差相对应的常量特征。\\n+\\n+```{.python .input}\\n+#@tab pytorch, tensorflow\\n+# Convert from NumPy ndarrays to tensors\\n+true_w, features, poly_features, labels = [d2l.tensor(x, dtype=\\n+    d2l.float32) for x in [true_w, features, poly_features, labels]]\\n+```\\n+\\n+```{.python .input}\\n+#@tab all\\n+features[:2], poly_features[:2, :], labels[:2]\\n+```\\n+\\n+### 培训和测试模型\\n+\\n+让我们首先实现一个函数来评估给定数据集的损失。\\n+\\n+```{.python .input}\\n+#@tab mxnet, tensorflow\\n+def evaluate_loss(net, data_iter, loss):  #@save\\n+    \"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\\n+    metric = d2l.Accumulator(2)  # Sum of losses, no. of examples\\n+    for X, y in data_iter:\\n+        l = loss(net(X), y)\\n+        metric.add(d2l.reduce_sum(l), d2l.size(l))\\n+    return metric[0] / metric[1]\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def evaluate_loss(net, data_iter, loss):  #@save\\n+    \"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\\n+    metric = d2l.Accumulator(2)  # Sum of losses, no. of examples\\n+    for X, y in data_iter:\\n+        out = net(X)\\n+        y = d2l.reshape(y, out.shape)\\n+        l = loss(out, y)\\n+        metric.add(d2l.reduce_sum(l), d2l.size(l))\\n+    return metric[0] / metric[1]\\n+```\\n+\\n+现在定义培训功能。\\n+\\n+```{.python .input}\\n+def train(train_features, test_features, train_labels, test_labels,\\n+          num_epochs=400):\\n+    loss = gluon.loss.L2Loss()\\n+    net = nn.Sequential()\\n+    # Switch off the bias since we already catered for it in the polynomial\\n+    # features\\n+    net.add(nn.Dense(1, use_bias=False))\\n+    net.initialize()\\n+    batch_size = min(10, train_labels.shape[0])\\n+    train_iter = d2l.load_array((train_features, train_labels), batch_size)\\n+    test_iter = d2l.load_array((test_features, test_labels), batch_size,\\n+                               is_train=False)\\n+    trainer = gluon.Trainer(net.collect_params(), \\'sgd\\',\\n+                            {\\'learning_rate\\': 0.01})\\n+    animator = d2l.Animator(xlabel=\\'epoch\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],\\n+                            legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        d2l.train_epoch_ch3(net, train_iter, loss, trainer)\\n+        if epoch == 0 or (epoch + 1) % 20 == 0:\\n+            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),\\n+                                     evaluate_loss(net, test_iter, loss)))\\n+    print(\\'weight:\\', net[0].weight.data().asnumpy())\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def train(train_features, test_features, train_labels, test_labels,\\n+          num_epochs=400):\\n+    loss = nn.MSELoss()\\n+    input_shape = train_features.shape[-1]\\n+    # Switch off the bias since we already catered for it in the polynomial\\n+    # features\\n+    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))\\n+    batch_size = min(10, train_labels.shape[0])\\n+    train_iter = d2l.load_array((train_features, train_labels.reshape(-1,1)),\\n+                                batch_size)\\n+    test_iter = d2l.load_array((test_features, test_labels.reshape(-1,1)),\\n+                               batch_size, is_train=False)\\n+    trainer = torch.optim.SGD(net.parameters(), lr=0.01)\\n+    animator = d2l.Animator(xlabel=\\'epoch\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],\\n+                            legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        d2l.train_epoch_ch3(net, train_iter, loss, trainer)\\n+        if epoch == 0 or (epoch + 1) % 20 == 0:\\n+            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),\\n+                                     evaluate_loss(net, test_iter, loss)))\\n+    print(\\'weight:\\', net[0].weight.data.numpy())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def train(train_features, test_features, train_labels, test_labels,\\n+          num_epochs=400):\\n+    loss = tf.losses.MeanSquaredError()\\n+    input_shape = train_features.shape[-1]\\n+    # Switch off the bias since we already catered for it in the polynomial\\n+    # features\\n+    net = tf.keras.Sequential()\\n+    net.add(tf.keras.layers.Dense(1, use_bias=False))\\n+    batch_size = min(10, train_labels.shape[0])\\n+    train_iter = d2l.load_array((train_features, train_labels), batch_size)\\n+    test_iter = d2l.load_array((test_features, test_labels), batch_size,\\n+                               is_train=False)\\n+    trainer = tf.keras.optimizers.SGD(learning_rate=.01)\\n+    animator = d2l.Animator(xlabel=\\'epoch\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],\\n+                            legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        d2l.train_epoch_ch3(net, train_iter, loss, trainer)\\n+        if epoch == 0 or (epoch + 1) % 20 == 0:\\n+            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),\\n+                                     evaluate_loss(net, test_iter, loss)))\\n+    print(\\'weight:\\', net.get_weights()[0].T)\\n+```\\n+\\n+### 三阶多项式函数拟合（正态）\\n+\\n+我们将首先使用三阶多项式函数，它与数据生成函数的阶数相同。结果表明，该模型能有效地减少训练损失和测试损失。学习的模型参数也接近真实值$w = [5, 1.2, -3.4, 5.6]$。\\n+\\n+```{.python .input}\\n+#@tab all\\n+# Pick the first four dimensions, i.e., 1, x, x^2/2!, x^3/3! from the\\n+# polynomial features\\n+train(poly_features[:n_train, :4], poly_features[n_train:, :4],\\n+      labels[:n_train], labels[n_train:])\\n+```\\n+\\n+### 线性函数拟合（欠拟合）\\n+\\n+让我们再来看看线性函数拟合。在经历了早期衰退之后，要进一步降低这种模型的训练损失就变得困难了。最后一次历元迭代完成后，训练损失仍然很大。当用于拟合非线性模式（如这里的三阶多项式函数）时，线性模型容易欠拟合。\\n+\\n+```{.python .input}\\n+#@tab all\\n+# Pick the first two dimensions, i.e., 1, x, from the polynomial features\\n+train(poly_features[:n_train, :2], poly_features[n_train:, :2],\\n+      labels[:n_train], labels[n_train:])\\n+```\\n+\\n+### 高阶多项式函数拟合（过拟合）\\n+\\n+现在让我们试着用高次多项式来训练模型。这里，没有足够的数据来了解高次系数的值应该接近于零。因此，我们过于复杂的模型非常容易受到训练数据中噪声的影响。虽然可以有效地减少训练损失，但测试损失仍然很大。结果表明，复杂模型对数据拟合过度。\\n+\\n+```{.python .input}\\n+#@tab all\\n+# Pick all the dimensions from the polynomial features\\n+train(poly_features[:n_train, :], poly_features[n_train:, :],\\n+      labels[:n_train], labels[n_train:], num_epochs=1500)\\n+```\\n+\\n+在后面的章节中，我们将继续讨论过度拟合的问题和处理它们的方法，例如重量衰减和脱落。\\n+\\n+## 摘要\\n+\\n+* 由于不能根据训练误差来估计泛化误差，所以简单地最小化训练误差并不一定意味着泛化误差的减少。机器学习模型需要注意防止过度拟合，以尽量减少泛化误差。\\n+* 一个验证集可以用于模型选择，前提是它的使用不是太自由。\\n+* 欠拟合意味着模型不能减少训练误差。当训练误差远小于验证误差时，存在过拟合现象。\\n+* 应适当选择样本不足的样本进行训练。\\n+\\n+## 练习\\n+\\n+1. 你能准确地解决多项式回归问题吗？提示：使用线性代数。\\n+1. 考虑多项式的模型选择：\\n+    1. 绘制训练损失与模型复杂度（多项式次数）的关系图。你观察到了什么？将训练损失减少到0需要多少次多项式？\\n+    1. 在这种情况下绘制测试损失图。\\n+    1. 生成与数据量函数相同的绘图。\\n+1. 如果你放弃正常化（$1/i）会怎么样！$) of the polynomial features $x^i$？你能用别的方法解决这个问题吗？\\n+1. 你能期望看到零泛化误差吗？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/96)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/97)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/234)\\n+:end_tab:\\ndiff --git a/chapter_multilayer-perceptrons/underfit-overfit_tencent.md b/chapter_multilayer-perceptrons/underfit-overfit_tencent.md\\nnew file mode 100644\\nindex 00000000..cf3dba05\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/underfit-overfit_tencent.md\\n@@ -0,0 +1,343 @@\\n+# 模型选择、不足拟合和过度拟合\\n+:label:`sec_model_selection`\\n+\\n+作为机器学习科学家，我们的目标是发现*模式*。但是，我们如何才能确定我们已经真正发现了一种“一般”模式，而不是简单地记住了我们的数据呢？例如，假设我们想要在将患者与他们的痴呆症状态联系起来的遗传标记中寻找模式，其中标签是从集合$\\\\{\\\\text{dementia}, \\\\text{mild cognitive impairment}, \\\\text{healthy}\\\\}$中提取的。因为每个人的基因都是唯一识别他们的(忽略完全相同的兄弟姐妹)，所以有可能记住整个数据集。\\n+\\n+我们不想让我们的模型说\\n+*“那是鲍勃！我记得他！他有痴呆症！”*\\n+原因很简单。当我们将来部署该模型时，我们会遇到该模型从未见过的患者。只有当我们的模型真正发现了一种“一般”模式时，我们的预测才会有用。\\n+\\n+更正式地说，我们的目标是发现模式，这些模式捕捉到了我们训练集所来自的潜在人群中的规律性。如果我们在这一努力中取得成功，那么我们就可以成功地评估风险，即使是对我们以前从未遇到过的个人来说也是如此。这个问题-如何发现“泛化”的模式-是机器学习的基本问题。\\n+\\n+危险在于，当我们训练模型时，我们只能访问一小部分数据样本。最大的公共图像数据集包含大约一百万张图像。更多时候，我们只能从数千或数万个数据例子中学习。在大型医院系统中，我们可能会访问数十万份医疗记录。在处理有限的样本时，我们可能会冒着这样的风险，即当我们收集更多数据时，我们可能会发现明显的关联，而这些关联最终证明是站不住脚的。\\n+\\n+将训练数据拟合得比潜在分布更接近的现象称为“过度拟合”，用于对抗过度拟合的技术称为“正则化”。在前面的部分中，您可能已经在试验Fashion-MNIST数据集时观察到了这种效果。如果在实验期间更改了模型结构或超参数，您可能已经注意到，如果有足够的神经元、层和训练周期，即使测试数据的准确性下降，模型最终也可以在训练集上达到完美的精度。\\n+\\n+## 训练误差和泛化误差\\n+\\n+为了更正式地讨论这一现象，我们需要区分训练误差和泛化误差。*训练误差*是我们的模型在训练数据集上计算的误差，而*推广误差*是我们将其应用于从与原始样本相同的底层数据分布中提取的无限多个附加数据示例的情况下模型误差的预期。\\n+\\n+从问题上讲，我们永远不能准确地计算出泛化误差。这是因为无限数据流是一个虚构的对象。在实践中，我们必须通过将我们的模型应用于一个独立的测试集来“估计”泛化误差，该测试集由从我们的训练集中保留的随机选择的数据示例组成。\\n+\\n+下面的三个思维实验将有助于更好地说明这种情况。假设一个大学生正在努力准备期末考试。一个勤奋的学生会努力练习好，并利用往年的考试来测试自己的能力。尽管如此，在过去的考试中取得好成绩并不能保证他会在重要的时候出类拔萃。例如，学生可能试图通过死记硬背考题的答案来做准备。这需要学生记住很多东西。她甚至可以完全记住过去考试的答案。另一名学生可能会通过试图理解给出某些答案的原因来做准备。在大多数情况下，后一个学生会做得更好。\\n+\\n+同样，考虑一个简单地使用查找表回答问题的模型。如果允许的输入集合是离散的并且相当小，那么也许在查看*许多**训练示例之后，该方法将执行得很好。尽管如此，当面对它从未见过的例子时，这个模型没有比随机猜测更好的能力了。实际上，输入空间太大了，无法记住对应于每一个可以想到的输入的答案。例如，考虑黑白$28\\\\times28$图像。如果每个像素可以取$256$个灰度值中的一个，则有$256^{784}$个可能的图像。这意味着低分辨率的灰度缩略图大小的图像比宇宙中的原子要多得多。即使我们可以遇到这样的数据，我们也永远负担不起存储查找表的费用。\\n+\\n+最后，考虑尝试根据一些可能可用的上下文特征对掷硬币的结果(0类：正面，1类：反面)进行分类的问题。假设硬币是公平的。无论我们想出什么算法，泛化误差始终是$\\\\frac{1}{2}$。然而，对于大多数算法，我们应该预期我们的训练误差会相当低，这取决于抽签的运气，即使我们没有任何功能！考虑数据集{0，1，1，1，0，1}。我们的无特征算法将不得不依赖于总是预测*多数类*，从我们有限的样本来看，它似乎是*1*。在这种情况下，总是预测类1的模型将产生$\\\\frac{1}{3}$的误差，这比我们的泛化误差要好得多。随着数据量的增加，头部比例明显偏离$\\\\frac{1}{2}$的可能性降低，我们的训练误差将与推广误差相匹配。\\n+\\n+### 统计学习理论\\n+\\n+由于泛化是机器学习中的基本问题，了解到许多数学家和理论家毕生致力于开发描述这一现象的形式理论，您可能不会感到惊讶。在他们的[同名theorem](https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem)，]中，格里文科和坎特利导出了训练误差收敛到泛化误差的速率。在一系列开创性的论文中，[Vapnik和Chervonenkis](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory)将这一理论扩展到更一般的函数类。这项工作为统计学习理论奠定了基础。\\n+\\n+在标准的监督学习设置中，我们到目前为止一直在讨论这一问题，并将在本书的大部分内容中坚持使用，我们假设训练数据和测试数据都是从*相同的*分布中“独立”提取的。这通常被称为*I.I.D.。假设*，这意味着对我们的数据进行采样的过程没有内存。换句话说，抽取的第二个示例和第三个样本并不比抽取的第二个样本和第200万个样本的相关性更强。\\n+\\n+要成为一名优秀的机器学习科学家需要批判性的思考，而且你应该已经在这个假设中找出漏洞，找出假设失败的常见情况。如果我们根据从加州大学旧金山分校医学中心的患者收集的数据培训死亡风险预报器，并将其应用于马萨诸塞州综合医院的患者，会怎么样？这些分布完全不一样。此外，抽签可能在时间上是相关的。如果我们对Tweet的主题进行分类呢？新闻周期会在正在讨论的话题中产生时间依赖性，这违反了任何独立的假设。\\n+\\n+有时候我们只要轻微违反身份证就可以逍遥法外。假设和我们的模型将继续运行得非常好。毕竟，几乎每个现实世界的申请都至少涉及到一些轻微的身份识别违规行为。然而，我们有许多有用的工具可用于各种应用，如人脸识别、语音识别和语言翻译。\\n+\\n+其他违规行为肯定会带来麻烦。例如，想象一下，如果我们试图训练一个人脸识别系统，只针对大学生进行培训，然后想要将其部署为一种工具，用于监测疗养院人口中的老年病。这不太可能起到很好的作用，因为大学生看起来往往与老年人有很大的不同。\\n+\\n+在接下来的章节中，我们将讨论因违反身份证而引起的问题。假设。目前，即使拿到身份证。假设是理所当然的，理解泛化是一个可怕的问题。此外，阐明可能解释为什么深层神经网络泛化得如此好的精确理论基础，继续困扰着学习理论中的最伟大的人。\\n+\\n+当我们训练我们的模型时，我们试图搜索一个尽可能符合训练数据的函数。如果该函数如此灵活，以至于它可以像捕捉真实关联一样容易地捕捉到虚假模式，那么它可能执行得“太好了”，而不会产生一个对看不见的数据进行很好概括的模型。这正是我们想要避免的，或者至少是想要控制的。深度学习中的许多技术都是启发式的和旨在防止过度适应的技巧。\\n+\\n+### 模型复杂性\\n+\\n+当我们有简单的模型和大量的数据时，我们期望泛化误差与训练误差相似。当我们处理更复杂的模型和更少的示例时，我们预计训练误差会下降，但泛化差距会增大。模型复杂性的确切构成是一个复杂的问题。一个模型是否能很好地推广，取决于很多因素。例如，具有更多参数的模型可能被认为更复杂。其参数可以采用更大范围值的模型可能更为复杂。通常，对于神经网络，我们认为需要更多训练迭代的模型比较复杂，而需要“提前停止”(较少训练迭代)的模型就不那么复杂。\\n+\\n+很难比较本质上不同模型类的成员之间的复杂性(例如，决策树与神经网络)。就目前而言，一条简单的经验法则相当有用：统计学家认为，能够轻松解释任意事实的模型是复杂的，而表达能力有限但仍能很好地解释数据的模型可能更接近真相。在哲学上，这与波普尔的科学理论的可证伪性标准密切相关：如果一个理论符合数据，如果有具体的测试可以用来证明它是错误的，那么它就是好的。这一点很重要，因为所有的统计估计都是\\n+*邮寄*，\\n+也就是说，我们在观察事实之后进行估计，因此容易受到相关谬误的影响。目前，我们将把哲学放在一边，坚持更切实的问题。\\n+\\n+在本节中，为了给您一些直观的印象，我们将重点介绍几个倾向于影响模型类的通用性的因素：\\n+\\n+1. 可调参数的数量。当可调参数(有时称为*自由度*)的数量很大时，模型往往更容易过度拟合。\\n+1. 参数采用的值。当权重的取值范围较大时，模型可能更容易过度拟合。\\n+1. 训练样例的数量。即使您的模型很简单，覆盖只包含一个或两个示例的数据集也是非常容易的。但是，用数百万个示例来过度拟合一个数据集需要一个极其灵活的模型。\\n+\\n+## 选型\\n+\\n+在机器学习中，我们通常在评估几个候选模型后选择最终的模型。这个过程叫做“选型”。有时，需要进行比较的模型在本质上是完全不同的(比如，决策树与线性模型)。在其他时间，我们比较已经用不同的超参数设置训练的同一类模型的成员。\\n+\\n+例如，对于MLP，我们可能希望比较具有不同数量的隐藏层、不同数量的隐藏单元以及应用于每个隐藏层的激活函数的各种选择的模型。为了确定候选模型中的最佳模型，我们通常会使用验证数据集。\\n+\\n+### 验证数据集\\n+\\n+原则上，在我们选择了所有的超参数之前，我们不应该接触我们的测试集。如果我们在模型选择过程中使用测试数据，可能会有过度拟合测试数据的风险。那我们就麻烦大了。如果我们过度匹配我们的训练数据，总会有对测试数据的评估来保持我们的诚实。但是如果我们过度拟合测试数据，我们怎么知道呢？\\n+\\n+因此，我们决不能依赖试验数据进行模型选择。然而，我们也不能仅仅依靠训练数据来选择模型，因为我们不能估计我们用来训练模型的数据的泛化误差。\\n+\\n+在实际应用中，情况变得更加模糊。虽然理想情况下我们只会触摸测试数据一次，以评估最好的模型或将少数模型相互比较，但现实世界的测试数据很少在使用一次后被丢弃。我们很少能负担得起每一轮实验的新测试集。\\n+\\n+解决此问题的常见做法是将我们的数据分成三种方式，除了训练和测试数据集之外，还合并一个*验证数据集*(或*验证集*)。结果是一种模糊的实践，验证和测试数据之间的边界模糊得令人担忧。除非另有明确说明，否则在这本书的实验中，我们实际上是在使用应该被正确地称为训练数据和验证数据的东西，没有真正的测试集。因此，书中每次实验报告的准确度都是真正的验证准确度，而不是真正的测试集准确度。\\n+\\n+### $K$倍交叉验证\\n+\\n+当训练数据稀缺时，我们甚至可能没有能力提供足够的数据来构成一个适当的验证集。这个问题的一个流行的解决方案是采用$K$倍交叉验证*。这里，原始训练数据被分成$K$个不重叠的子集。然后执行$K$次模型训练和验证，每次在$K-1$个子集上进行训练，并在不同的子集(在该轮中没有用于训练的子集)上进行验证。最后，通过对$K$个实验的结果进行平均来估计训练和验证误差。\\n+\\n+## 不太合身还是太合身？\\n+\\n+当我们比较训练和验证错误时，我们要注意两种常见的情况。首先，我们要注意这样的情况：我们的训练错误和验证错误都很严重，但它们之间有一点差距。如果模型不能减少训练错误，这可能意味着我们的模型过于简单(即，表达能力不足)，无法捕获我们试图建模的模式。此外，由于我们的训练和验证错误之间的“泛化差距”很小，我们有理由相信我们可以用一个更复杂的模型逃脱惩罚。这种现象被称为“不合身”。\\n+\\n+另一方面，正如我们上面所讨论的，当我们的训练误差明显低于我们的验证误差，表明严重的“过度拟合”时，我们要注意这种情况。请注意，过度贴身并不总是一件坏事。特别是在深度学习方面，众所周知，最好的预测模型在训练数据上的表现往往比在抵抗数据上好得多。最终，我们通常更关心验证错误，而不是训练错误和验证错误之间的差距。\\n+\\n+我们是否过度匹配可能取决于我们模型的复杂性和可用训练数据集的大小，这两个主题将在下面讨论。\\n+\\n+### 模型复杂性\\n+\\n+为了说明一些关于过拟合和模型复杂性的经典直觉，我们用多项式给出了一个例子。给定由单个特征$x$和对应的实值标签$y$组成的训练数据，我们试图找到次数为$d$的多项式\\n+\\n+$$\\\\hat{y}= \\\\sum_{i=0}^d x^i w_i$$\\n+\\n+以估计标签$y$。这只是一个线性回归问题，我们的特征是$x$的幂给出的，模型的权重是$w_i$给出的，偏差是$w_0$给出的，因为所有的$x$都是$x^0 = 1$。由于这只是一个线性回归问题，我们可以使用平方误差作为我们的损失函数。\\n+\\n+由于高次多项式的参数较多，模型函数的选择范围较广，因此高次多项式函数比低次多项式函数复杂得多。在固定训练数据集的情况下，高次多项式函数相对于低次多项式的训练误差应该始终更低(最坏情况下是相等的)。事实上，只要每个数据样本都有$x$的不同值，次数等于数据样本数量的多项式函数就可以很好地拟合训练集。在:numref:`fig_capacity_vs_error`中，我们直观地描述了多项式次数和欠拟合与过拟合之间的关系。\\n+\\n+![Influence of model complexity on underfitting and overfitting](../img/capacity_vs_error.svg)\\n+:label:`fig_capacity_vs_error`\\n+\\n+### 数据集大小\\n+\\n+另一个需要牢记的重要因素是数据集的大小。修正我们的模型，训练数据集中的样本越少，我们遇到过拟合的可能性(也就越严重)。随着训练数据量的增加，泛化误差通常会减小。此外，一般来说，更多的数据不会有什么坏处。对于固定的任务和数据分布，通常在模型复杂性和数据集大小之间存在关系。给出更多的数据，我们可能会尝试拟合一个更复杂的模型，这可能是有益的。如果没有足够的数据，简单的模型可能更难击败。对于许多任务，深度学习只有在有数千个训练示例可用时才优于线性模型。在一定程度上，深度学习目前的成功要归功于互联网公司、廉价存储、互联设备以及广泛的经济数字化带来的海量数据集。\\n+\\n+## 多项式回归\\n+\\n+我们现在可以通过将多项式拟合到数据来交互地探索这些概念。\\n+\\n+```{.python .input}\\n+from d2l import mxnet as d2l\\n+from mxnet import gluon, np, npx\\n+from mxnet.gluon import nn\\n+import math\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+from d2l import torch as d2l\\n+import torch\\n+from torch import nn\\n+import numpy as np\\n+import math\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+import numpy as np\\n+import math\\n+```\\n+\\n+### 生成数据集\\n+\\n+首先，我们需要数据。给定$x$，我们将使用以下三次多项式来生成训练和测试数据的标签：\\n+\\n+$$y = 5 + 1.2x - 3.4\\\\frac{x^2}{2!} + 5.6 \\\\frac{x^3}{3!} + \\\\epsilon \\\\text{ where }\\n+\\\\epsilon \\\\sim \\\\mathcal{N}(0, 0.1^2).$$\\n+\\n+噪声项$\\\\epsilon$服从均值为0且标准差为0.1的正态分布。对于优化，我们通常希望避免非常大的渐变值或损失。这就是将*功能*从$x^i$重新调整为$\\\\frac{x^i}{i！}$的原因。它允许我们避免大指数$i$的非常大的值。我们将为训练集和测试集各合成100个样本。\\n+\\n+```{.python .input}\\n+#@tab all\\n+max_degree = 20  # Maximum degree of the polynomial\\n+n_train, n_test = 100, 100  # Training and test dataset sizes\\n+true_w = np.zeros(max_degree)  # Allocate lots of empty space\\n+true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\\n+\\n+features = np.random.normal(size=(n_train + n_test, 1))\\n+np.random.shuffle(features)\\n+poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))\\n+for i in range(max_degree):\\n+    poly_features[:, i] /= math.gamma(i + 1)  # `gamma(n)` = (n-1)!\\n+# Shape of `labels`: (`n_train` + `n_test`,)\\n+labels = np.dot(poly_features, true_w)\\n+labels += np.random.normal(scale=0.1, size=labels.shape)\\n+```\\n+\\n+同样，存储在`poly_features`中的单项式由GAMMA函数重新缩放，其中$\\\\GAMMA(N)=(n-1)！$。看一下生成的数据集中的前2个样本。从技术上讲，值1是一个特征，即与偏置相对应的恒定特征。\\n+\\n+```{.python .input}\\n+#@tab pytorch, tensorflow\\n+# Convert from NumPy ndarrays to tensors\\n+true_w, features, poly_features, labels = [d2l.tensor(x, dtype=\\n+    d2l.float32) for x in [true_w, features, poly_features, labels]]\\n+```\\n+\\n+```{.python .input}\\n+#@tab all\\n+features[:2], poly_features[:2, :], labels[:2]\\n+```\\n+\\n+### 对模型进行培训和测试\\n+\\n+让我们首先实现一个函数来评估给定数据集的损失。\\n+\\n+```{.python .input}\\n+#@tab mxnet, tensorflow\\n+def evaluate_loss(net, data_iter, loss):  #@save\\n+    \"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\\n+    metric = d2l.Accumulator(2)  # Sum of losses, no. of examples\\n+    for X, y in data_iter:\\n+        l = loss(net(X), y)\\n+        metric.add(d2l.reduce_sum(l), d2l.size(l))\\n+    return metric[0] / metric[1]\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def evaluate_loss(net, data_iter, loss):  #@save\\n+    \"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\\n+    metric = d2l.Accumulator(2)  # Sum of losses, no. of examples\\n+    for X, y in data_iter:\\n+        out = net(X)\\n+        y = d2l.reshape(y, out.shape)\\n+        l = loss(out, y)\\n+        metric.add(d2l.reduce_sum(l), d2l.size(l))\\n+    return metric[0] / metric[1]\\n+```\\n+\\n+现在定义训练函数。\\n+\\n+```{.python .input}\\n+def train(train_features, test_features, train_labels, test_labels,\\n+          num_epochs=400):\\n+    loss = gluon.loss.L2Loss()\\n+    net = nn.Sequential()\\n+    # Switch off the bias since we already catered for it in the polynomial\\n+    # features\\n+    net.add(nn.Dense(1, use_bias=False))\\n+    net.initialize()\\n+    batch_size = min(10, train_labels.shape[0])\\n+    train_iter = d2l.load_array((train_features, train_labels), batch_size)\\n+    test_iter = d2l.load_array((test_features, test_labels), batch_size,\\n+                               is_train=False)\\n+    trainer = gluon.Trainer(net.collect_params(), \\'sgd\\',\\n+                            {\\'learning_rate\\': 0.01})\\n+    animator = d2l.Animator(xlabel=\\'epoch\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],\\n+                            legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        d2l.train_epoch_ch3(net, train_iter, loss, trainer)\\n+        if epoch == 0 or (epoch + 1) % 20 == 0:\\n+            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),\\n+                                     evaluate_loss(net, test_iter, loss)))\\n+    print(\\'weight:\\', net[0].weight.data().asnumpy())\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def train(train_features, test_features, train_labels, test_labels,\\n+          num_epochs=400):\\n+    loss = nn.MSELoss()\\n+    input_shape = train_features.shape[-1]\\n+    # Switch off the bias since we already catered for it in the polynomial\\n+    # features\\n+    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))\\n+    batch_size = min(10, train_labels.shape[0])\\n+    train_iter = d2l.load_array((train_features, train_labels.reshape(-1,1)),\\n+                                batch_size)\\n+    test_iter = d2l.load_array((test_features, test_labels.reshape(-1,1)),\\n+                               batch_size, is_train=False)\\n+    trainer = torch.optim.SGD(net.parameters(), lr=0.01)\\n+    animator = d2l.Animator(xlabel=\\'epoch\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],\\n+                            legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        d2l.train_epoch_ch3(net, train_iter, loss, trainer)\\n+        if epoch == 0 or (epoch + 1) % 20 == 0:\\n+            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),\\n+                                     evaluate_loss(net, test_iter, loss)))\\n+    print(\\'weight:\\', net[0].weight.data.numpy())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def train(train_features, test_features, train_labels, test_labels,\\n+          num_epochs=400):\\n+    loss = tf.losses.MeanSquaredError()\\n+    input_shape = train_features.shape[-1]\\n+    # Switch off the bias since we already catered for it in the polynomial\\n+    # features\\n+    net = tf.keras.Sequential()\\n+    net.add(tf.keras.layers.Dense(1, use_bias=False))\\n+    batch_size = min(10, train_labels.shape[0])\\n+    train_iter = d2l.load_array((train_features, train_labels), batch_size)\\n+    test_iter = d2l.load_array((test_features, test_labels), batch_size,\\n+                               is_train=False)\\n+    trainer = tf.keras.optimizers.SGD(learning_rate=.01)\\n+    animator = d2l.Animator(xlabel=\\'epoch\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],\\n+                            legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        d2l.train_epoch_ch3(net, train_iter, loss, trainer)\\n+        if epoch == 0 or (epoch + 1) % 20 == 0:\\n+            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),\\n+                                     evaluate_loss(net, test_iter, loss)))\\n+    print(\\'weight:\\', net.get_weights()[0].T)\\n+```\\n+\\n+### 三次多项式函数拟合(正态)\\n+\\n+我们将首先使用三阶多项式函数，它与数据生成函数的阶数相同。结果表明，该模型能有效降低训练损失和测试损失。学习的模型参数也接近真值$w = [5, 1.2, -3.4, 5.6]$。\\n+\\n+```{.python .input}\\n+#@tab all\\n+# Pick the first four dimensions, i.e., 1, x, x^2/2!, x^3/3! from the\\n+# polynomial features\\n+train(poly_features[:n_train, :4], poly_features[n_train:, :4],\\n+      labels[:n_train], labels[n_train:])\\n+```\\n+\\n+### 线性函数拟合(欠拟合)\\n+\\n+让我们再看看线性函数拟合。在经历了早期的衰落之后，进一步减少该模式的训练损失变得困难起来。在最后一个历元迭代完成后，训练损失仍然很高。当用来拟合非线性模式(如这里的三次多项式函数)时，线性模型容易拟合不足。\\n+\\n+```{.python .input}\\n+#@tab all\\n+# Pick the first two dimensions, i.e., 1, x, from the polynomial features\\n+train(poly_features[:n_train, :2], poly_features[n_train:, :2],\\n+      labels[:n_train], labels[n_train:])\\n+```\\n+\\n+### 高次多项式函数拟合(过拟合)\\n+\\n+现在，让我们尝试使用过高的多项式来训练模型。这里，没有足够的数据来了解高次系数应该具有接近于零的值。因此，我们的过于复杂的模型是如此敏感，以至于它受到训练数据中的噪声的影响。虽然训练损失可以有效地降低，但测试损失仍然很高。结果表明，复杂模型对数据的拟合效果较差。\\n+\\n+```{.python .input}\\n+#@tab all\\n+# Pick all the dimensions from the polynomial features\\n+train(poly_features[:n_train, :], poly_features[n_train:, :],\\n+      labels[:n_train], labels[n_train:], num_epochs=1500)\\n+```\\n+\\n+在接下来的章节中，我们将继续讨论过度安装的问题和处理这些问题的方法，例如体重衰减和辍学。\\n+\\n+## 摘要\\n+\\n+* 由于不能基于训练误差来估计泛化误差，因此简单地最小化训练误差并不一定意味着泛化误差的减小。机器学习模型需要注意防止过拟合，以使泛化误差最小化。\\n+* 验证集可以用于模型选择，前提是不能过于随意地使用它。\\n+* 欠拟合是指模型不能减少训练误差。当训练误差远小于验证误差时，存在过拟合。\\n+* 我们应该选择一个适当复杂的模型，避免使用不足的训练样本。\\n+\\n+## 练习\\n+\\n+1. 你能准确地解决这个多项式回归问题吗？提示：使用线性代数。\\n+1. 考虑多项式的模型选择：\\n+    1. 绘制训练损失与模型复杂度(多项式的次数)的关系图。你观察到了什么？您需要多项式的次数才能将训练损失减少到0？\\n+    1. 在这种情况下绘制测试损失图。\\n+    1. 根据数据量生成相同的曲线图。\\n+1. 如果您放弃规范化($1/i！$) of the polynomial features $x^i$？你能用其他方法解决这个问题吗？\\n+1. 您能期望看到零泛化错误吗？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/96)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/97)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/234)\\n+:end_tab:\\ndiff --git a/chapter_multilayer-perceptrons/weight-decay_baidu.md b/chapter_multilayer-perceptrons/weight-decay_baidu.md\\nnew file mode 100644\\nindex 00000000..5b3b66bc\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/weight-decay_baidu.md\\n@@ -0,0 +1,366 @@\\n+# 重量衰减\\n+:label:`sec_weight_decay`\\n+\\n+既然我们已经描述了过度拟合的问题，我们可以介绍一些标准的正则化模型的技术。回想一下，我们总是可以通过外出收集更多的培训数据来缓解过度适应。这可能是昂贵的，耗时的，或者完全失去我们的控制，这在短期内是不可能的。现在，我们可以假设我们已经拥有尽可能多的高质量数据，并将重点放在正则化技术上。\\n+\\n+回想一下，在我们的多项式回归示例（:numref:`sec_model_selection`）中，我们可以通过调整拟合多项式的次数来限制模型的容量。实际上，限制特性的数量是一种流行的技术，可以减轻过度拟合。然而，简单地放弃特性对于这项工作来说可能过于生硬。坚持多项式回归的例子，考虑高维输入可能会发生什么。多项式对多元数据的自然扩展称为*单项式*，它只是变量幂的乘积。单项式的度是幂和。例如，$x_1^2 x_2$和$x_3 x_5^2$都是3阶的单项式。\\n+\\n+请注意，随着$d$的增长，$d$度的项数迅速增加。给定$k$个变量，阶数$d$（即$k$多选$d$）的个数为${k - 1 + d} \\\\choose {k - 1}$。即使是程度上的微小变化，比如从$2$到$3$，也会显著增加我们模型的复杂性。因此，我们经常需要一个更细粒度的工具来调整函数的复杂性。\\n+\\n+## 规范与权重衰减\\n+\\n+我们已经描述了$L_2$规范和$L_1$规范，它们是$L_p$规范在$L_p$中的特殊情况。\\n+*权重衰减*（通常称为$L_2$正则化），\\n+可能是正则化参数化机器学习模型最广泛使用的技术。这项技术是基于一个基本直觉，即在所有函数$f$中，函数$f = 0$（为所有输入赋值$0$）在某种意义上是最简单的，我们可以通过函数与零的距离来衡量函数的复杂度。但是我们应该如何精确地测量一个函数和零之间的距离呢？没有一个正确的答案。事实上，整个数学分支，包括函数分析和Banach空间理论，都致力于回答这个问题。\\n+\\n+一种简单的解释可以是通过线性函数$f(\\\\mathbf{x}) = \\\\mathbf{w}^\\\\top \\\\mathbf{x}$的权向量的某个范数来度量其复杂性，例如$\\\\| \\\\mathbf{w} \\\\|^2$。保证小权向量最常用的方法是将其范数作为惩罚项加到损失最小化的问题中。这样我们就取代了原来的目标，\\n+*最小化训练标签上的预测损失*，\\n+有了新的目标，\\n+*最小化预测损失和惩罚项*之和。\\n+现在，如果我们的权重向量增长太大，我们的学习算法可能会集中在最小化权重范数$\\\\| \\\\mathbf{w} \\\\|^2$与最小化训练误差。这正是我们想要的。为了在代码中说明一些事情，让我们重温一下:numref:`sec_linear_regression`中的线性回归示例。我们的损失是由\\n+\\n+$$L(\\\\mathbf{w}, b) = \\\\frac{1}{n}\\\\sum_{i=1}^n \\\\frac{1}{2}\\\\left(\\\\mathbf{w}^\\\\top \\\\mathbf{x}^{(i)} + b - y^{(i)}\\\\right)^2.$$\\n+\\n+回想一下$\\\\mathbf{x}^{(i)}$是特征，$y^{(i)}$是所有数据的标签示例$i$和$(\\\\mathbf{w}, b)$分别是权重和偏差参数。为了惩罚权重向量的大小，我们必须在损失函数中添加$\\\\| \\\\mathbf{w} \\\\|^2$，但是模型应该如何权衡标准损失来获得新的附加惩罚呢？实际上，我们通过*正则化常数*$\\\\lambda$来描述这种权衡，这是一个非负超参数，我们使用验证数据拟合：\\n+\\n+$$L(\\\\mathbf{w}, b) + \\\\frac{\\\\lambda}{2} \\\\|\\\\mathbf{w}\\\\|^2,$$\\n+\\n+对于$\\\\lambda = 0$，我们恢复了原来的损失函数。对于$\\\\lambda > 0$，我们限制$\\\\| \\\\mathbf{w} \\\\|$的大小。我们按照惯例除以$2$：当我们取一个二次函数的导数时，$2$和$1/2$会相消，以确保更新表达式看起来既漂亮又简单。精明的读者可能会想知道为什么我们使用平方范数而不是标准范数（即欧几里得距离）。我们这样做是为了便于计算。通过平方$L_2$范数，我们去掉平方根，留下权重向量每个分量的平方和。这使得惩罚的导数很容易计算：导数的和等于和的导数。\\n+\\n+此外，您可能会问为什么我们首先使用$L_2$规范，而不是$L_1$规范。事实上，其他选择在整个统计数据中都是有效的和受欢迎的。当$L_2$正则化线性模型构成经典的*岭回归*算法时，$L_1$正则化线性回归是统计学中类似的基本模型，通常被称为*套索回归*。\\n+\\n+使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。在实践中，这可能使它们对单个变量中的测量误差更为稳健。相比之下，$L_1$惩罚会导致模型通过将其他权重清除为零而将权重集中在一小部分特性上。这称为*特征选择*，这可能是由于其他原因而需要的。\\n+\\n+使用:eqref:`eq_linreg_batch_update`中的相同符号，$L_2$正则化回归的小批量随机梯度下降更新如下：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+\\\\mathbf{w} & \\\\leftarrow \\\\left(1- \\\\eta\\\\lambda \\\\right) \\\\mathbf{w} - \\\\frac{\\\\eta}{|\\\\mathcal{B}|} \\\\sum_{i \\\\in \\\\mathcal{B}} \\\\mathbf{x}^{(i)} \\\\left(\\\\mathbf{w}^\\\\top \\\\mathbf{x}^{(i)} + b - y^{(i)}\\\\right).\\n+\\\\end{aligned}\\n+$$\\n+\\n+如前所述，我们根据我们的估计值与观察值之间的差异来更新$\\\\mathbf{w}$。然而，我们也将$\\\\mathbf{w}$的大小缩小到零。这就是为什么这种方法有时被称为“权重衰减”：仅考虑惩罚项，我们的优化算法*在训练的每一步*衰减*权重。与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。较小的$\\\\lambda$值对应较少约束的$\\\\mathbf{w}$，而较大的$\\\\lambda$值对$\\\\mathbf{w}$的约束更大。\\n+\\n+我们是否包括相应的偏差惩罚$b^2$可以在不同的实现中变化，并且可以在神经网络的各个层之间变化。通常，我们不正则化网络输出层的偏差项。\\n+\\n+## 高维线性回归\\n+\\n+我们可以通过一个简单的例子来说明综合权重的衰减。\\n+\\n+```{.python .input}\\n+%matplotlib inline\\n+from d2l import mxnet as d2l\\n+from mxnet import autograd, gluon, init, np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+%matplotlib inline\\n+from d2l import torch as d2l\\n+import torch\\n+import torch.nn as nn\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+%matplotlib inline\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+```\\n+\\n+首先，我们像以前一样生成一些数据\\n+\\n+$$y = 0.05 + \\\\sum_{i = 1}^d 0.01 x_i + \\\\epsilon \\\\text{ where }\\n+\\\\epsilon \\\\sim \\\\mathcal{N}(0, 0.01^2).$$\\n+\\n+我们选择我们的标签是我们输入的线性函数，被高斯噪声破坏，平均值为零，标准偏差为0.01。为了使过度拟合的效果更加明显，我们可以将问题的维数增加到$d = 200$，并使用一个只包含20个示例的小训练集。\\n+\\n+```{.python .input}\\n+#@tab all\\n+n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5\\n+true_w, true_b = d2l.ones((num_inputs, 1)) * 0.01, 0.05\\n+train_data = d2l.synthetic_data(true_w, true_b, n_train)\\n+train_iter = d2l.load_array(train_data, batch_size)\\n+test_data = d2l.synthetic_data(true_w, true_b, n_test)\\n+test_iter = d2l.load_array(test_data, batch_size, is_train=False)\\n+```\\n+\\n+## 从头开始实施\\n+\\n+在下面，我们将从头开始实现权重衰减，只需将$L_2$的平方惩罚添加到原始目标函数中。\\n+\\n+### 初始化模型参数\\n+\\n+首先，我们将定义一个函数来随机初始化我们的模型参数。\\n+\\n+```{.python .input}\\n+def init_params():\\n+    w = np.random.normal(scale=1, size=(num_inputs, 1))\\n+    b = np.zeros(1)\\n+    w.attach_grad()\\n+    b.attach_grad()\\n+    return [w, b]\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def init_params():\\n+    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)\\n+    b = torch.zeros(1, requires_grad=True)\\n+    return [w, b]\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def init_params():\\n+    w = tf.Variable(tf.random.normal(mean=1, shape=(num_inputs, 1)))\\n+    b = tf.Variable(tf.zeros(shape=(1, )))\\n+    return [w, b]\\n+```\\n+\\n+### 定义$L_2$标准罚款\\n+\\n+也许实施这一处罚最方便的方法是把所有条款都放在适当的地方，并加以总结。\\n+\\n+```{.python .input}\\n+def l2_penalty(w):\\n+    return (w**2).sum() / 2\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def l2_penalty(w):\\n+    return torch.sum(w.pow(2)) / 2\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def l2_penalty(w):\\n+    return tf.reduce_sum(tf.pow(w, 2)) / 2\\n+```\\n+\\n+### 定义训练循环\\n+\\n+下面的代码适合于训练集上的模型，并在测试集中对其进行求值。自:numref:`chap_linear`以来，线性网络和平方损耗没有改变，所以我们将通过`d2l.linreg`和`d2l.squared_loss`导入它们。唯一的变化是我们的损失现在包括了惩罚条款。\\n+\\n+```{.python .input}\\n+def train(lambd):\\n+    w, b = init_params()\\n+    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\\n+    num_epochs, lr = 100, 0.003\\n+    animator = d2l.Animator(xlabel=\\'epochs\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[5, num_epochs], legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            with autograd.record():\\n+                # The L2 norm penalty term has been added, and broadcasting\\n+                # makes `l2_penalty(w)` a vector whose length is `batch_size`\\n+                l = loss(net(X), y) + lambd * l2_penalty(w)\\n+            l.backward()\\n+            d2l.sgd([w, b], lr, batch_size)\\n+        if (epoch + 1) % 5 == 0:\\n+            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),\\n+                                     d2l.evaluate_loss(net, test_iter, loss)))\\n+    print(\\'L2 norm of w:\\', np.linalg.norm(w))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def train(lambd):\\n+    w, b = init_params()\\n+    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\\n+    num_epochs, lr = 100, 0.003\\n+    animator = d2l.Animator(xlabel=\\'epochs\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[5, num_epochs], legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            with torch.enable_grad():\\n+                # The L2 norm penalty term has been added, and broadcasting\\n+                # makes `l2_penalty(w)` a vector whose length is `batch_size`\\n+                l = loss(net(X), y) + lambd * l2_penalty(w)\\n+            l.sum().backward()\\n+            d2l.sgd([w, b], lr, batch_size)\\n+        if (epoch + 1) % 5 == 0:\\n+            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),\\n+                                     d2l.evaluate_loss(net, test_iter, loss)))\\n+    print(\\'L2 norm of w:\\', torch.norm(w).item())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def train(lambd):\\n+    w, b = init_params()\\n+    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\\n+    num_epochs, lr = 100, 0.003\\n+    animator = d2l.Animator(xlabel=\\'epochs\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[5, num_epochs], legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            with tf.GradientTape() as tape:\\n+                # The L2 norm penalty term has been added, and broadcasting\\n+                # makes `l2_penalty(w)` a vector whose length is `batch_size`\\n+                l = loss(net(X), y) + lambd * l2_penalty(w)\\n+            grads = tape.gradient(l, [w, b])\\n+            d2l.sgd([w, b], grads, lr, batch_size)\\n+        if (epoch + 1) % 5 == 0:\\n+            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),\\n+                                     d2l.evaluate_loss(net, test_iter, loss)))\\n+    print(\\'L2 norm of w:\\', tf.norm(w).numpy())\\n+```\\n+\\n+### 无正规化培训\\n+\\n+我们现在用`lambd = 0`运行这个代码，禁用重量衰减。注意，我们过度拟合，减少了训练误差，但没有减少测试误差——这是textook过度拟合的一个例子。\\n+\\n+```{.python .input}\\n+#@tab all\\n+train(lambd=0)\\n+```\\n+\\n+### 使用权重衰减\\n+\\n+下面，我们的体重大幅下降。注意，训练误差增大，但测试误差减小。这正是我们期望从正规化中得到的效果。\\n+\\n+```{.python .input}\\n+#@tab all\\n+train(lambd=3)\\n+```\\n+\\n+## 简明实施\\n+\\n+由于权值衰减在神经网络优化中无处不在，深度学习框架使其特别方便，将权值衰减集成到优化算法中，以便与任何损失函数结合使用。此外，这种集成还有计算上的好处，允许实现技巧在不增加任何额外的计算开销的情况下向算法中添加权重衰减。由于更新的权重衰减部分仅依赖于每个参数的当前值，因此优化器必须至少接触每个参数一次。\\n+\\n+:begin_tab:`mxnet`\\n+在下面的代码中，我们在实例化`Trainer`时直接通过`wd`指定weight decay超参数。默认情况下，胶子同时衰减权重和偏移。注意，更新模型参数时，超参数`wd`将乘以`wd_mult`。因此，如果我们将`wd_mult`设置为零，则偏移参数$b$将不会衰减。\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+在下面的代码中，我们在实例化优化器时直接通过`weight_decay`指定weight decay超参数。默认情况下，Pythorch同时衰减权重和偏移。这里我们只为权重设置了`weight_decay`，所以bias参数$b$不会衰减。\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+在下面的代码中，我们使用权重衰减超参数$L_2$创建一个$L_2$正则化器，并通过`kernel_regularizer`参数将其应用于层。\\n+:end_tab:\\n+\\n+```{.python .input}\\n+def train_concise(wd):\\n+    net = nn.Sequential()\\n+    net.add(nn.Dense(1))\\n+    net.initialize(init.Normal(sigma=1))\\n+    loss = gluon.loss.L2Loss()\\n+    num_epochs, lr = 100, 0.003\\n+    trainer = gluon.Trainer(net.collect_params(), \\'sgd\\',\\n+                            {\\'learning_rate\\': lr, \\'wd\\': wd})\\n+    # The bias parameter has not decayed. Bias names generally end with \"bias\"\\n+    net.collect_params(\\'.*bias\\').setattr(\\'wd_mult\\', 0)\\n+    animator = d2l.Animator(xlabel=\\'epochs\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[5, num_epochs], legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            with autograd.record():\\n+                l = loss(net(X), y)\\n+            l.backward()\\n+            trainer.step(batch_size)\\n+        if (epoch + 1) % 5 == 0:\\n+            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),\\n+                                     d2l.evaluate_loss(net, test_iter, loss)))\\n+    print(\\'L2 norm of w:\\', np.linalg.norm(net[0].weight.data()))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def train_concise(wd):\\n+    net = nn.Sequential(nn.Linear(num_inputs, 1))\\n+    for param in net.parameters():\\n+        param.data.normal_()\\n+    loss = nn.MSELoss()\\n+    num_epochs, lr = 100, 0.003\\n+    # The bias parameter has not decayed\\n+    trainer = torch.optim.SGD([\\n+        {\"params\":net[0].weight,\\'weight_decay\\': wd},\\n+        {\"params\":net[0].bias}], lr=lr)\\n+    animator = d2l.Animator(xlabel=\\'epochs\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[5, num_epochs], legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            with torch.enable_grad():\\n+                trainer.zero_grad()\\n+                l = loss(net(X), y)\\n+            l.backward()\\n+            trainer.step()\\n+        if (epoch + 1) % 5 == 0:\\n+            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),\\n+                                     d2l.evaluate_loss(net, test_iter, loss)))\\n+    print(\\'L2 norm of w:\\', net[0].weight.norm().item())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def train_concise(wd):\\n+    net = tf.keras.models.Sequential()\\n+    net.add(tf.keras.layers.Dense(\\n+        1, kernel_regularizer=tf.keras.regularizers.l2(wd)))\\n+    net.build(input_shape=(1, num_inputs))\\n+    w, b = net.trainable_variables\\n+    loss = tf.keras.losses.MeanSquaredError()\\n+    num_epochs, lr = 100, 0.003\\n+    trainer = tf.keras.optimizers.SGD(learning_rate=lr)\\n+    animator = d2l.Animator(xlabel=\\'epochs\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[5, num_epochs], legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            with tf.GradientTape() as tape:\\n+                # `tf.keras` requires retrieving and adding the losses from\\n+                # layers manually for custom training loop.\\n+                l = loss(net(X), y) + net.losses\\n+            grads = tape.gradient(l, net.trainable_variables)\\n+            trainer.apply_gradients(zip(grads, net.trainable_variables))\\n+        if (epoch + 1) % 5 == 0:\\n+            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),\\n+                                     d2l.evaluate_loss(net, test_iter, loss)))\\n+    print(\\'L2 norm of w:\\', tf.norm(net.get_weights()[0]).numpy())\\n+```\\n+\\n+这些图看起来和我们从零开始实现权重衰减时的图相同。然而，它们运行得更快，更容易实现，对于更大的问题，这一好处将变得更加明显。\\n+\\n+```{.python .input}\\n+#@tab all\\n+train_concise(0)\\n+```\\n+\\n+```{.python .input}\\n+#@tab all\\n+train_concise(3)\\n+```\\n+\\n+到目前为止，我们只讨论了一个简单线性函数的概念。此外，什么构成一个简单的非线性函数可能是一个更复杂的问题。例如，[再生核希尔伯特空间（RKHS）](https://en.wikipedia.org/wiki/reproducting_kernel_Hilbert_空间)允许在非线性上下文中应用为线性函数引入的工具。不幸的是，基于RKHS的算法往往难以扩展到大的、高维的数据。在这本书中，我们将默认使用简单的启发式方法，即在深层网络的所有层上应用权重衰减。\\n+\\n+## 摘要\\n+\\n+* 正则化是处理过拟合的常用方法。在训练集的损失函数中加入惩罚项，以降低学习模型的复杂度。\\n+* 保持模型简单的一个特别的选择是使用$L_2$惩罚的权重衰减。这会导致学习算法更新步骤中的权重衰减。\\n+* 权重衰减功能在深度学习框架的优化器中提供。\\n+* 在同一训练循环中，不同的参数集可以有不同的更新行为。\\n+\\n+## 练习\\n+\\n+1. 在本节的估计问题中，使用$\\\\lambda$的值进行实验。绘制训练和测试精度作为$\\\\lambda$的函数。你观察到了什么？\\n+1. 使用验证集来找到最佳值$\\\\lambda$。它真的是最优值吗？这有关系吗？\\n+1. 如果我们使用$\\\\sum_i |w_i|$作为我们选择的惩罚（$L_1$正则化），那么更新方程会是什么样子？\\n+1. 我们知道$\\\\|\\\\mathbf{w}\\\\|^2 = \\\\mathbf{w}^\\\\top \\\\mathbf{w}$。你能找到类似的矩阵方程吗（见:numref:`subsec_lin-algebra-norms`中的Frobenius范数）？\\n+1. 回顾了训练误差和泛化误差之间的关系。除了体重下降、增加训练、使用适当复杂度的模型之外，你还能想出其他什么方法来处理过度拟合？\\n+1. 在贝叶斯统计中，我们使用先验和似然的乘积通过$P(w \\\\mid x) \\\\propto P(x \\\\mid w) P(w)$到达后验点。如何识别$P(w)$与正规化？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/98)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/99)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/236)\\n+:end_tab:\\ndiff --git a/chapter_multilayer-perceptrons/weight-decay_tencent.md b/chapter_multilayer-perceptrons/weight-decay_tencent.md\\nnew file mode 100644\\nindex 00000000..0f3c0800\\n--- /dev/null\\n+++ b/chapter_multilayer-perceptrons/weight-decay_tencent.md\\n@@ -0,0 +1,366 @@\\n+# 重量衰减\\n+:label:`sec_weight_decay`\\n+\\n+既然我们已经描述了过拟合问题的特征，我们可以介绍一些用于正规化模型的标准技术。回想一下，我们总是可以通过走出去收集更多的培训数据来缓解过度适应。这可能是昂贵的、耗时的，或者完全不受我们的控制，在短期内是不可能的。目前，我们可以假设在资源允许的情况下，我们已经拥有了尽可能多的高质量数据，并将重点放在正规化技术上。\\n+\\n+回想一下，在我们的多项式回归示例(:numref:`sec_model_selection`)中，我们可以简单地通过调整拟合多项式的次数来限制模型的容量。实际上，限制特征的数量是缓解过度拟合的一种流行技术。然而，简单地把功能放在一边对于这项工作来说可能太迟钝了。继续使用多项式回归示例，考虑高维输入可能发生的情况。多项式对多变量数据的自然扩展称为*单项式*，它简单地说是变量幂的乘积。单项式的次数是幂的和。例如，$x_1^2 x_2$和$x_3 x_5^2$都是3次单项式。\\n+\\n+请注意，具有$d$度的术语的数量随着$d$的增长而迅速增加。在给定$k$个变量的情况下，$d$次单项式的数量(即，$k$个多项式$d$)是${k - 1 + d} \\\\choose {k - 1}$个。即使是阶数上的微小变化，比如从$2$到$3$，也会极大地增加我们模型的复杂性。因此，我们经常需要更细粒度的工具来调整功能复杂性。\\n+\\n+## 范数和权重衰减\\n+\\n+我们已经描述了$L_2$规范和$L_1$规范，它们是:numref:`subsec_lin-algebra-norms`中更一般的$L_p$规范的特例。\\n+*权重衰减*(通常称为$L_2$正则化)，\\n+可能是规则化参数机器学习模型的最广泛使用的技术。该技术是由这样的基本直觉驱动的，即在所有函数$f$中，函数$f = 0$(将值$0$赋给所有输入)在某种意义上是*最简单的*，并且我们可以通过函数离零的距离来测量函数的复杂性。但是，我们应该如何精确地测量函数和零之间的距离呢？没有唯一正确的答案。事实上，整个数学分支，包括部分泛函分析和Banach空间理论，都致力于回答这个问题。\\n+\\n+一种简单的解释可以是通过线性函数$f(\\\\mathbf{x}) = \\\\mathbf{w}^\\\\top \\\\mathbf{x}$的权重向量的某个范数(例如，$\\\\| \\\\mathbf{w} \\\\|^2$)来测量其复杂性。保证一个小的权向量的最常见的方法是将它的范数作为惩罚项添加到最小化损失的问题中。这样我们就取代了原来的目标，\\n+*最小化训练标签上的预测损失*，\\n+有了新的目标，\\n+*最小化预测损失和惩罚项之和*。\\n+现在，如果我们的权重向量变得太大，我们的学习算法可能会专注于最小化权重范数$\\\\| \\\\mathbf{w} \\\\|^2$而不是最小化训练误差。这正是我们想要的。为了用代码来说明问题，让我们从:numref:`sec_linear_regression`开始复习前面的线性回归示例。在那里，我们的损失是由\\n+\\n+$$L(\\\\mathbf{w}, b) = \\\\frac{1}{n}\\\\sum_{i=1}^n \\\\frac{1}{2}\\\\left(\\\\mathbf{w}^\\\\top \\\\mathbf{x}^{(i)} + b - y^{(i)}\\\\right)^2.$$\\n+\\n+回想一下，$\\\\mathbf{x}^{(i)}$是特征，$y^{(i)}$是所有数据示例$i$的标签，$(\\\\mathbf{w}, b)$分别是权重和偏差参数。为了惩罚权重向量的大小，我们必须以某种方式将损失函数增加$\\\\| \\\\mathbf{w} \\\\|^2$，但是模型应该如何权衡这个新的附加惩罚的标准损失呢？在实践中，我们通过*正则化常数*$\\\\lambda$(我们使用验证数据拟合的非负超参数)来表征这种权衡：\\n+\\n+$$L(\\\\mathbf{w}, b) + \\\\frac{\\\\lambda}{2} \\\\|\\\\mathbf{w}\\\\|^2,$$\\n+\\n+对于$\\\\lambda = 0$，我们恢复了原来的损失函数。对于$\\\\lambda > 0$，我们将大小限制为$\\\\| \\\\mathbf{w} \\\\|$。我们按惯例除以$2$：当我们取二次函数的导数时，$2$和$1/2$被抵消，从而确保更新的表达式看起来又好又简单。精明的读者可能想知道为什么我们使用平方范数而不是标准范数(即欧几里德距离)。我们这样做是为了便于计算。通过对$L_2$范数平方，我们去掉了平方根，留下了权重向量每个分量的平方和。这使得罚金的导数很容易计算：导数的和等于和的导数。\\n+\\n+此外，您可能会问，为什么我们一开始就使用$L_2$规范，而不是，比方说，使用$L_1$规范。事实上，在整个统计过程中，其他选择都是有效和受欢迎的。$L_2$正则线性模型构成经典的“岭回归”算法，而$L_1$正则线性回归是统计学中类似的基本模型，俗称“套索回归”。\\n+\\n+使用$L_2$规范的一个原因是它对权重向量的大分量施加了过大的惩罚。这使我们的学习算法偏向在大量特征上均匀分配权重的模型。在实践中，这可能会使它们对单个变量中的测量误差更具鲁棒性。相比之下，$L_1$的惩罚导致通过将其他权重清除为零来将权重集中在一小部分特征上的模型。这就是所谓的“特征选择”，这可能是出于其他原因而需要的。\\n+\\n+使用:eqref:`eq_linreg_batch_update`中的相同符号，$L_2$正则化回归的小批量随机梯度下降更新如下：\\n+\\n+$$\\n+\\\\begin{aligned}\\n+\\\\mathbf{w} & \\\\leftarrow \\\\left(1- \\\\eta\\\\lambda \\\\right) \\\\mathbf{w} - \\\\frac{\\\\eta}{|\\\\mathcal{B}|} \\\\sum_{i \\\\in \\\\mathcal{B}} \\\\mathbf{x}^{(i)} \\\\left(\\\\mathbf{w}^\\\\top \\\\mathbf{x}^{(i)} + b - y^{(i)}\\\\right).\\n+\\\\end{aligned}\\n+$$\\n+\\n+和以前一样，我们根据我们的估计与观测结果不同的金额更新了$\\\\mathbf{w}$。然而，我们也将$\\\\mathbf{w}$的规模缩小到接近零。这就是为什么这种方法有时被称为“权重衰减”：仅在给定惩罚项的情况下，我们的优化算法在训练的每一步都会“衰减”权重。与特征选择不同，权重衰减为我们提供了一种调整函数复杂性的连续机制。较小的值$\\\\lambda$对应较少的约束$\\\\mathbf{w}$，而较大的值$\\\\lambda$则更明显地约束$\\\\mathbf{w}$。\\n+\\n+我们是否包括相应的偏差惩罚$b^2$可以在不同的实现中变化，并且可以在神经网络的各个层上变化。通常，我们不将网络输出层的偏置项正则化。\\n+\\n+## 高维线性回归\\n+\\n+我们可以通过一个简单的合成例子来说明重量衰减的好处。\\n+\\n+```{.python .input}\\n+%matplotlib inline\\n+from d2l import mxnet as d2l\\n+from mxnet import autograd, gluon, init, np, npx\\n+from mxnet.gluon import nn\\n+npx.set_np()\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+%matplotlib inline\\n+from d2l import torch as d2l\\n+import torch\\n+import torch.nn as nn\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+%matplotlib inline\\n+from d2l import tensorflow as d2l\\n+import tensorflow as tf\\n+```\\n+\\n+首先，我们像以前一样生成一些数据\\n+\\n+$$y = 0.05 + \\\\sum_{i = 1}^d 0.01 x_i + \\\\epsilon \\\\text{ where }\\n+\\\\epsilon \\\\sim \\\\mathcal{N}(0, 0.01^2).$$\\n+\\n+我们选择我们的标签是我们输入的线性函数，被具有零均值和标准差0.01的高斯噪声破坏。为了使过度拟合的效果明显，我们可以将问题的维度增加到$d = 200$，并使用仅包含20个示例的小训练集。\\n+\\n+```{.python .input}\\n+#@tab all\\n+n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5\\n+true_w, true_b = d2l.ones((num_inputs, 1)) * 0.01, 0.05\\n+train_data = d2l.synthetic_data(true_w, true_b, n_train)\\n+train_iter = d2l.load_array(train_data, batch_size)\\n+test_data = d2l.synthetic_data(true_w, true_b, n_test)\\n+test_iter = d2l.load_array(test_data, batch_size, is_train=False)\\n+```\\n+\\n+## 从头开始实施\\n+\\n+在下面，我们将从头开始实现权重衰减，只需将$L_2$的平方惩罚添加到原始目标函数中即可。\\n+\\n+### 正在初始化模型参数\\n+\\n+首先，我们将定义一个函数来随机初始化模型参数。\\n+\\n+```{.python .input}\\n+def init_params():\\n+    w = np.random.normal(scale=1, size=(num_inputs, 1))\\n+    b = np.zeros(1)\\n+    w.attach_grad()\\n+    b.attach_grad()\\n+    return [w, b]\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def init_params():\\n+    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)\\n+    b = torch.zeros(1, requires_grad=True)\\n+    return [w, b]\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def init_params():\\n+    w = tf.Variable(tf.random.normal(mean=1, shape=(num_inputs, 1)))\\n+    b = tf.Variable(tf.zeros(shape=(1, )))\\n+    return [w, b]\\n+```\\n+\\n+### 定义$L_2$定额罚款\\n+\\n+也许实施这一处罚最方便的方法是将所有条款放在适当的位置，并将其汇总。\\n+\\n+```{.python .input}\\n+def l2_penalty(w):\\n+    return (w**2).sum() / 2\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def l2_penalty(w):\\n+    return torch.sum(w.pow(2)) / 2\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def l2_penalty(w):\\n+    return tf.reduce_sum(tf.pow(w, 2)) / 2\\n+```\\n+\\n+### 定义培训循环\\n+\\n+下面的代码适合训练集上的一个模型，并在测试集中评估它。线性网络和平方损耗自:numref:`chap_linear`以来没有变化，因此我们将仅通过`d2l.linreg`和`d2l.squared_loss`导入它们。这里唯一的变化是我们的损失现在包括了罚金期限。\\n+\\n+```{.python .input}\\n+def train(lambd):\\n+    w, b = init_params()\\n+    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\\n+    num_epochs, lr = 100, 0.003\\n+    animator = d2l.Animator(xlabel=\\'epochs\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[5, num_epochs], legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            with autograd.record():\\n+                # The L2 norm penalty term has been added, and broadcasting\\n+                # makes `l2_penalty(w)` a vector whose length is `batch_size`\\n+                l = loss(net(X), y) + lambd * l2_penalty(w)\\n+            l.backward()\\n+            d2l.sgd([w, b], lr, batch_size)\\n+        if (epoch + 1) % 5 == 0:\\n+            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),\\n+                                     d2l.evaluate_loss(net, test_iter, loss)))\\n+    print(\\'L2 norm of w:\\', np.linalg.norm(w))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def train(lambd):\\n+    w, b = init_params()\\n+    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\\n+    num_epochs, lr = 100, 0.003\\n+    animator = d2l.Animator(xlabel=\\'epochs\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[5, num_epochs], legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            with torch.enable_grad():\\n+                # The L2 norm penalty term has been added, and broadcasting\\n+                # makes `l2_penalty(w)` a vector whose length is `batch_size`\\n+                l = loss(net(X), y) + lambd * l2_penalty(w)\\n+            l.sum().backward()\\n+            d2l.sgd([w, b], lr, batch_size)\\n+        if (epoch + 1) % 5 == 0:\\n+            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),\\n+                                     d2l.evaluate_loss(net, test_iter, loss)))\\n+    print(\\'L2 norm of w:\\', torch.norm(w).item())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def train(lambd):\\n+    w, b = init_params()\\n+    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\\n+    num_epochs, lr = 100, 0.003\\n+    animator = d2l.Animator(xlabel=\\'epochs\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[5, num_epochs], legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            with tf.GradientTape() as tape:\\n+                # The L2 norm penalty term has been added, and broadcasting\\n+                # makes `l2_penalty(w)` a vector whose length is `batch_size`\\n+                l = loss(net(X), y) + lambd * l2_penalty(w)\\n+            grads = tape.gradient(l, [w, b])\\n+            d2l.sgd([w, b], grads, lr, batch_size)\\n+        if (epoch + 1) % 5 == 0:\\n+            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),\\n+                                     d2l.evaluate_loss(net, test_iter, loss)))\\n+    print(\\'L2 norm of w:\\', tf.norm(w).numpy())\\n+```\\n+\\n+### 不正规化训练\\n+\\n+我们现在使用`lambd = 0`运行此代码，禁用重量衰减。请注意，我们的过度拟合非常严重，减少了训练误差，但没有减少测试误差-这是一个过度拟合的文本化情况。\\n+\\n+```{.python .input}\\n+#@tab all\\n+train(lambd=0)\\n+```\\n+\\n+### 使用权重衰减\\n+\\n+下面，我们运行时会有相当大的重量衰减。注意，训练误差增加，但测试误差减小。这正是我们期待的正规化效果。\\n+\\n+```{.python .input}\\n+#@tab all\\n+train(lambd=3)\\n+```\\n+\\n+## 简明实施\\n+\\n+由于权重衰减在神经网络优化中普遍存在，深度学习框架使其变得特别方便，它将权重衰减集成到优化算法本身中，以便与任何损失函数结合使用。此外，这种集成具有计算优势，允许实现技巧在不增加任何计算开销的情况下增加算法的权重衰减。由于更新的权重衰减部分仅取决于每个参数的当前值，因此优化器无论如何都必须触及每个参数一次。\\n+\\n+:begin_tab:`mxnet`\\n+在下面的代码中，我们在实例化`wd`时直接通过`Trainer`指定权重衰减超参数。默认情况下，胶子会同时衰退重量和偏移。注意，当更新模型参数时，超参数`wd`将乘以`wd_mult`。因此，如果我们将`wd_mult`设置为零，则偏置参数$b$将不会衰减。\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+在下面的代码中，我们在实例化优化器时直接通过`weight_decay`指定权重衰减超参数。默认情况下，PyTorch会同时衰退权重和偏移。在这里，我们仅将权重设置为`weight_decay`，因此偏移参数$b$不会衰减。\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+在下面的代码中，我们使用权重衰减超参数`wd`创建了一个$L_2$的正则化函数，并通过`kernel_regularizer`参数将其应用于层。\\n+:end_tab:\\n+\\n+```{.python .input}\\n+def train_concise(wd):\\n+    net = nn.Sequential()\\n+    net.add(nn.Dense(1))\\n+    net.initialize(init.Normal(sigma=1))\\n+    loss = gluon.loss.L2Loss()\\n+    num_epochs, lr = 100, 0.003\\n+    trainer = gluon.Trainer(net.collect_params(), \\'sgd\\',\\n+                            {\\'learning_rate\\': lr, \\'wd\\': wd})\\n+    # The bias parameter has not decayed. Bias names generally end with \"bias\"\\n+    net.collect_params(\\'.*bias\\').setattr(\\'wd_mult\\', 0)\\n+    animator = d2l.Animator(xlabel=\\'epochs\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[5, num_epochs], legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            with autograd.record():\\n+                l = loss(net(X), y)\\n+            l.backward()\\n+            trainer.step(batch_size)\\n+        if (epoch + 1) % 5 == 0:\\n+            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),\\n+                                     d2l.evaluate_loss(net, test_iter, loss)))\\n+    print(\\'L2 norm of w:\\', np.linalg.norm(net[0].weight.data()))\\n+```\\n+\\n+```{.python .input}\\n+#@tab pytorch\\n+def train_concise(wd):\\n+    net = nn.Sequential(nn.Linear(num_inputs, 1))\\n+    for param in net.parameters():\\n+        param.data.normal_()\\n+    loss = nn.MSELoss()\\n+    num_epochs, lr = 100, 0.003\\n+    # The bias parameter has not decayed\\n+    trainer = torch.optim.SGD([\\n+        {\"params\":net[0].weight,\\'weight_decay\\': wd},\\n+        {\"params\":net[0].bias}], lr=lr)\\n+    animator = d2l.Animator(xlabel=\\'epochs\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[5, num_epochs], legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            with torch.enable_grad():\\n+                trainer.zero_grad()\\n+                l = loss(net(X), y)\\n+            l.backward()\\n+            trainer.step()\\n+        if (epoch + 1) % 5 == 0:\\n+            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),\\n+                                     d2l.evaluate_loss(net, test_iter, loss)))\\n+    print(\\'L2 norm of w:\\', net[0].weight.norm().item())\\n+```\\n+\\n+```{.python .input}\\n+#@tab tensorflow\\n+def train_concise(wd):\\n+    net = tf.keras.models.Sequential()\\n+    net.add(tf.keras.layers.Dense(\\n+        1, kernel_regularizer=tf.keras.regularizers.l2(wd)))\\n+    net.build(input_shape=(1, num_inputs))\\n+    w, b = net.trainable_variables\\n+    loss = tf.keras.losses.MeanSquaredError()\\n+    num_epochs, lr = 100, 0.003\\n+    trainer = tf.keras.optimizers.SGD(learning_rate=lr)\\n+    animator = d2l.Animator(xlabel=\\'epochs\\', ylabel=\\'loss\\', yscale=\\'log\\',\\n+                            xlim=[5, num_epochs], legend=[\\'train\\', \\'test\\'])\\n+    for epoch in range(num_epochs):\\n+        for X, y in train_iter:\\n+            with tf.GradientTape() as tape:\\n+                # `tf.keras` requires retrieving and adding the losses from\\n+                # layers manually for custom training loop.\\n+                l = loss(net(X), y) + net.losses\\n+            grads = tape.gradient(l, net.trainable_variables)\\n+            trainer.apply_gradients(zip(grads, net.trainable_variables))\\n+        if (epoch + 1) % 5 == 0:\\n+            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),\\n+                                     d2l.evaluate_loss(net, test_iter, loss)))\\n+    print(\\'L2 norm of w:\\', tf.norm(net.get_weights()[0]).numpy())\\n+```\\n+\\n+这些情节看起来与我们从头开始实施重量衰减时的情况完全相同。但是，它们的运行速度要快得多，而且更容易实现，对于较大的问题，这一优势将变得更加明显。\\n+\\n+```{.python .input}\\n+#@tab all\\n+train_concise(0)\\n+```\\n+\\n+```{.python .input}\\n+#@tab all\\n+train_concise(3)\\n+```\\n+\\n+到目前为止，我们只触及了构成简单线性函数的一个概念。此外，什么构成一个简单的非线性函数可能是一个更复杂的问题。例如，[再生核希尔伯特空间(RKHS)](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space)]允许人们在非线性环境中应用为线性函数引入的工具。不幸的是，基于RKHS的算法往往不能很好地扩展到大型、高维数据。在本书中，我们将默认使用简单的启发式方法，即在深层网络的所有层上应用权重衰减。\\n+\\n+## 摘要\\n+\\n+* 正则化是处理过拟合的常用方法。该算法在训练集的损失函数中加入惩罚项，降低了学习模型的复杂度。\\n+* 保持模型简单的一个特殊选择是重量衰减，使用$L_2$的惩罚。这导致学习算法的更新步骤中的权重衰减。\\n+* 深度学习框架的优化器中提供了权重衰减功能。\\n+* 在同一训练循环内，不同的参数集可以具有不同的更新行为。\\n+\\n+## 练习\\n+\\n+1. 在本节的估计问题中使用值$\\\\lambda$进行试验。绘制训练和测试精度与$\\\\lambda$的函数关系图。你观察到了什么？\\n+1. 使用验证集查找最佳值$\\\\lambda$。这真的是最优值吗？这有关系吗？\\n+1. 如果我们使用$\\\\|\\\\mathbf{w}\\\\|^2$而不是$\\\\sum_i |w_i|$作为我们选择的惩罚($L_1$正则化)，更新方程式会是什么样子？\\n+1. 我们知道那$\\\\|\\\\mathbf{w}\\\\|^2 = \\\\mathbf{w}^\\\\top \\\\mathbf{w}$。你能为矩阵找到一个类似的方程式(参见:numref:`subsec_lin-algebra-norms`中的弗罗贝尼乌斯范数)吗？\\n+1. 回顾训练误差和泛化误差之间的关系。除了体重下降，增加训练，使用适当复杂的模型外，你还能想到其他什么方法来处理过度适应的问题吗？\\n+1. 在贝叶斯统计中，我们使用先验和可能性的乘积来得出后验通孔$P(w \\\\mid x) \\\\propto P(x \\\\mid w) P(w)$。你怎么能把$P(w)$等同于正规化呢？\\n+\\n+:begin_tab:`mxnet`\\n+[Discussions](https://discuss.d2l.ai/t/98)\\n+:end_tab:\\n+\\n+:begin_tab:`pytorch`\\n+[Discussions](https://discuss.d2l.ai/t/99)\\n+:end_tab:\\n+\\n+:begin_tab:`tensorflow`\\n+[Discussions](https://discuss.d2l.ai/t/236)\\n+:end_tab:'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git.diff('upstream/release','translate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' README.md                                                                   |   2 +-',\n",
       " ' chapter_appendix-mathematics-for-deep-learning/eigendecomposition.md        |   2 +-',\n",
       " ' .../geometry-linear-algebraic-ops.md                                        |   2 +-',\n",
       " ' chapter_appendix-mathematics-for-deep-learning/statistics.md                |   2 +-',\n",
       " ' chapter_attention-mechanisms/transformer.md                                 |  14 +-',\n",
       " ' chapter_convolutional-modern/alexnet_baidu.md                               | 278 ++++++++++++++++++',\n",
       " ' chapter_convolutional-modern/alexnet_tencent.md                             | 278 ++++++++++++++++++',\n",
       " ' chapter_convolutional-modern/batch-norm_baidu.md                            | 477 +++++++++++++++++++++++++++++++',\n",
       " ' chapter_convolutional-modern/batch-norm_tencent.md                          | 477 +++++++++++++++++++++++++++++++',\n",
       " ' chapter_convolutional-modern/densenet_baidu.md                              | 380 +++++++++++++++++++++++++',\n",
       " ' chapter_convolutional-modern/densenet_tencent.md                            | 380 +++++++++++++++++++++++++',\n",
       " ' chapter_convolutional-modern/googlenet.md                                   |  11 +-',\n",
       " ' chapter_convolutional-modern/googlenet_baidu.md                             | 343 ++++++++++++++++++++++',\n",
       " ' chapter_convolutional-modern/googlenet_tencent.md                           | 343 ++++++++++++++++++++++',\n",
       " ' chapter_convolutional-modern/index_baidu.md                                 |  20 ++',\n",
       " ' chapter_convolutional-modern/index_tencent.md                               |  20 ++',\n",
       " ' chapter_convolutional-modern/nin_baidu.md                                   | 191 +++++++++++++',\n",
       " ' chapter_convolutional-modern/nin_tencent.md                                 | 191 +++++++++++++',\n",
       " ' chapter_convolutional-modern/resnet.md                                      |   2 +-',\n",
       " ' chapter_convolutional-modern/resnet_baidu.md                                | 374 ++++++++++++++++++++++++',\n",
       " ' chapter_convolutional-modern/resnet_tencent.md                              | 374 ++++++++++++++++++++++++',\n",
       " ' chapter_convolutional-modern/vgg_baidu.md                                   | 215 ++++++++++++++',\n",
       " ' chapter_convolutional-modern/vgg_tencent.md                                 | 215 ++++++++++++++',\n",
       " ' chapter_convolutional-neural-networks/channels_baidu.md                     | 172 +++++++++++',\n",
       " ' chapter_convolutional-neural-networks/channels_tencent.md                   | 172 +++++++++++',\n",
       " ' chapter_convolutional-neural-networks/conv-layer_baidu.md                   | 313 ++++++++++++++++++++',\n",
       " ' chapter_convolutional-neural-networks/conv-layer_tencent.md                 | 313 ++++++++++++++++++++',\n",
       " ' chapter_convolutional-neural-networks/index_baidu.md                        |  21 ++',\n",
       " ' chapter_convolutional-neural-networks/index_tencent.md                      |  21 ++',\n",
       " ' chapter_convolutional-neural-networks/lenet_baidu.md                        | 336 ++++++++++++++++++++++',\n",
       " ' chapter_convolutional-neural-networks/lenet_tencent.md                      | 336 ++++++++++++++++++++++',\n",
       " ' chapter_convolutional-neural-networks/padding-and-strides_baidu.md          | 212 ++++++++++++++',\n",
       " ' chapter_convolutional-neural-networks/padding-and-strides_tencent.md        | 212 ++++++++++++++',\n",
       " ' chapter_convolutional-neural-networks/pooling_baidu.md                      | 233 +++++++++++++++',\n",
       " ' chapter_convolutional-neural-networks/pooling_tencent.md                    | 233 +++++++++++++++',\n",
       " ' chapter_convolutional-neural-networks/why-conv_baidu.md                     | 115 ++++++++',\n",
       " ' chapter_convolutional-neural-networks/why-conv_tencent.md                   | 115 ++++++++',\n",
       " ' chapter_deep-learning-computation/custom-layer.md                           |   3 +-',\n",
       " ' chapter_deep-learning-computation/custom-layer_baidu.md                     | 234 +++++++++++++++',\n",
       " ' chapter_deep-learning-computation/custom-layer_tencent.md                   | 234 +++++++++++++++',\n",
       " ' chapter_deep-learning-computation/deferred-init_baidu.md                    | 106 +++++++',\n",
       " ' chapter_deep-learning-computation/deferred-init_tencent.md                  | 106 +++++++',\n",
       " ' chapter_deep-learning-computation/index_baidu.md                            |  17 ++',\n",
       " ' chapter_deep-learning-computation/index_tencent.md                          |  17 ++',\n",
       " ' chapter_deep-learning-computation/model-construction_baidu.md               | 452 +++++++++++++++++++++++++++++',\n",
       " ' chapter_deep-learning-computation/model-construction_tencent.md             | 452 +++++++++++++++++++++++++++++',\n",
       " ' chapter_deep-learning-computation/parameters_baidu.md                       | 550 ++++++++++++++++++++++++++++++++++++',\n",
       " ' chapter_deep-learning-computation/parameters_tencent.md                     | 550 ++++++++++++++++++++++++++++++++++++',\n",
       " ' chapter_deep-learning-computation/read-write_baidu.md                       | 238 ++++++++++++++++',\n",
       " ' chapter_deep-learning-computation/read-write_tencent.md                     | 238 ++++++++++++++++',\n",
       " ' chapter_deep-learning-computation/use-gpu_baidu.md                          | 343 ++++++++++++++++++++++',\n",
       " ' chapter_deep-learning-computation/use-gpu_tencent.md                        | 343 ++++++++++++++++++++++',\n",
       " ' chapter_linear-networks/linear-regression-concise.md                        |   2 +-',\n",
       " ' chapter_multilayer-perceptrons/backprop_baidu.md                            | 133 +++++++++',\n",
       " ' chapter_multilayer-perceptrons/backprop_tencent.md                          | 133 +++++++++',\n",
       " ' chapter_multilayer-perceptrons/dropout_baidu.md                             | 382 +++++++++++++++++++++++++',\n",
       " ' chapter_multilayer-perceptrons/dropout_tencent.md                           | 382 +++++++++++++++++++++++++',\n",
       " ' chapter_multilayer-perceptrons/environment_baidu.md                         | 252 +++++++++++++++++',\n",
       " ' chapter_multilayer-perceptrons/environment_tencent.md                       | 252 +++++++++++++++++',\n",
       " ' chapter_multilayer-perceptrons/index_baidu.md                               |  19 ++',\n",
       " ' chapter_multilayer-perceptrons/index_tencent.md                             |  19 ++',\n",
       " ' chapter_multilayer-perceptrons/kaggle-house-price_baidu.md                  | 483 +++++++++++++++++++++++++++++++',\n",
       " ' chapter_multilayer-perceptrons/kaggle-house-price_tencent.md                | 483 +++++++++++++++++++++++++++++++',\n",
       " ' chapter_multilayer-perceptrons/mlp-concise_baidu.md                         | 110 ++++++++',\n",
       " ' chapter_multilayer-perceptrons/mlp-concise_tencent.md                       | 110 ++++++++',\n",
       " ' chapter_multilayer-perceptrons/mlp-scratch_baidu.md                         | 202 +++++++++++++',\n",
       " ' chapter_multilayer-perceptrons/mlp-scratch_tencent.md                       | 202 +++++++++++++',\n",
       " ' chapter_multilayer-perceptrons/mlp_baidu.md                                 | 287 +++++++++++++++++++',\n",
       " ' chapter_multilayer-perceptrons/mlp_tencent.md                               | 287 +++++++++++++++++++',\n",
       " ' chapter_multilayer-perceptrons/numerical-stability-and-init_baidu.md        | 182 ++++++++++++',\n",
       " ' chapter_multilayer-perceptrons/numerical-stability-and-init_tencent.md      | 182 ++++++++++++',\n",
       " ' chapter_multilayer-perceptrons/underfit-overfit_baidu.md                    | 343 ++++++++++++++++++++++',\n",
       " ' chapter_multilayer-perceptrons/underfit-overfit_tencent.md                  | 343 ++++++++++++++++++++++',\n",
       " ' chapter_multilayer-perceptrons/weight-decay_baidu.md                        | 366 ++++++++++++++++++++++++',\n",
       " ' chapter_multilayer-perceptrons/weight-decay_tencent.md                      | 366 ++++++++++++++++++++++++',\n",
       " ' .../natural-language-inference-attention.md                                 |   6 +-',\n",
       " ' .../natural-language-inference-bert.md                                      |   2 +-',\n",
       " ' chapter_natural-language-processing-applications/sentiment-analysis-cnn.md  |   2 +-',\n",
       " ' chapter_optimization/adagrad.md                                             |   2 +-',\n",
       " ' chapter_optimization/adam.md                                                |   2 +-',\n",
       " ' chapter_preface/index.md                                                    |   2 +-',\n",
       " ' chapter_preliminaries/probability.md                                        |  10 +-',\n",
       " ' chapter_recommender-systems/neumf.md                                        |   2 +-',\n",
       " ' chapter_recommender-systems/seqrec.md                                       |   2 +-',\n",
       " ' chapter_recurrent-modern/bi-rnn.md                                          | 188 ++++++++----',\n",
       " ' chapter_recurrent-modern/deep-rnn.md                                        |  99 +++++--',\n",
       " ' chapter_recurrent-modern/gru.md                                             | 214 +++++++++-----',\n",
       " ' chapter_recurrent-modern/index.md                                           |  57 ++--',\n",
       " ' chapter_recurrent-modern/lstm.md                                            | 141 ++++++---',\n",
       " ' chapter_recurrent-modern/machine-translation-and-dataset.md                 |   4 +-',\n",
       " ' chapter_recurrent-neural-networks/bptt.md                                   |   8 +-',\n",
       " ' chapter_recurrent-neural-networks/rnn-scratch.md                            |  28 +-',\n",
       " ' chapter_recurrent-neural-networks/rnn.md                                    |   1 -',\n",
       " ' chapter_recurrent-neural-networks/sequence.md                               |   2 +-',\n",
       " ' d2l/tensorflow.py                                                           |  13 +-',\n",
       " ' graffle/rnn/deep-rnn.graffle                                                | Bin 1151061 -> 1004964 bytes',\n",
       " ' graffle/rnn/hmm.graffle                                                     | Bin 315593 -> 335387 bytes',\n",
       " ' img/deep-rnn.svg                                                            |  21 +-',\n",
       " ' img/hmm.svg                                                                 |  84 +++---',\n",
       " ' static/frontpage/frontpage.html                                             |   1 +',\n",
       " ' 100 files changed, 17351 insertions(+), 338 deletions(-)']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "GitCommandError",
     "evalue": "Cmd('git') failed due to: exit code(128)\n  cmdline: git branch master\n  stderr: 'fatal: A branch named 'master' already exists.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGitCommandError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-550cd772b353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepository\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'master'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/git/cmd.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mLazyMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_persistent_git_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/git/cmd.py\u001b[0m in \u001b[0;36m_call_process\u001b[0;34m(self, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1003\u001b[0m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexec_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parse_object_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/git/cmd.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, istream, with_extended_output, with_exceptions, as_process, output_stream, stdout_as_string, kill_after_timeout, with_stdout, universal_newlines, shell, env, max_chunk_size, **subprocess_kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwith_exceptions\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGitCommandError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstdout_as_string\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# could also be output_stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGitCommandError\u001b[0m: Cmd('git') failed due to: exit code(128)\n  cmdline: git branch master\n  stderr: 'fatal: A branch named 'master' already exists.'"
     ]
    }
   ],
   "source": [
    "git = repository.git\n",
    "git.branch('master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.Commit \"a74f76ee9b080b0264f0a98f227bb2453768c336\">"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repository.commit('a74f76ee9b080b0264f0a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repository.index.diff(repository.commit('a74f76ee9b080b0264f0a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchPathError",
     "evalue": "/home/ubuntu/notebooks/d2l-en/https:/github.com/d2l-ai/d2l-en.git",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchPathError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-721c39c7a291>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrepo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRepo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://github.com/d2l-ai/d2l-en.git\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/git/repo/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, odbt, search_parent_directories, expand_vars)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchPathError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m## Walk up the path to find the `.git` dir.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchPathError\u001b[0m: /home/ubuntu/notebooks/d2l-en/https:/github.com/d2l-ai/d2l-en.git"
     ]
    }
   ],
   "source": [
    "repo = git.Repo(\"https://github.com/d2l-ai/d2l-en.git\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "GitCommandError",
     "evalue": "Cmd('git') failed due to: exit code(128)\n  cmdline: git branch master\n  stderr: 'fatal: A branch named 'master' already exists.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGitCommandError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a0bb76ca34c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgit1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepository\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgit1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'master'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/git/cmd.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mLazyMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_persistent_git_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/git/cmd.py\u001b[0m in \u001b[0;36m_call_process\u001b[0;34m(self, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1003\u001b[0m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexec_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parse_object_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/git/cmd.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, istream, with_extended_output, with_exceptions, as_process, output_stream, stdout_as_string, kill_after_timeout, with_stdout, universal_newlines, shell, env, max_chunk_size, **subprocess_kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwith_exceptions\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGitCommandError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstdout_as_string\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# could also be output_stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGitCommandError\u001b[0m: Cmd('git') failed due to: exit code(128)\n  cmdline: git branch master\n  stderr: 'fatal: A branch named 'master' already exists.'"
     ]
    }
   ],
   "source": [
    "git1 = repository.git\n",
    "git1.branch('master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git = repo.git\n",
    "git.checkout('HEAD', b=\"my_new_branch\")         # create a new branch\n",
    "git.branch('another-new-one')\n",
    "git.branch('-D', 'another-new-one')             # pass strings for full control over argument order\n",
    "git.for_each_ref()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "commit = repository.commit(rev=revision)\n",
    "revision = \"ace83d9d2a97cfe8a8aa9bdd7b46ce71713fb494\"\n",
    "\n",
    "\n",
    "# Git ignore white space at the end of line, empty lines,\n",
    "# renamed files and also copied files\n",
    "diff_index = commit.diff(revision+'~1', create_patch=True, ignore_blank_lines=True, \n",
    "                         ignore_space_at_eol=True, diff_filter='cr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
