# 环境和分销转变

在前几节中，我们学习了许多机器学习的动手应用程序，将模型拟合到各种数据集。然而，我们从未停下来思考数据最初从何而来，或者我们计划最终如何处理我们模型的输出。通常情况下，拥有数据的机器学习开发人员没有停下来考虑这些基本问题就急于开发模型。

许多失败的机器学习部署都可以追溯到此模式。有时候，按照测试集的准确性衡量，模型似乎表现得非常出色，但当数据分布突然改变时，部署就会灾难性地失败。更隐秘的是，有时模型的部署本身可能是扰乱数据分发的催化剂。例如，假设我们训练了一个模型来预测谁将偿还贷款或违约，发现申请者选择的鞋子与违约风险相关(牛津鞋表示偿还，运动鞋表示违约)。此后，我们可能倾向于向所有穿着牛津运动鞋的申请者发放贷款，而拒绝所有穿运动鞋的申请者。

在这种情况下，我们从模式识别到决策的考虑不周的飞跃，以及我们未能批判性地考虑环境问题，可能会产生灾难性的后果。首先，一旦我们开始根据鞋类做出决定，客户就会意识到这一点，并改变他们的行为。不久之后，所有申请者都将穿着牛津鞋，信用状况没有任何相应的改善。花一分钟来消化这一点，因为在机器学习的许多应用程序中，类似的问题比比皆是：通过将我们基于模型的决策引入环境，我们可能会打破模型。

虽然我们不可能在一个部分中完整地讨论这些主题，但我们在这里的目标是揭示一些常见的问题，并激发所需的批判性思维，以及早发现这些情况，减轻损害，并负责任地使用机器学习。有些解决方案很简单(要求“正确”的数据)，有些解决方案在技术上很困难(实现强化学习系统)，还有一些方案要求我们完全跳出统计预测的领域，努力解决与算法的伦理应用有关的棘手的哲学问题。

## 分配班次的类型

首先，我们坚持被动预测设置，考虑到数据分布可能发生变化的各种方式，以及为挽救模型性能可能采取的措施。在一种经典设置中，我们假设我们的训练数据是从某个分布$p_S(\mathbf{x},y)$采样的，但是我们的测试数据将包括从某个不同的分布$p_T(\mathbf{x},y)$提取的未标记的示例。我们已经必须面对一个发人深省的现实。如果没有任何关于$p_S$和$p_T$如何相互关联的假设，学习健壮的分类器是不可能的。

考虑一个二进制分类问题，我们希望区分狗和猫。如果分布可以以任意方式移动，那么我们的设置允许在病理情况下，输入上的分布保持不变：$p_S(\mathbf{x}) = p_T(\mathbf{x})$，但是标签全部翻转：$p_S(y | \mathbf{x}) = 1 - p_T(y | \mathbf{x})$。换句话说，如果上帝突然决定未来所有的“猫”现在都是狗，我们以前所说的“狗”现在是猫-投入$p(\mathbf{x})$的分布没有任何改变，那么我们就不可能将这个设置与分布完全没有改变的设置区分开来。

幸运的是，在对我们的数据未来可能发生变化的一些有限假设下，原则性算法可以检测到偏移，有时甚至可以在飞翔上进行调整，从而提高了原始分类器的准确性。

### 协变量移位

在分布漂移的分类中，协变量漂移可能是研究最广泛的。这里，我们假设虽然输入的分布可以随时间改变，但标记函数(即条件分布$P(y \mid \mathbf{x})$)不改变。统计学家称之为“协变量转移”，因为这个问题是由于协变量(特征)分布的转移引起的。虽然我们有时可以在不引用因果关系的情况下对分布偏移进行推理，但我们注意到协变量偏移是在我们认为$\mathbf{x}$导致$y$的设置中调用的自然假设。

想想区分猫和狗的挑战吧。我们的训练数据可能由:numref:`fig_cat-dog-train`中的图像组成。

![Training data for distinguishing cats and dogs.](../img/cat-dog-train.svg)
:label:`fig_cat-dog-train`

在测试时，我们被要求对:numref:`fig_cat-dog-test`中的图像进行分类。

![Test data for distinguishing cats and dogs.](../img/cat-dog-test.svg)
:label:`fig_cat-dog-test`

训练集由照片组成，而测试集只包含卡通。在具有与测试集本质不同的特征的数据集上进行训练可能会带来麻烦，因为没有关于如何适应新领域的连贯计划。

### 标签移位

*LABEL SHIFT*描述了反向问题。
这里，我们假设标签边缘$P(y)$可以改变，但是类别条件分布$P(\mathbf{x} \mid y)$跨域保持固定。当我们认为$y$导致$\mathbf{x}$时，标签移位是一个合理的假设。例如，我们可能想要根据其症状(或其他表现)来预测诊断，即使诊断的相对流行率正在随着时间的推移而变化。标签移位在这里是合适的假设，因为疾病会引起症状。在某些退化情况下，标签移位和协变量移位假设可以同时成立。例如，当标签是确定性的时，即使当$y$导致$\mathbf{x}$时，也将满足协变量移位假设。有趣的是，在这些情况下，使用源自标签移位假设的方法通常是有利的。这是因为这些方法往往涉及操作看起来像标签的对象(通常是低维的)，而不是看起来像输入的对象，后者在深度学习中往往是高维的。

### 观念转变

我们可能还会遇到相关的“概念转移”问题，这是当标签的定义可能发生变化时出现的。这听起来很奇怪-“猫”是“猫”，不是吗？但是，随着时间的推移，其他类别的使用可能会发生变化。精神疾病的诊断标准，什么是流行的，什么是职称，都会受到相当大的概念转变的影响。事实证明，如果我们在美国各地导航，根据地理位置改变我们的数据来源，我们会发现关于“软饮料”名称分布的概念发生了相当大的变化，如:numref:`fig_popvssoda`所示。

![Concept shift on soft drink names in the United States.](../img/popvssoda.png)
:width:`400px`
:label:`fig_popvssoda`

如果我们要构建机器翻译系统，则分发$P(y \mid \mathbf{x})$可能会因我们的位置不同而不同。这个问题可能很难发现。我们可能希望利用这样一种知识，即转变只会在时间或地理意义上逐渐发生。

## 分布移位示例

在深入研究形式主义和算法之前，我们可以讨论一些具体的情况，在这些情况下，协变量或概念转换可能不明显。

### 医疗诊断学

想象一下，您想要设计一种检测癌症的算法。你从健康人和病人那里收集数据，然后训练你的算法。它工作得很好，给你很高的精确度，你会得出结论，你已经准备好在医疗诊断领域取得成功。
*不要那么快*

产生训练数据的分布可能与您在野外遇到的分布有很大不同。这发生在一家不幸的初创公司身上，我们中的一些人(作者)几年前就和它合作过。他们正在开发一种主要影响老年男性的疾病的血液测试，并希望使用他们从患者那里收集的血液样本进行研究。然而，从健康男性身上获取血液样本比从系统中已经存在的病人身上获取血液样本要困难得多。为了补偿，这家初创公司向一所大学校园的学生募集献血，作为开发测试的健康对照。然后他们问我们是否可以帮助他们建立一个分类器来检测疾病。

正如我们向他们解释的那样，确实很容易以近乎完美的准确性区分健康和患病的人群。然而，这是因为测试对象在年龄、激素水平、体力活动、饮食、饮酒以及更多与疾病无关的因素上存在差异。这不太可能是真正的病人的情况。由于他们的抽样程序，我们可以预料到会遇到极端的协变量漂移。此外，这种情况不太可能通过传统的方法来纠正。简而言之，他们浪费了一大笔钱。

### 自动驾驶汽车

假设一家公司想要利用机器学习来开发自动驾驶汽车。这里的一个关键部件是路边探测器。由于真正的注释数据获取成本很高，他们有一个(聪明而可疑的)想法，即使用来自游戏渲染引擎的合成数据作为额外的训练数据。这对从渲染引擎提取的“测试数据”非常有效。唉，在一辆真正的车里，这简直是一场灾难。事实证明，路边被渲染成了非常简单化的纹理。更重要的是，“所有”路边都是用“相同”纹理渲染的，路边探测器很快就了解到了这一“特征”。

当美军第一次试图探测森林中的坦克时，类似的事情也发生在他们身上。他们在没有坦克的情况下航拍了森林的照片，然后把坦克开进森林，又拍了一组照片。分类器似乎工作得“完美”。不幸的是，它只学会了如何区分有阴影的树木和没有阴影的树木-第一组照片是在清晨拍摄的，第二组照片是在中午拍摄的。

### 非平稳分布

当分布变化缓慢(也称为“非平稳分布”)并且模型没有充分更新时，会出现一种更为微妙的情况。以下是一些典型案例。

* 我们训练了一个计算广告模型，然后没有频繁地更新它(例如，我们忘记了纳入一款名为iPad的鲜为人知的新设备刚刚推出)。
* 我们建了一个垃圾过滤。它可以很好地检测到我们到目前为止看到的所有垃圾邮件。但随后，垃圾邮件发送者变得聪明起来，精心制作了看起来与我们以前见过的任何邮件都不同的新邮件。
* 我们建立了一个产品推荐系统。它在整个冬天都有效，但圣诞节过后很久还会继续推荐圣诞帽。

### 更多趣闻轶事

* 我们造了一个面部探测器。它在所有基准上都工作得很好。不幸的是，它在测试数据上失败了-令人不快的例子是面部充满整个图像的特写镜头(训练集中没有这样的数据)。
* 我们为美国市场建立了一个网络搜索引擎，并希望将其部署在英国。
* 我们通过编译一个大型数据集来训练图像分类器，其中一大组类中的每一个在数据集中都相等地表示，比如说1000个类别，每个类别由1000个图像表示。然后，我们将该系统部署在现实世界中，在现实世界中，照片的实际标签分布显然是不均匀的。

## 分配偏移的修正

如我们所讨论的，存在训练和测试分布$P(\mathbf{x}, y)$不同的许多情况。在某些情况下，我们很幸运，尽管协变量、标签或概念发生了变化，但模型仍然有效。在其他情况下，我们可以通过采取有原则的策略来应对这种转变，做得更好。本节的其余部分将变得更加技术性。不耐烦的读者可以继续阅读下一节，因为本材料不是后续概念的先决条件。

### 经验风险与真实风险

让我们首先反映在模型训练期间究竟发生了什么：我们迭代训练数据$\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}$的特征和相关联的标签，并且在每个小批量之后更新模型$f$的参数。为简单起见，我们不考虑正规化，因此我们在很大程度上将培训损失降至最低：

$$\mathop{\mathrm{minimize}}_f \frac{1}{n} \sum_{i=1}^n l(f(\mathbf{x}_i), y_i),$$
:eqlabel:`eq_empirical-risk-min`

其中$l$是测量预测$f(\mathbf{x}_i)$有多差的损失函数，被给予关联的标签$y_i$。统计学家将:eqref:`eq_empirical-risk-min`中的术语称为“经验风险”。
*经验风险*是训练数据的平均损失
为了近似“真实风险”，其是从数据的真实分布$p(\mathbf{x},y)$中提取的整个数据总体上的损失的预期：

$$E_{p(\mathbf{x}, y)} [l(f(\mathbf{x}), y)] = \int\int l(f(\mathbf{x}), y) p(\mathbf{x}, y) \;d\mathbf{x}dy.$$
:eqlabel:`eq_true-risk`

然而，在实践中，我们通常不能获得全部数据。因此，:eqref:`eq_empirical-risk-min`提出的“经验风险最小化”是一种实用的机器学习策略，希望近似最小化真实风险。

### 协变量移位校正
:label:`subsec_covariate-shift-correction`

假设我们想要估计一些依赖性$P(y \mid \mathbf{x})$，我们已经为其标记了数据$(\mathbf{x}_i, y_i)$。不幸的是，观测$\mathbf{x}_i$取自一些*源分布*$q(\mathbf{x})$，而不是*目标分布*$p(\mathbf{x})$。幸运的是，相依性假设意味着条件分布不会改变：$p(y \mid \mathbf{x}) = q(y \mid \mathbf{x})$。如果源分布$q(\mathbf{x})$是“错误的”，我们可以通过使用以下真实风险中的简单身份来纠正：

$$
\begin{aligned}
\int\int l(f(\mathbf{x}), y) p(y \mid \mathbf{x})p(\mathbf{x}) \;d\mathbf{x}dy =
\int\int l(f(\mathbf{x}), y) q(y \mid \mathbf{x})q(\mathbf{x})\frac{p(\mathbf{x})}{q(\mathbf{x})} \;d\mathbf{x}dy.
\end{aligned}
$$

换句话说，我们需要根据正确分布与错误分布的概率之比来重新衡量每个数据示例的权重：

$$\beta_i \stackrel{\mathrm{def}}{=} \frac{p(\mathbf{x}_i)}{q(\mathbf{x}_i)}.$$

插入每个数据示例$\beta_i$的权重$(\mathbf{x}_i, y_i)$，我们可以使用
*加权经验风险最小化*：

$$\mathop{\mathrm{minimize}}_f \frac{1}{n} \sum_{i=1}^n \beta_i l(f(\mathbf{x}_i), y_i).$$
:eqlabel:`eq_weighted-empirical-risk-min`

唉，我们不知道这个比率，所以在我们可以做任何有用的事情之前，我们需要估计它。有很多方法可用，包括一些奇特的算子理论方法，它们试图使用最小范数或最大熵原理直接重新校准期望算子。注意，对于任何这样的方法，我们需要从两个分布中提取样本-例如通过访问测试数据而得到的“真”$p$，以及用于生成训练集$q$的样本(后者是普通可用的)。然而，请注意，我们只需要特征$\mathbf{x} \sim p(\mathbf{x})$；我们不需要访问标签$y \sim p(y)$。

在这种情况下，存在一种非常有效的方法，它将给出几乎与原始方法一样好的结果：Logistic回归，这是二进制分类的软最大回归(见:numref:`sec_softmax`)的特例。这就是计算估计概率比所需的全部内容。我们学习了一个分类器来区分从$p(\mathbf{x})$提取的数据和从$q(\mathbf{x})$提取的数据。如果无法区分这两个分布，则意味着关联的实例同样可能来自这两个分布中的任何一个。另一方面，任何可以很好区分的实例都应该相应地显著增加或减少权重。

为简单起见，假设我们分别拥有来自发行版$p(\mathbf{x})$和$q(\mathbf{x})$的相同数量的实例。现在用$z$来表示标签，这些标签对于从$p$提取的数据是$1$，对于从$q$提取的数据是$-1$。则混合数据集中的概率由下式给出

$$P(z=1 \mid \mathbf{x}) = \frac{p(\mathbf{x})}{p(\mathbf{x})+q(\mathbf{x})} \text{ and hence } \frac{P(z=1 \mid \mathbf{x})}{P(z=-1 \mid \mathbf{x})} = \frac{p(\mathbf{x})}{q(\mathbf{x})}.$$

因此，如果我们使用逻辑回归方法，其中$P(z=1 \mid \mathbf{x})=\frac{1}{1+\exp(-h(\mathbf{x}))}$($h$是参数化函数)，则遵循

$$
\beta_i = \frac{1/(1 + \exp(-h(\mathbf{x}_i)))}{\exp(-h(\mathbf{x}_i))/(1 + \exp(-h(\mathbf{x}_i)))} = \exp(h(\mathbf{x}_i)).
$$

因此，我们需要解决两个问题：第一个问题是区分从两个分布中提取的数据，然后是:eqref:`eq_weighted-empirical-risk-min`中的加权经验风险最小化问题，在这个问题中，我们将项加权$\beta_i$。

现在我们准备描述一种校正算法。假设我们具有训练集$\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}$和未标记的测试集$\{\mathbf{u}_1, \ldots, \mathbf{u}_m\}$。对于协变量平移，我们假设$\mathbf{x}_i$的全部$1 \leq i \leq n$取自某个源分布，$\mathbf{u}_i$的全部$1 \leq i \leq m$取自目标分布。以下是校正协变量偏移的典型算法：

1. 生成二进制分类训练集：$\{(\mathbf{x}_1, -1), \ldots, (\mathbf{x}_n, -1), (\mathbf{u}_1, 1), \ldots, (\mathbf{u}_m, 1)\}$。
1. 使用逻辑回归训练二进制分类器以获得函数$h$。
1. 使用$\beta_i = \exp(h(\mathbf{x}_i))$或更好的$\beta_i = \min(\exp(h(\mathbf{x}_i)), c)$来加权训练数据，对于某个常数$c$。
1. 使用权重$\beta_i$在:eqref:`eq_weighted-empirical-risk-min`的$\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}$上进行训练。

请注意，上述算法依赖于一个重要的假设。为了使该方案起作用，我们需要目标(例如，测试时间)分布中的每个数据示例具有在训练时间出现的非零概率。如果我们找到$p(\mathbf{x}) > 0$但$q(\mathbf{x}) = 0$的点，那么相应的重要性权重应该是无穷大。

### 标签移位校正

假设我们正在处理一个具有$k$个类别的分类任务。在:numref:`subsec_covariate-shift-correction`、$q$和$p$中使用相同的符号分别是源分布(例如，训练时间)和目标分布(例如，测试时间)。假设标签的分布随时间变化：$q(y) \neq p(y)$，但类条件分布保持不变：$q(\mathbf{x} \mid y)=p(\mathbf{x} \mid y)$。如果源分布$q(y)$是“错误的”，我们可以根据如:eqref:`eq_true-risk`中定义的真实风险中的以下身份进行校正：

$$
\begin{aligned}
\int\int l(f(\mathbf{x}), y) p(\mathbf{x} \mid y)p(y) \;d\mathbf{x}dy =
\int\int l(f(\mathbf{x}), y) q(\mathbf{x} \mid y)q(y)\frac{p(y)}{q(y)} \;d\mathbf{x}dy.
\end{aligned}
$$

这里，我们的重要性权重将对应于标签似然比

$$\beta_i \stackrel{\mathrm{def}}{=} \frac{p(y_i)}{q(y_i)}.$$

标签移动的一个好处是，如果我们在源分布上有一个相当好的模型，那么我们可以得到这些权重的一致估计，而不需要处理环境维度。在深度学习中，输入往往是高维对象，如图像，而标签通常是更简单的对象，如类别。

为了估计目标标签分布，我们首先采用性能相当好的现成分类器(通常基于训练数据进行训练)，并使用验证集(也来自训练分布)计算其念力矩阵。*念力矩阵*，$\mathbf{C}$，只是一个$k \times k$矩阵，其中每列对应于标签类别(基本事实)，每行对应于我们模型的预测类别。每个单元格的值$c_{ij}$是验证集上总预测的分数，其中真实标签为$j$，我们的模型预测为$i$。

现在，我们不能直接计算目标数据上的念力矩阵，因为我们无法看到我们在野外看到的示例的标签，除非我们投资于复杂的实时注释管道。然而，我们可以做的是一起在测试时间对我们所有的模型预测进行平均，产生平均模型输出$\mu(\hat{\mathbf{y}}) \in \mathbb{R}^k$，其$i^\mathrm{th}$元素$\mu(\hat{y}_i)$是测试集上我们的模型预测$i$的总预测的分数。

结果表明，在一些温和的条件下-如果我们的分类器首先相当准确，如果目标数据只包含我们以前见过的类别，并且如果标签移动假设首先成立(这里是最强的假设)，那么我们可以通过求解一个简单的线性系统来估计测试集标签分布

$$\mathbf{C} p(\mathbf{y}) = \mu(\hat{\mathbf{y}}),$$

因为作为估计，$\sum_{j=1}^k c_{ij} p(y_j) = \mu(\hat{y}_i)$对于所有$1 \leq i \leq k$都成立，其中$p(y_j)$是$k$维标签分布向量$p(\mathbf{y})$的$j^\mathrm{th}$个元素。如果我们的分类器一开始就足够准确，那么念力矩阵$\mathbf{C}$将是可逆的，并且我们得到解$p(\mathbf{y}) = \mathbf{C}^{-1} \mu(\hat{\mathbf{y}})$。

因为我们观察源数据上的标签，所以很容易估计分布$q(y)$。然后，对于任何具有标签$y_i$的训练示例$i$，我们可以取我们估计的$p(y_i)/q(y_i)$的比率来计算权重$\beta_i$，并将其插入到:eqref:`eq_weighted-empirical-risk-min`中的加权经验风险最小化中。

### 概念转移纠正

概念转变很难有原则地解决。举例来说，当问题突然从区分猫和狗变成区分白色和黑色动物时，假设我们可以做得更好，而不仅仅是从零开始收集新标签和训练，这是不合理的。幸运的是，在实践中，这种极端的转变是罕见的。相反，通常会发生的情况是，任务一直在缓慢变化。为了让事情更具体，这里有一些例子：

* 在计算广告中，新产品被推出，
旧产品变得不那么受欢迎了。这意味着广告的分布和受欢迎程度会逐渐改变，任何点击率预测指标都需要随之逐渐改变。
* 交通摄像镜头由于环境磨损逐渐退化，逐渐影响图像质量。
* 新闻内容逐渐发生变化(即大部分新闻保持不变，但有新的故事出现)。

在这种情况下，我们可以使用与训练网络相同的方法来使它们适应数据的变化。换句话说，我们使用现有的网络权重，并简单地使用新数据执行几个更新步骤，而不是从头开始训练。

## 学习问题的分类学研究

掌握了如何处理分发中的更改的知识后，我们现在可以考虑机器学习问题表达的其他一些方面。

### 批处理学习

在*批处理学习*中，我们可以访问训练特征和标签$\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}$，我们使用它们来训练模型$f(\mathbf{x})$。稍后，我们部署此模型来对来自同一分布的新数据$(\mathbf{x}, y)$进行评分。这是我们这里讨论的任何问题的默认假设。例如，我们可以根据大量的猫和狗的照片来训练猫探测器。一旦我们训练了它，我们就把它作为智能猫门计算机视觉系统的一部分发货，该系统只允许猫进入。然后将其安装在客户家中，并且再也不会更新(除非在极端情况下)。

### 在线学习

现在假设数据$(\mathbf{x}_i, y_i)$每次到达一个样本。更具体地说，假设我们首先观察到$\mathbf{x}_i$，然后我们需要得出估计的$f(\mathbf{x}_i)$，只有当我们做到这一点后，我们才观察到$y_i$，随之而来的是，根据我们的决定，我们会获得回报或招致损失。很多真正的问题都属于这一类。例如，我们需要预测明天的股价，这使得我们可以根据这个估计进行交易，在一天结束时，我们会发现我们的估计是否允许我们盈利。换句话说，在*在线学习*中，我们有以下循环，在这个循环中，我们不断改进我们的模型，并提供新的观察结果。

$$
\mathrm{model} ~ f_t \longrightarrow
\mathrm{data} ~ \mathbf{x}_t \longrightarrow
\mathrm{estimate} ~ f_t(\mathbf{x}_t) \longrightarrow
\mathrm{observation} ~ y_t \longrightarrow
\mathrm{loss} ~ l(y_t, f_t(\mathbf{x}_t)) \longrightarrow
\mathrm{model} ~ f_{t+1}
$$

### 土匪

*强盗*是上述问题的特例。虽然在大多数学习问题中，我们具有连续参数化的函数$f$，其中我们想要学习其参数(例如，深度网络)，但是在*强盗*问题中，我们仅具有有限数量的我们可以拉动的手臂，即，我们可以采取的有限数量的动作。对于这个更简单的问题，可以获得更强的最优性理论保证，这并不令人惊讶。我们之所以列出它，主要是因为这个问题经常(令人困惑地)被视为一个独特的学习环境。

### 控制

在很多情况下，环境会记住我们的所作所为。不一定是以敌对的方式，但它只会记住，反应将取决于之前发生的事情。例如，咖啡锅炉控制器将观察到不同的温度，这取决于它之前是否正在加热锅炉。PID(比例-积分-微分)控制器算法在那里是一个流行的选择。同样，用户在新闻网站上的行为将取决于我们之前向他展示的内容(例如，他将只读一次大多数新闻)。许多这样的算法形成了一个环境模型，它们在其中采取行动，从而使它们的决定看起来不那么随机。最近，控制理论(例如，PID变体)也已被用于自动调整超参数，以实现更好的解缠和重建质量，并提高生成文本的多样性和生成图像:cite:`Shao.Yao.Sun.ea.2020`的重建质量。

### 强化学习

在更一般的有记忆的环境中，我们可能会遇到环境试图与我们合作的情况(合作游戏，特别是非零和游戏)，或者其他环境试图取胜的情况。国际象棋、围棋、双陆棋或星际争霸都是强化学习中的一些例子。同样，我们可能想要为自动驾驶汽车制造一个好的控制器。其他汽车可能会以不平凡的方式对自动驾驶汽车的驾驶方式做出反应，例如，试图避开它，试图造成事故，并试图与其合作。

### 考虑到环境问题

上述不同情况之间的一个关键区别是，在固定环境中可能始终有效的相同策略，在环境可以适应的情况下可能不会始终有效。例如，交易员发现的套利机会一旦开始利用，很可能就会消失。环境变化的速度和方式在很大程度上决定了我们可以采用的算法类型。例如，如果我们知道事情可能只会缓慢改变，我们可以强制任何估计也只能缓慢改变。如果我们知道环境可能会瞬间变化，但变化的频率很低，我们就可以考虑到这一点。这些类型的知识对于有抱负的数据科学家处理概念转变至关重要，也就是说，当他试图解决的问题随着时间的推移而发生变化时。

## 机器学习中的公平性、问责性和透明度

最后，重要的是要记住，当您部署机器学习系统时，您不仅仅是在优化预测模型-您通常会提供一个将用于(部分或全部)自动化决策的工具。这些技术系统可能会影响个人的生活，受到由此产生的决定的影响。从考虑预测到决策的飞跃不仅提出了新的技术问题，而且还提出了一系列必须仔细考虑的伦理问题。如果我们正在部署医疗诊断系统，我们需要知道它可能适用于哪些人群，哪些可能不适用。忽视亚群福利的可预见风险可能会导致我们管理较差的护理。此外，一旦我们考虑决策系统，我们必须退后一步，重新考虑我们如何评估我们的技术。在这种范围变化的其他后果中，我们会发现“准确性”很少是正确的衡量标准。例如，在将预测转化为行动时，我们通常会想要考虑到以各种方式出错的潜在成本敏感性。如果一种错误分类图像的方式可以被认为是种族花招，而错误分类到不同的类别将是无害的，那么我们可能想要相应地调整我们的阈值，在设计决策方案时考虑到社会价值。我们还希望小心预测系统如何导致反馈循环。例如，考虑预测性警务系统，它将巡逻人员分配到预测犯罪率较高的地区。很容易看出一个令人担忧的模式是如何出现的：

 1. 犯罪率较高的社区会有更多的巡逻。
 1. 因此，在这些社区发现了更多的犯罪，输入了可供未来迭代使用的训练数据。
 1. 在接触到更多积极因素的情况下，该模型预测这些社区的犯罪率还会更高。
 1. 在下一次迭代中，更新的模型针对相同的社区甚至更严重，从而导致更多的犯罪被发现，等等。

通常，模型的预测与其训练数据相耦合的各种机制在建模过程中是无法解释的。这可能会导致研究人员所说的“失控反馈循环”。此外，我们首先要小心我们是否解决了正确的问题。预测算法现在在信息传播中扮演着非常重要的角色。个人遇到的新闻是否应该由他们“喜欢”的一组Facebook页面来决定？这些只是你在机器学习的职业生涯中可能会遇到的许多紧迫的伦理困境中的一小部分。

## 摘要

* 在许多情况下，训练集和测试集不是来自相同的分布。这就是所谓的分配转移。
* 真正的风险是从数据的真实分布中提取的数据在整个人群中丢失的预期。然而，这整个群体通常是不可用的。经验风险是对训练数据的平均损失，以近似真实风险。在实践中，我们执行经验风险最小化。
* 在相应的假设条件下，可以在测试时检测并校正协变量和标签偏移。在测试时，无法解释这种偏差可能会成为问题。
* 在某些情况下，环境可能会记住自动操作并以令人惊讶的方式做出响应。在构建模型时，我们必须考虑到这种可能性，并继续监控实时系统，并对我们的模型和环境以意想不到的方式纠缠在一起的可能性持开放态度。

## 练习

1. 当我们改变搜索引擎的行为时会发生什么？用户可能会做些什么呢？广告商呢？
1. 实现了一个协变量移位检测器。提示：构建一个分类器。
1. 实现一个协变量移位校正器。
1. 除了分布变化，还有什么会影响经验风险如何接近真实风险？

[Discussions](https://discuss.d2l.ai/t/105)
