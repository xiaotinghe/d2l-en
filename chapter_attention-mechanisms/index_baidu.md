# 注意机制
:label:`chap_attention`

灵长类视觉系统的视神经接收大量的感觉输入，远远超过大脑所能完全处理的。幸运的是，并非所有的刺激都是平等的。在复杂的视觉环境中，意识的聚焦和集中使得灵长类动物能够将注意力引导到感兴趣的物体上，如猎物和掠食者。只关注一小部分信息的能力具有进化意义，使人类得以生存和成功。

自19世纪以来，科学家们一直在认知神经科学领域研究注意力。在本章中，我们将首先回顾一个流行的框架，解释如何在视觉场景中部署注意力。受此框架中注意线索的启发，我们将设计利用这些注意线索的模型。值得注意的是，1964年的Nadaraya-Waston核回归是机器学习与“注意机制”的简单演示。

接下来，我们将继续介绍在深度学习的注意模型设计中广泛使用的注意函数。具体来说，我们将展示如何使用这些函数来设计*Bahdanau注意力*，这是一个在深度学习中具有开创性的注意力模型，它可以双向对齐并且是可微的。

最后，配备了最近的
*多头注意*
在“自我注意”设计中，我们将描述完全基于注意机制的“变压器”架构。自2017年提议以来，变形金刚已广泛应用于现代深度学习应用，如语言、视觉、言语和强化学习领域。

```toc
:maxdepth: 2

attention-cues
nadaraya-waston
attention-scoring-functions
bahdanau-attention
multihead-attention
self-attention-and-positional-encoding
transformer
```
