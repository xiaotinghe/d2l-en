# 注意机制
:label:`chap_attention`

灵长类动物视觉系统的视神经接受大量的感官输入，远远超过大脑完全处理的能力。幸运的是，并不是所有的刺激都是一样的。意识的聚焦和集中使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，如猎物和捕食者。只关注一小部分信息的能力具有进化意义，使人类得以生存和成功。

自19世纪以来，科学家们一直在研究认知神经科学领域的注意力。在这一章中，我们将首先回顾一个流行的框架，解释注意力是如何在视觉场景中展开的。受此框架中注意力线索的启发，我们将设计利用此类注意力线索的模型。值得注意的是，1964年的Nadaraya-Waston核回归是具有*注意机制*的机器学习的一个简单演示。

接下来，我们将继续介绍在深度学习中的注意模型设计中广泛使用的注意函数。具体地说，我们将展示如何使用这些函数来设计*巴赫达诺注意力*，这是深度学习中一种突破性的注意力模型，可以双向对齐，并且是可区分的。

最后，配备了更新的
*多头注意**
和“自我关注”设计，我们将描述完全基于注意机制的“转换器”架构。自2017年提出以来，Transformers已经渗透到现代深度学习应用中，例如在语言、视觉、语音和强化学习领域。

```toc
:maxdepth: 2

attention-cues
nadaraya-waston
attention-scoring-functions
bahdanau-attention
multihead-attention
self-attention-and-positional-encoding
transformer
```
