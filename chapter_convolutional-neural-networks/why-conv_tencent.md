# 从全连通层到卷积
:label:`sec_why-conv`

直到今天，当我们处理表格数据时，我们到目前为止已经讨论过的模型仍然是合适的选择。通过表格，我们的意思是数据由对应于示例的行和对应于特性的列组成。对于表格数据，我们可能预计我们寻找的模式可能涉及特征之间的交互，但我们不假定关于特征如何交互的任何结构*先验*。

有时候，我们确实缺乏知识来指导更巧妙的架构的构建。在这些情况下，MLP可能是我们能做的最好的选择。然而，对于高维感知数据，这样的无结构网络可能会变得笨拙。

例如，让我们回到我们正在运行的区分猫和狗的例子上。假设我们在数据收集方面做了一次彻底的工作，收集了一百万像素照片的带注释的数据集。这意味着网络的每一次输入都有一百万个维度。即使大幅减少到1,000个隐藏维度，也需要一个以$10^6 \times 10^3 = 10^9$个参数为特征的完全连接层。除非我们有大量的GPU，在分布式优化方面的天赋，以及非凡的耐心，否则学习这个网络的参数可能被证明是不可行的。

细心的读者可能会反对这一论点，因为1百万像素的分辨率可能不是必要的。然而，虽然我们可能可以用10万像素逃脱惩罚，但我们1000大小的隐藏层严重低估了学习图像良好表示所需的隐藏单元的数量，因此一个实用的系统仍然需要数十亿个参数。此外，通过拟合如此多的参数来学习分类器可能需要收集大量的数据集。然而，今天人类和计算机都能很好地区分猫和狗，似乎与这些直觉相矛盾。这是因为图像显示了丰富的结构，人类和机器学习模型都可以利用这些结构。卷积神经网络(CNNs)是机器学习在利用自然图像中的一些已知结构时采用的一种创造性的方法。

## 不变性

假设您想要检测图像中的一个对象。这似乎是合理的，无论我们使用什么方法来识别对象，都不应该过分关注对象在图像中的精确位置。理想情况下，我们的系统应该利用这些知识。猪通常不会飞翔，飞机通常不会游泳。尽管如此，如果一头猪出现在图像的顶部，我们仍然应该认出它。我们可以从儿童游戏“沃尔多在哪里”(:numref:`img_waldo`)中得到一些启发。这个游戏由一些混乱的场景组成，充斥着各种活动。沃尔多出现在每一个地方，通常潜伏在一些不太可能的地方。读者的目标是找到他。尽管他的着装很有特色，但由于有大量的分心事情，这可能会出人意料地困难。然而，*Waldo是什么样子*并不取决于*Waldo位于*哪里*。我们可以用沃尔多探测器扫描图像，它可以给每个补丁分配一个分数，表明补丁包含沃尔多的可能性。CNNs将这一“空间不变性”的概念系统化，利用它来学习参数较少的有用表示。

![An image of the "Where's Waldo" game.](../img/where-wally-walker-books.jpg)
:width:`400px`
:label:`img_waldo`

现在，我们可以通过列举一些理想的数据来指导我们设计适用于计算机视觉的神经网络体系结构，从而使这些直觉变得更加具体：

1. 在最早的层中，我们的网络应该对相同的补丁做出类似的响应，而不管它出现在图像中的哪个位置。这一原理称为“平移不变性”。
1. 网络的最早层应该集中在本地区域，而不考虑远程区域的图像内容。这就是“地方性”原则。最终，这些局部表示可以被聚合以在整个图像级别上做出预测。

让我们看看这是如何转化为数学的。

## 约束MLP

首先，我们可以考虑具有二维图像$\mathbf{X}$作为输入并且它们的直接隐藏表示$\mathbf{H}$类似地被表示为数学上的矩阵和代码中的二维张量的多维图像处理，其中$\mathbf{X}$和$\mathbf{H}$具有相同的形状。让这一点深入人心。我们现在认为，不仅输入，而且隐藏的表征都具有空间结构。

设$[\mathbf{X}]_{i, j}$和$[\mathbf{H}]_{i, j}$分别表示输入图像和隐藏表示中位置($i$,$j$)处的像素。因此，为了使每个隐藏单元接收来自每个输入像素的输入，我们将从使用权重矩阵(如我们先前在MLP中所做的那样)切换到将我们的参数表示为四阶权重张量$\mathsf{W}$。假设$\mathbf{U}$包含偏差，我们可以将完全连接层正式表示为

$$\begin{aligned} \left[\mathbf{H}\right]_{i, j} &= [\mathbf{U}]_{i, j} + \sum_k \sum_l[\mathsf{W}]_{i, j, k, l}  [\mathbf{X}]_{k, l}\\ &=  [\mathbf{U}]_{i, j} +
\sum_a \sum_b [\mathsf{V}]_{i, j, a, b}  [\mathbf{X}]_{i+a, j+b}.\end{aligned},$$

其中，由于在两个四阶张量中的系数之间存在一对一的对应关系，因此从$\mathsf{W}$到$\mathsf{V}$的切换目前完全是表面上的。我们简单地重新索引下标$(k, l)$，使得$k = i+a$和$l = j+b$。换句话说，我们设定了$[\mathsf{V}]_{i, j, a, b} = [\mathsf{W}]_{i, j, i+a, j+b}$。索引$a$和$b$在正偏移量和负偏移量两者上运行，覆盖整个图像。对于隐藏表示$[\mathbf{H}]_{i, j}$中的任何给定位置($i$,$j$)，我们通过对$x$中的像素求和，以$(i, j)$为中心并按$[\mathsf{V}]_{i, j, a, b}$加权来计算其值。

### 平移不变性

现在让我们引用上面建立的第一个原则：平移不变性。这意味着输入$\mathbf{X}$中的移位应该简单地导致隐藏表示$\mathbf{H}$中的移位。这只有在$\mathsf{V}$和$\mathbf{U}$实际上不依赖于$(i, j)$的情况下才有可能，也就是说，我们有$[\mathsf{V}]_{i, j, a, b} = [\mathbf{V}]_{a, b}$，而$\mathbf{U}$是一个常数，比如说$u$。因此，我们可以简化$\mathbf{H}$的定义：

$$[\mathbf{H}]_{i, j} = u + \sum_a\sum_b [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}.$$

这是一个*卷积*！我们在$(i+a, j+b)$用系数$[\mathbf{V}]_{a, b}$对位置$(i, j)$附近的像素进行有效加权，以获得值$[\mathbf{H}]_{i, j}$。请注意，$[\mathbf{V}]_{a, b}$需要的系数比$[\mathsf{V}]_{i, j, a, b}$少得多，因为它不再依赖于图像中的位置。我们已经取得了重大进展！

###  地方性

现在让我们引用第二个原则：局部性。如上所述，我们认为，我们不应该为了收集相关信息来评估$(i, j)$地点正在发生的事情，就必须在距离地点$[\mathbf{H}]_{i, j}$很远的地方寻找。这意味着在某个范围$|a|> \Delta$或$|b| > \Delta$之外，我们应该设置为$[\mathbf{V}]_{a, b} = 0$。等效地，我们可以将$[\mathbf{H}]_{i, j}$重写为

$$[\mathbf{H}]_{i, j} = u + \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}.$$
:eqlabel:`eq_conv-layer`

注意，简而言之，:eqref:`eq_conv-layer`是*卷积层*。
*卷积神经网络(CNNs)
是包含卷积层的一类特殊的神经网络。在深度学习研究社区中，$\mathbf{V}$被称为“卷积内核”、“过滤”，或者仅仅是层的“权重”，它们通常是可学习的参数。当本地区域较小时，与完全连接的网络相比，差异可能会很大。以前，我们可能需要数十亿个参数来表示图像处理网络中的单个层，而现在我们通常只需要几百个参数，而不改变输入或隐藏表示的维度。这种参数的急剧减少所付出的代价是，我们的功能现在是平移不变的，并且在确定每个隐藏激活的值时，我们的层只能合并本地信息。所有的学习都依赖于强加归纳偏见。当这种偏见与现实相符时，我们就得到了样本效率高的模型，它很好地推广到了看不见的数据。但当然，如果这些偏见与现实不符，例如，如果图像被证明不是平移不变的，我们的模型甚至可能难以拟合我们的训练数据。

## 卷积

在进一步讨论之前，我们应该简单回顾一下为什么上述运算被称为卷积。在数学中，两个函数(比方说$f, g: \mathbb{R}^d \to \mathbb{R}$)之间的“卷积”定义为

$$(f * g)(\mathbf{x}) = \int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d\mathbf{z}.$$

也就是说，当一个函数被“翻转”并移位$f$时，我们测量$f$和$g$之间的重叠。只要我们有离散的物体，积分就会变成和。例如，对于指数超过$\mathbb{Z}$的平方可和无限维向量集合中的向量，我们获得以下定义：

$$(f * g)(i) = \sum_a f(a) g(i-a).$$

对于二维张量，我们有一个相应的总和，指数分别为$(a, b)$($f$)和$(i-a, j-b)$($g$)：

$$(f * g)(i, j) = \sum_a\sum_b f(a, b) g(i-a, j-b).$$
:eqlabel:`eq_2d-conv-discrete`

这看起来与:eqref:`eq_conv-layer`类似，但有一个主要区别。我们使用的不是$(i+a, j+b)$，而是差值。不过请注意，这种区别主要是表面性的，因为我们总是可以匹配:eqref:`eq_conv-layer`和:eqref:`eq_2d-conv-discrete`之间的记号。我们在:eqref:`eq_conv-layer`中的原始定义更恰当地描述了*交叉相关*。我们将在下一节回到这一点。

## 重温“沃尔多在哪里”

回到我们的沃尔多探测器，让我们看看这是什么样子。卷积层根据过滤$\mathsf{V}$挑选给定大小的窗口并加权强度，如:numref:`fig_waldo_mask`中所示。我们的目标可能是学习一个模型，这样无论在哪里“瓦朗度”最高，我们都应该在隐藏层表示中找到一个峰值。

![Detect Waldo.](../img/waldo-mask.jpg)
:width:`400px`
:label:`fig_waldo_mask`

### 频道
:label:`subsec_why-conv-channels`

这种方法只有一个问题。到目前为止，我们幸福地忽略了图像由3个通道组成：红色、绿色和蓝色。实际上，图像不是二维对象，而是三阶张量，其特征在于高度、宽度和通道，例如形状为$1024 \times 1024 \times 3$像素。虽然这些轴中的前两个涉及空间关系，但第三个轴可以被视为为每个像素位置分配多维表示。因此，我们将$\mathsf{X}$索引为$[\mathsf{X}]_{i, j, k}$。错综复杂的过滤不得不做出相应的调整。我们现在有$[\mathsf{V}]_{a,b,c}$人，而不是$[\mathbf{V}]_{a,b}$人。

此外，正如我们的输入由三阶张量组成一样，事实证明，将我们的隐藏表示类似地表示为三阶张量$\mathsf{H}$也是一个好主意。换句话说，我们想要对应于每个空间位置的隐藏表示的整个向量，而不是只有一个与每个空间位置相对应的隐藏表示。我们可以认为隐藏的表示是由若干个堆叠在一起的二维网格组成的。与输入一样，这些有时也称为*通道*。它们有时也称为“要素地图”，因为每个地图都向后续图层提供了一组空间化的学习要素。直观地说，您可能会想象在更靠近输入的较低层，某些通道可能会专门用于识别边，而其他通道可能会识别纹理。

要同时支持输入($\mathsf{X}$)和隐藏表示($\mathsf{H}$)中的多个通道，我们可以将第四个坐标添加到$\mathsf{V}$：$[\mathsf{V}]_{a, b, c, d}$。把我们拥有的一切都放在一起：

$$[\mathsf{H}]_{i,j,d} = \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} \sum_c [\mathsf{V}]_{a, b, c, d} [\mathsf{X}]_{i+a, j+b, c},$$
:eqlabel:`eq_conv-layer-channels`

其中$d$索引隐藏表示$\mathsf{H}$中的输出通道。随后的卷积层将继续接受三阶张量$\mathsf{H}$作为输入。更一般地，:eqref:`eq_conv-layer-channels`是多个信道的卷积层的定义，其中$\mathsf{V}$是该层的核心或过滤。

我们仍有许多行动需要解决。例如，我们需要弄清楚如何将所有隐藏的表示组合到单个输出中，例如，图像中是否有Waldo“Anywhere”。我们还需要决定如何有效地计算事物，如何组合多层、适当的激活函数，以及如何做出合理的设计选择，以产生在实践中有效的网络。我们将在本章的剩余部分讨论这些问题。

## 摘要

* 图像中的平移不变性意味着图像的所有块都将以相同的方式处理。
* 局部性意味着只有一小部分像素将用于计算相应的隐藏表示。
* 在图像处理中，卷积层通常比完全连通的层需要的参数少得多。
* CNN是一类特殊的包含卷积层的神经网络。
* 输入和输出上的通道允许我们的模型在每个空间位置捕捉图像的多个方面。

## 练习

1. 假设卷积核的大小为$\Delta = 0$。显示在这种情况下，卷积内核为每组信道独立地实现MLP。
1. 为什么翻译不变毕竟不是一个好主意呢？
1. 在决定如何处理与图像边界上的像素位置相对应的隐藏表示时，我们必须处理哪些问题？
1. 描述音频的类似卷积层。
1. 您认为卷积层是否也适用于文本数据？为什么或者为什么不？
1. 证明那$f * g = g * f$。

[Discussions](https://discuss.d2l.ai/t/64)
