# 从完全连接的层到卷积
:label:`sec_why-conv`

时至今日，当我们处理表格数据时，我们迄今为止讨论的模型仍然是适当的选择。通过表格，我们的意思是数据由对应于示例的行和对应于特征的列组成。对于表格数据，我们可以预期我们所寻求的模式可能涉及到特征之间的交互，但是我们没有假设任何关于特征如何交互的结构。

有时候，我们真的缺乏知识来指导建造更精巧的建筑。在这种情况下，MLP可能是我们能做的最好的。然而，对于高维感知数据，这种无结构的网络可能会变得笨拙。

例如，让我们回到区分猫和狗的例子。假设我们在数据收集方面做了彻底的工作，收集了一个一百万像素照片的注释数据集。这意味着网络的每个输入都有一百万个维度。即使是积极地减少到一千个隐藏维度，也需要一个以$10^6 \times 10^3 = 10^9$个参数为特征的完全连接层。除非我们有大量的gpu，一个分布式优化的天才，以及非常多的耐心，否则学习这个网络的参数可能是不可行的。

细心的读者可能会反对这一论点，理由是可能不需要一百万像素的分辨率。然而，虽然我们可能可以逃脱十万像素，我们的隐藏层大小1000严重低估了隐藏单元的数量，它需要学习良好的图像表现，所以一个实际的系统仍然需要数十亿个参数。此外，通过拟合这么多参数来学习分类器可能需要收集大量的数据集。然而今天，人类和计算机都能很好地区分猫和狗，似乎与这些直觉相矛盾。这是因为图像具有丰富的结构，可以被人类和机器学习模型利用。卷积神经网络（CNNs）是一种创造性的机器学习方法，它利用自然图像中的某些已知结构。

## 不变性

假设您想检测图像中的对象。不管我们用什么方法来识别物体，都不应该过分关注物体在图像中的精确位置，这似乎是合理的。理想情况下，我们的系统应该利用这些知识。猪通常不会飞，飞机通常不会游泳。尽管如此，我们仍然应该认识到一头猪出现在图像的顶部。我们可以从儿童游戏《瓦尔多在哪里》（:numref:`img_waldo`中描述）中获得一些灵感。这个游戏由一系列充满活动的混乱场景组成。瓦尔多出现在每一个地方，通常潜伏在一些不太可能的地方。读者的目标是找到他。尽管他的服装很有特色，但这可能是令人惊讶的困难，因为大量的分心。然而，*瓦尔多的样子*并不取决于*瓦尔多在哪里*。我们可以用Waldo检测器扫描图像，它可以给每个补丁分配一个分数，表明该补丁包含Waldo的可能性。CNNs系统化了空间不变性的概念，利用它来学习用较少参数表示的有用表示。

![An image of the "Where's Waldo" game.](../img/where-wally-walker-books.jpg)
:width:`400px`
:label:`img_waldo`

现在，我们可以通过列举一些需要帮助我们设计适用于计算机视觉的神经网络体系结构，使这些直觉更加具体：

1. 在最早的层中，我们的网络应该对相同的补丁做出类似的响应，而不管它出现在图像中的哪个位置。这个原理叫做“平移不变性”。
1. 网络最早的层次应该集中在局部区域，而不考虑遥远地区的图像内容。这就是“局部性”原则。最终，这些局部表示可以聚合起来，在整个图像级别上做出预测。

让我们看看这是如何转化为数学的。

## 限制MLP

首先，我们可以考虑以二维图像$\mathbf{X}$作为输入，其直接隐藏表示$\mathbf{H}$在数学上类似地表示为矩阵，在代码中表示为二维张量，其中$\mathbf{X}$和$\mathbf{H}$具有相同的形状。让它沉下去。我们现在不仅认为输入，而且还认为隐藏的表示具有空间结构。

让$[\mathbf{X}]_{i, j}$和$[\mathbf{H}]_{i, j}$分别表示输入图像和隐藏表示中的位置（$i$、$j$）处的像素。因此，为了让每个隐藏单元接收来自每个输入像素的输入，我们将从使用权重矩阵（如我们先前在MLPs中所做的那样）切换到将我们的参数表示为四阶权重张量$\mathsf{W}$。假设$\mathbf{U}$包含偏差，我们可以将全连通层正式表示为

$$\begin{aligned} \left[\mathbf{H}\right]_{i, j} &= [\mathbf{U}]_{i, j} + \sum_k \sum_l[\mathsf{W}]_{i, j, k, l}  [\mathbf{X}]_{k, l}\\ &=  [\mathbf{U}]_{i, j} +
\sum_a \sum_b [\mathsf{V}]_{i, j, a, b}  [\mathbf{X}]_{i+a, j+b}.\end{aligned},$$

其中，从$\mathsf{W}$到$\mathsf{V}$的转换现在完全是化妆品，因为在两个四阶张量中，系数之间存在一对一的对应关系。我们只需重新索引下标$(k, l)$，使$k = i+a$和$l = j+b$。换句话说，我们设置了$[\mathsf{V}]_{i, j, a, b} = [\mathsf{W}]_{i, j, i+a, j+b}$。索引$a$和$b$覆盖了正偏移和负偏移，覆盖了整个图像。对于隐藏表示$[\mathbf{H}]_{i, j}$中的任何给定位置（$i$、$j$），我们通过对$x$中以$(i, j)$为中心并按$[\mathsf{V}]_{i, j, a, b}$加权的像素求和来计算其值。

### 平移不变性

现在让我们引用上面建立的第一个原则：翻译不变性。这意味着输入$\mathbf{X}$中的移位应该仅仅导致隐藏表示$\mathbf{H}$中的移位。只有当$\mathsf{V}$和$\mathbf{U}$实际上不依赖于$(i, j)$时，这才有可能，也就是说，我们有$[\mathsf{V}]_{i, j, a, b} = [\mathbf{V}]_{a, b}$，$\mathbf{U}$是一个常数，比如$u$。因此，我们可以简化$\mathbf{H}$的定义：

$$[\mathbf{H}]_{i, j} = u + \sum_a\sum_b [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}.$$

这是个卷积！我们使用系数$(i+a, j+b)$有效地加权位置$(i, j)$附近的像素以获得值$[\mathbf{H}]_{i, j}$。注意，$[\mathbf{V}]_{a, b}$需要的系数比$[\mathsf{V}]_{i, j, a, b}$少很多，因为它不再依赖于图像中的位置。我们取得了重大进展！

###  地点

现在让我们引用第二个原则：局部性。如上所述，我们认为，为了收集相关信息，以评估$[\mathbf{H}]_{i, j}$发生的情况，我们不应将目光投向离$(i, j)$很远的地方。这意味着在$|a|> \Delta$或$|b| > \Delta$的范围之外，我们应该设置$[\mathbf{V}]_{a, b} = 0$。等效地，我们可以将$[\mathbf{H}]_{i, j}$重写为

$$[\mathbf{H}]_{i, j} = u + \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}.$$
:eqlabel:`eq_conv-layer`

注意，:eqref:`eq_conv-layer`，简而言之，是一个*卷积层*。
*卷积神经网络*（CNNs）
是包含卷积层的神经网络的一个特殊家族。在深度学习研究社区中，$\mathbf{V}$被称为卷积核*、过滤器*，或者简单地说，层的*权重*通常是可学习的参数。当局部区域很小时，与完全连接的网络相比，差异可能是巨大的。以前，我们可能需要数十亿个参数来表示图像处理网络中的一个层，而现在通常只需要几百个参数，而不改变输入或隐藏表示的维数。参数的急剧减少所付出的代价是，我们的特征现在是平移不变的，并且我们的层在确定每个隐藏激活的值时只能包含局部信息。所有的学习都依赖于施加归纳偏差。当这种偏差与实际情况相符时，我们得到了有效的样本模型，这些模型能很好地推广到不可见的数据中。但当然，如果这些偏差与实际情况不符，例如，如果图像不是平移不变的，我们的模型甚至可能难以适应我们的训练数据。

## 卷积

在进一步讨论之前，我们应该简要回顾一下为什么上面的操作被称为卷积。在数学中，两个函数（比如$f, g: \mathbb{R}^d \to \mathbb{R}$）之间的卷积被定义为

$$(f * g)(\mathbf{x}) = \int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d\mathbf{z}.$$

也就是说，当一个函数被“翻转”并移位$\mathbf{x}$时，我们测量$f$和$g$之间的重叠。当我们有离散对象时，积分就变成和。例如，对于索引超过$\mathbb{Z}$的可平方和无限维向量集合中的向量，我们得到以下定义：

$$(f * g)(i) = \sum_a f(a) g(i-a).$$

对于二维张量，对于$f$和$g$，我们分别具有$(a, b)$和$(i-a, j-b)$的对应和：

$$(f * g)(i, j) = \sum_a\sum_b f(a, b) g(i-a, j-b).$$
:eqlabel:`eq_2d-conv-discrete`

这看起来与:eqref:`eq_conv-layer`相似，但有一个主要区别。我们不是使用$(i+a, j+b)$，而是使用差异。但是请注意，这种区别主要是表面上的，因为我们总是可以匹配:eqref:`eq_conv-layer`和:eqref:`eq_2d-conv-discrete`之间的符号。我们在:eqref:`eq_conv-layer`中的原始定义更恰当地描述了*互相关*。我们将在下一节讨论这个问题。

## “瓦尔多在哪儿”又被重温了一遍

回到瓦尔多探测器，让我们看看这是什么样子。卷积层根据滤波器$\mathsf{V}$拾取给定尺寸的窗口并加权强度，如:numref:`fig_waldo_mask`中所示。我们的目标可能是学习一个模型，以便在“waldoness”最高的地方，我们应该在隐藏层表示中找到一个峰值。

![Detect Waldo.](../img/waldo-mask.jpg)
:width:`400px`
:label:`fig_waldo_mask`

### 渠道
:label:`subsec_why-conv-channels`

这种方法只有一个问题。到目前为止，我们很高兴地忽略了图像由3个通道组成：红色、绿色和蓝色。实际上，图像不是二维对象，而是三阶张量，其特征是高度、宽度和通道，例如，具有$1024 \times 1024 \times 3$像素的形状。虽然这些轴的前两个与空间关系有关，但第三个轴可被视为为为每个像素位置分配多维表示。因此，我们将$\mathsf{X}$索引为$[\mathsf{X}]_{i, j, k}$。卷积滤波器必须相应地进行调整。而不是$[\mathbf{V}]_{a,b}$，我们现在有$[\mathsf{V}]_{a,b,c}$。

此外，正如我们的输入是由三阶张量组成的，将我们的隐藏表示类似地表示为三阶张量$\mathsf{H}$是一个好主意。换言之，我们需要一个与每个空间位置对应的隐藏表示的完整向量，而不仅仅是与每个空间位置对应的隐藏表示。我们可以把隐藏的表示看作是由一系列相互叠加的二维网格组成的。在输入中，这些有时称为*通道*。它们有时也被称为*特征映射*，因为每一个都向后续层提供一组空间化的学习特征。直观地说，您可以想象，在靠近输入的较低层，一些通道可以专门识别边，而其他通道可以识别纹理。

为了支持输入（$\mathsf{X}$）和隐藏表示（$\mathsf{H}$）中的多个通道，我们可以在$\mathsf{V}$:$[\mathsf{V}]_{a, b, c, d}$中添加第四个坐标。把我们所有的东西放在一起：

$$[\mathsf{H}]_{i,j,d} = \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} \sum_c [\mathsf{V}]_{a, b, c, d} [\mathsf{X}]_{i+a, j+b, c},$$
:eqlabel:`eq_conv-layer-channels`

其中$d$索引隐藏表示$\mathsf{H}$中的输出信道。随后的卷积层将继续以三阶张量$\mathsf{H}$作为输入。更一般地说，:eqref:`eq_conv-layer-channels`是用于多个信道的卷积层的定义，其中$\mathsf{V}$是该层的内核或滤波器。

我们仍有许多行动需要解决。例如，我们需要弄清楚如何将所有隐藏的表示组合到一个输出中，例如，图像中是否存在Waldo*anywhere*。我们还需要决定如何有效地计算事物，如何组合多个层次，适当的激活函数，以及如何做出合理的设计选择，以产生实际有效的网络。我们将在本章的其余部分讨论这些问题。

## 摘要

* 图像的平移不变性意味着图像的所有面片都将以相同的方式进行处理。
* 局部性意味着只有一小部分像素邻域将用于计算相应的隐藏表示。
* 在图像处理中，卷积层通常比完全连接的层需要更少的参数。
* cnn是一类特殊的神经网络，包含卷积层。
* 输入和输出通道允许我们的模型在每个空间位置捕捉图像的多个方面。

## 练习

1. 假设卷积核的大小是$\Delta = 0$。说明在这种情况下，卷积内核为每组信道独立地实现了MLP。
1. 为什么翻译不变性不是一个好主意呢？
1. 在决定如何处理与图像边界像素位置对应的隐藏表示时，我们必须处理哪些问题？
1. 描述一个类似的音频卷积层。
1. 你认为卷积层也适用于文本数据吗？为什么或者为什么不呢？
1. 证明$f * g = g * f$。

[Discussions](https://discuss.d2l.ai/t/64)
